# Research Topics for Productions - 1. Model Compression (Quantization, Distillation, Prunning)
## 모델을 최적화 해야하는 이유 - 크기축소
더 작은 저장 크기
  - 작은 모델은 사용자의 기기에서 저장 공간을 적게 차지한다. -> 저장 공간이 작은 모바일 기기에서의 활용성을 높일 수 있다
    
더 작은 다운로드 크기
  - 모델을 사용자의 기기에 다운로드할 때의 소요시간과 대역폭을 줄일 수 있다

메모리 사용량 감소
  - 모델이 작을수록 실행시 더적은 RAM을 사용하므로 애플리케이션의 다른부분에서 사용할 수 있는 메모리를 확보할 수 있고, 성능과 안정성을 향상시킬 수 있다

## 모델을 최적화 해야하는 이유 - 지연 시간(latency) 감소
Research 환경에서는 일반적으로 모델의 성능 평가에 속도(speed)가 들어가지 않는 경우가 많다.  
Real-time production 상황에서는 그러나, 아무리 성능이 좋아도 inference time이 나오지 않으면 production에 나갈 수 없다.  
  -> AI 모델의 속도 문제는 production 상황에서 언제나 중요하다.

## Inference 속도를 높이는 일반적인 방법
1. Network의 Forward 로직과 전후 처리 로직이 비효율적인지 점검한다. (https://github.com/rkern/line_profiler)
2. 모델 아키텍처를 교체한다. (x ?배)
   - 더 얕은 모델, Auto-regressiveàSelf-attention 기반의 feed-forward 방식 등  
3. 모델을 서빙하는 Framework / Infra를 바꿔본다.
   - Tensorflow/Pytorch -> ONNX/TensorRT
   - Python -> C++
   - CPU-inference -> GPU-inference pod의 replica 증가
