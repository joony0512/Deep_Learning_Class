># Deep_Learning_Class
>>INTRO
### Chapter 01. 딥러닝 소개	
### Chapter 02. 딥러닝의 원리	
### Chapter 03. 딥러닝 공부법
>>Part 1
### Ch 01. 환경설정
### Ch 02. 데이터 타입과 컬렉션
### Ch 03. 조건문과 반복문
### Ch 04. 함수 이해 및 활용
### Ch 05. 파이썬 모듈
### Ch 06. 클래스와 인스턴스
### Ch 07. 정규 표현식
>>Part 2
## Ch 01. 딥러닝 네트워크의 연산
	- CH00_01. Introduction
	- CH00_02. CoLacoratory
### Lecture.1 Artificial Neuron
	- CH01_01. [이론]Parameteric Functions and Datasets
	- CH01_02. [이론]Artificial_Neurons
	- CH01_03. [구현]Affine_Functions
	- CH01_04. [구현]Artificial_Neurons
### Lecture.2 Dense Layers
	- CH02_01. [이론]_Dense_Layers
	- CH02_02. [이론]_The_First_Dense_Layer.
	- CH02_03. [이론]_The_Second_Dense_Layer.
	- CH02_04. [이론]_Minibatches_in_Dense_Layer.
	- CH02_05. [구현]_Dense_Layers
	- CH02_06. [구현]_Cascaded_Dense_Layers
	- CH02_07. [구현]_Model_Implementation_with_Dense_Layers
### Lecture.3 Sigmoid and Softmax
	- CH03_01. [이론]_Logit_and_Sigmoid
	- CH03_02. [이론]_Softmas_Layer
	- CH03_03. [구현]_Binary_Classifiers
	- CH03_04. [구현]_Multiclass_Classifiers
### Lecture.4 Loss Function
	- CH04_01. [이론]_Mean_Squared_Error
	- CH04_02. [이론]_Binary_Cross_Entropy
	- CH04_03. [이론]_Categorical_Cross_Entropy
	- CH04_04. [구현]_Toy_Datasets_for_Regression_and_Binary_Classification
	- CH04_05. [구현]_Toy_Datasets_for_Multiclass_Classification
### Lecture.5 Convolutional Layers
	- CH05_01. [이론]_image_Tensors_and_Classical_Correlation.
	- CH05_02. [이론]_Computations_of_Conv_Layers.
	- CH05_03. [이론]_Conv_Layers_for_Multichannel_Input
	- CH05_04. [구현]_Conv2D_Layers
	- CH05_05. [구현]_Conv2D_with_Filters
	- CH05_06. [구현]_Model_Implementation_with_Conv2D_Layers
### Lecture.6 Padding Layers
	- CH06_01. [이론]_Pooling_Layers
	- CH06_02. [구현]_Max_and_Average_Pooling_Layers
	- CH06_03. [구현]_Padding_and_Strides
### Lecture.7 Convolutional Neural Network
	- CH07_01. [이론]_Convolutional Neural Network
	- CH07_02. [구현]_Shapes_in_CNNs.
	- CH07_03. [구현]_CNN_Implementation
	- CH07_04. [구현]_LeNet_Implementation
	
	
## Ch.02 Jacobian Matrix와 Backpropagation

### Lecture.0 Orientation
	- CH00_01. Orientation
### Lecture.1 Why Bacpropagation and Jacobians?
	- CH01_01. Trainable_Variables_and_Gradients
	- CH01_02. Gradient-based_Learning_Implementation
	- CH01_03. Backpropagation
	- CH01_04. Why_Jacobian_Matrices
### Lecture.2 Basic Differentiation
	- CH02_01. Rate_of_Changes
	- CH02_02. Differentiation_and_Derivatives
	- CH02_03.Diff_of_Constant_and_Power_Functions
	- CH02_04. Diff_of_Log_and_Exp_Functions
	- CH02_05. Diff_of_Trigonometric_and_Piece-wise_Defined_Functions
	- CH02_06. Constant_Multiplie_and_Sum_Rules
	- CH02_07. LTI_System_and_Differentiation
	- CH02_08. Product_and_Quotient_Rules
	- CH02_09. Composite_Functions_and_Chain_Rule
	- CH02_10.Backpropagation_Modules
### Lecture.3 Multivariate Functions and Jacobians
	- CH03_01.Multivariate_Functions
	- CH03_02.Partial_Derivatives_and_Parameter_Updates
	- CH03_03.Partial_Derivatives_and_Gradients
	- CH03_04.Gradient_and_Parameter_Update
	- CH03_05.Jacobians_of_Affine_Functions
	- CH03_06.Artificial_Neuron_and_Backpropagation
	- CH03_07.Jacobians_of_Minibatches
	- CH03_08.Jacobians_of_MSE_and_BCEE
	- CH03_09.Jacobians_of_CCEE
	- CH03_10.Jacobians_of_Softmax
### Lecture.4 Linear and Logistic Regression1
	- CH04_01.Linear_Regression_(Theory)
	- CH04_02.Linear_Regression_(Implementation, 1 Feature)
	- CH04_03.Linear_Regression_(Implementation, N Feature)
	- CH04_04.Logistic_Regression_(Theory)_and_Sigmoids_Params
	- CH04_05.Properties_of_Logistic_Regression
	- CH04_06.Logistic_Regression_(Implementation, 1 Feature)
	- CH04_07.Logistic_Regression_(Implementation, n Feature)
### Lecture.5 Vector Functions and Jacobians

	- CH05_01.Vector_Functions
	- CH05_02.Jacobians_of_Vector_Functions
	- CH05_03.Affine_Functions_as_a_Vector_Function1
	- CH05_04.Affine_Functions_as_a_Vector_Function2
	- CH05_05.Jacobians_of_Softmax
### Lecture.6 Element-Wise Operations and Jacobians
	- CH06_01.Diagonal_Matrices
	- CH06_02.Unary_Element-wise_Operations
	- CH06_03.Jacobians_of_Activation_Functions
	- CH06_04.Backpropagation_within_Dense_Layers
	- CH06_05.Artificial_Neuron_and_Mini-batches
	- CH06_06.Binary_Element-wise_Operations
	- CH06_07.Backpropagation_within_Loss_Functions
### Lecture.7 Linear and Logistic Regression2
	- CH07_01.Linear_Regression_with_Mini-batches_Theory
	- CH07_02.Linear_Regression_with_Mini-batches_Implementation
	- CH07_03.Logistic_Regression_with_Mini-batches

### Lecture.8 Total Derivatives
	- CH08_01.Multipath_of_Functions
	- CH08_02.Total_Derivatives1
	- CH08_03.Total_Derivatives2
	- CH08_04.Vector_Functions_and_Total_Derivative
	- CH08_05.Linear_Logistics_Regression_and_Total_Derivatives
 

### Lecture.9 Expansion of Jacobians

	- CH09_01.Introduction_to_Expanded_Jacobians
	- CH09_02.Keypoints_to_Expanded_Jacobians
	- CH09_03.Unary_Element-wise_Operations_and_Expanded_Jacobians 
	- CH09_04.Binary_Element-wise_Operations_and_Expanded_Jacobians
  
### Lecture.10 Expanded Jacobians in Deep Learning

	- CH10_01.MSE_BCEE_and_Expanded_Jacobians
	- CH10_02.CCEE_and_Expanded_Jacobians
	- CH10_03.Softmax_and_Expanded_Jacobians
	- CH10_04.Matrix_Multiplication_Revisited
	- CH10_05.Matrix_Multiplication_and_Expanded_Jacobians
	- CH10_06.Bias_Addition_and_Expanded_Jacobians
	
	
### Lecture.11 Application of Expanded Jacobians

	- CH11_01.MLP_Theory
	- CH11_02.Training_MLP_Using_Expanded_Jacobians

>> Basic Mathmatics	
# Basic Algebra

## Chap.1 Algebraic Properties
	- CH01_01 . Orientation
	- CH01_02 . Algebraic Properties
	- CH01_03 . Identities and Inverse
	- CH01_04 . Equations
## Chap.2 Sets

	- CH02_01. Sets
	- CH02_02. Usages of Set
	- CH02_03. Cardimality of Sets
	- CH02_04. Inclusion and Exclusion
	- CH02_05. Unary Set Operation
	- CH02_06. Intersections and Unions1
	- CH02_07. Intersections and Unions2
	- CH02_08. Set Differences1
	- CH02_09. Set Differences2
	- Ch02_10. Cartesian Products
	- CH02_11. Partitions
	
>> Part3
# Part3.딥러닝/인공지능의 이해
## Ch 01_인공지능에 대한 이해
	- CH01_01_AI_Machine Learning
	- CH01_02_Data
	- CH01_03_Artificial Neural Network
	- CH01_04_Training Neural Network
	- CH01_05_Historical Review of Deep Learning
## Ch 02_딥러닝 개발 준비
	- CH02_01. Anaconda, Tensorflow, Pytorch 설치하기, Colab Jupyter Notebook 사용법
	- CH02_02. Numpy Tutorial 1
	- CH02_03. Numpy Tutorial 2
	- CH02_04. Numpy Tutorial 3
	- CH02_05. Data 시각화 - Matplotlib 1
	- CH02_06. Data 시각화 - Matplotlib 2
>> Part4
# Part4. 딥러닝 대표 3대장 프레임워크 기초
## Ch 03_텐서플로우/케라스 이론 및 실습
	- CH03_01. Tensorflow Keras Basic
	- CH03_02. Data pipeline
	- CH03_03. Model
	- CH03_04. Training Validation
	- CH03_05. Model save & restore Tensorboard
## Ch 04_파이토치 이론 및 실습
	- CH04_01. Pytorch Basic
	- CH04_02. Dataset & DataLoader Model
	- CH04_03. Training Validation Model save &restore
	- CH04_04. Tensorboard
>> Part5
# Part5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘
## Ch 01. ML기초
	- CH01_01. AI vs 머신러닝 vs 딥러닝
	- CH01_02.기계학습의 종류
	- CH01_03.선형회귀, 로지스틱회귀, log-likelihood
	- CH01_04.기계학습으로문제를해결하는일반적인순서
	- CH01_05.[TheorySession1]경험적위험도(empirical risk)와ML의일반화(generalization)
	- CH01_06.[TheorySession2]BiasVarianceTrade-off
	- CH01_07.[TheorySession3]정보이론
	- CH01_08.[TheorySession4]CrossEntropy와MaximumLikelihood Estimation(MLE)
	
## Ch 02. Feedforward Network
	- CH02_01.Feedforward Network
	- CH02_02.[실습0]-GoogleColab
	- CH02_03.[실습1]-MLP구현(Pytorch)
	- CH02_03-TF-01.MLP구현part1[PracticeSession1]
	- CH02_03-TF-02.MLP구현part2[PracticeSession1]
	- CH02_04.[TheorySession1]역전파
	- CH02_05.[TheorySession2]왜피드포워드네트워크는충분히잘될까
	- CH02_06.[심화학습]참고하면좋은자료들
	
## Ch 03. Regularization
	- CH03_01.Regularization소개
	- CH03_02.Norm기반Regularization
	- CH03_03.앙상블
	- CH03_04-Regularization-Dropout
	- CH03_05-Regularization-Early-stopping
	- CH03_06-Regularization-파라미터공유
	- CH03_07-Regularization-Multi-taskLearning
	- CH03_08-Regularization-AdversarialLearning
	- CH03_09-Regularization-DataAugmentation
	- CH03_10-Regularization-Dropout구현[PracticeSession1]
	- CH03_11-Regularization-Early-stopping구현[PracticeSession2]
	- CH03_10~11-TF-01.Dropout및Early-stopping구현[PracticeSession1-2]
	- CH03_12-Regularization-L2-norm=Early-stopping[TheorySession1]
	- CH03_13-Regularization-DeepDoubleDescent현상과Regularization[TheorySession2]
	- CH03_14-Regularization-[심화학습]참고하면좋은자료들
	


