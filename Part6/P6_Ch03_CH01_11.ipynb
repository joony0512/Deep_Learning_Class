{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1nC4_Mo64It559GUK57iHuud5nNJJ1Gif",
      "authorship_tag": "ABX9TyOwIYmEZJHPhLR8r1f/vfVY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joony0512/Deep_Learning_Class/blob/main/Part6/P6_Ch03_CH01_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 오픈 도메인 대화시스템 실습\n",
        "## 생성기반 방식 TEXT -> Encoder -> Decoder -> TEXT (End2End)"
      ],
      "metadata": {
        "id": "ZVorYnORMueb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CldeeLMLOyUQ",
        "outputId": "4398ca81-780c-42ad-bd0e-42d0e6e882e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install pytorch-crf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ3J-o69NGgj",
        "outputId": "f3a64f24-0349-42b9-a377-6f4553d32d6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrUECNSEMhhx",
        "outputId": "e1776dc4-3e80-49f1-dfb8-04d047e125b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/딥러닝_정주행/P6_Ch03.자연어처리실습_챗봇\n",
            "/root\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "%cd /content/drive/MyDrive/딥러닝_정주행/P6_Ch03.자연어처리실습_챗봇\n",
        "from src.model import Tformer, save\n",
        "from src.dataset import Preprocessing, MakeDataset\n",
        "%cd /root"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class E2E_dialog:\n",
        "    def __init__(self, dataset, model_path):\n",
        "        self.vocab = dataset.transformers_tokenizer\n",
        "        self.vocab_size = dataset.transformers_tokenizer.vocab_size()\n",
        "\n",
        "        self.model = Tformer(num_tokens=self.vocab_size, dim_model=256, num_heads=8, dff=512, num_layers=2, dropout_p=0.1)\n",
        "        # device = torch.device('cpu')\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        self.model.eval()\n",
        "        self.MAX_LENGTH = 50\n",
        "\n",
        "    def preprocess_sentence(self, sentence):\n",
        "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "        sentence = sentence.strip()\n",
        "        return sentence\n",
        "\n",
        "    def evaluate(self, sentence):\n",
        "        sentence = self.preprocess_sentence(sentence)\n",
        "        input = torch.tensor([[2] + self.vocab.encode_as_ids(sentence) + [3]])\n",
        "        output = torch.tensor([[2]])\n",
        "\n",
        "        # 디코더의 예측 시작\n",
        "        ps = []\n",
        "        for i in range(self.MAX_LENGTH):\n",
        "            src_mask = self.model.generate_square_subsequent_mask(input.shape[1])\n",
        "            tgt_mask = self.model.generate_square_subsequent_mask(output.shape[1])\n",
        "\n",
        "            src_padding_mask = self.model.gen_attention_mask(input)\n",
        "            tgt_padding_mask = self.model.gen_attention_mask(output)\n",
        "\n",
        "            predictions = self.model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1)\n",
        "            # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "            predictions = torch.softmax(predictions.view(-1).cpu(), dim=0)\n",
        "            predictions = torch.max(predictions, axis = -1)\n",
        "            predicted_p = predictions.values\n",
        "            ps.append(predicted_p)\n",
        "            predicted_id =predictions.indices.view(1,1)\n",
        "\n",
        "\n",
        "            # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "            if torch.equal(predicted_id[0][0], torch.tensor(3)):\n",
        "                break\n",
        "\n",
        "            # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
        "            # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
        "            output = torch.cat([output, predicted_id], axis=1)\n",
        "\n",
        "        return torch.squeeze(output, axis=0).cpu().numpy(), (sum(ps)/len(ps)).detach().numpy()\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        prediction, predicted_sentence_p = self.evaluate(sentence)\n",
        "        predicted_sentence = self.vocab.Decode(list(map(int,[i for i in prediction if i < self.vocab_size])))\n",
        "\n",
        "        print('Input: {}'.format(sentence))\n",
        "        print('Output: {}'.format(predicted_sentence))\n",
        "\n",
        "        return predicted_sentence, predicted_sentence_p"
      ],
      "metadata": {
        "id": "XHjPIGwUMxPd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chitchat_pretrain_path = \"/content/drive/MyDrive/딥러닝_정주행/P6_Ch03.자연어처리실습_챗봇/data/pretraining/save/4_chitchat_transformer_model/chitchat_transformer_1.215381_steps_81.pt\""
      ],
      "metadata": {
        "id": "q8LkHG2KOVVa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "dataset = MakeDataset()\n",
        "e2e = E2E_dialog(dataset,chitchat_pretrain_path)"
      ],
      "metadata": {
        "id": "L8WS-s_fOadh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "s, p = e2e.predict(\"난 뭘 해야 할까?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz9AZM9wOdHw",
        "outputId": "75c95dad-0cd4-4f1d-9a5f-09b933ad4020"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 난 뭘 해야 할까?\n",
            "Output: 모르는 사실에 더 화가 나 있을 거예요.\n",
            "CPU times: user 135 ms, sys: 0 ns, total: 135 ms\n",
            "Wall time: 156 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-uMbR38PQXF",
        "outputId": "f3a20888-fca0-4562-dd01-31609e2cdb6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40460020303726196"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EO31gFqPPWtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}