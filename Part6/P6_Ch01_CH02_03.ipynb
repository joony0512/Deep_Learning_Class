{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joony0512/Deep_Learning_Class/blob/main/Part6/P6_Ch01_CH02_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5de32b72",
      "metadata": {
        "id": "5de32b72"
      },
      "source": [
        "# DL development pipeline\n",
        "\n",
        "### Data Loader -> Model Definition -> Training Script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FsBHBaJRw6g",
        "outputId": "f23bf47c-5ec4-4970-8f21-45e1b1ee91ec"
      },
      "id": "6FsBHBaJRw6g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a27aa8e2",
      "metadata": {
        "id": "a27aa8e2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CygcYS9BU_ji",
        "outputId": "24ca0916-3235-4437-c111-11459021a6cd"
      },
      "id": "CygcYS9BU_ji",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e8ffcd",
      "metadata": {
        "id": "b7e8ffcd"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4f5998",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4f5998",
        "outputId": "94e815ad-c8d5-4e1c-ef67-11b4542fc107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 128, 128, 64)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 128, 128, 64)      256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 64, 64, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " global_max_pooling2d (Glob  (None, 128)               0         \n",
            " alMaxPooling2D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 277569 (1.06 MB)\n",
            "Trainable params: 277185 (1.06 MB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def get_sequential_model(input_shape):\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            # Input\n",
        "            layers.Input(input_shape),\n",
        "\n",
        "            # 1st Conv block\n",
        "            layers.Conv2D(64, 3, strides = 1, activation = 'relu', padding ='same'), # padding ='same'으로 이미지 in, out 같은사이즈로 유지\n",
        "            layers.Conv2D(64, 3, strides = 1, activation = 'relu', padding ='same'),\n",
        "            layers.MaxPool2D(),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.5),\n",
        "\n",
        "            # 2nd Conv block\n",
        "            layers.Conv2D(128, 3, strides = 1, activation = 'relu', padding ='same'), # 위 block의 maxpooling으로 이미지 사이즈 절반으로 줄고 heuristic하게 filter size 2배로 올림\n",
        "            layers.Conv2D(128, 3, strides = 1, activation = 'relu', padding ='same'),\n",
        "            layers.MaxPool2D(),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            # Classfier\n",
        "            layers.GlobalMaxPool2D(), # 필터별 가장 큰값 반환 -> 필터수만큼 대표값 반환\n",
        "            layers.Dense(128, activation = \"relu\"),\n",
        "            layers.Dense(1, activation = \"sigmoid\"), # 이진분류\n",
        "\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "input_shape = (256, 256, 3)\n",
        "model = get_sequential_model(input_shape)\n",
        "\n",
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = 'accuracy'\n",
        "\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3867eff1",
      "metadata": {
        "id": "3867eff1"
      },
      "source": [
        "### dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a347966",
      "metadata": {
        "id": "6a347966"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size,\n",
        "        csv_path,\n",
        "        fold,\n",
        "        image_size,\n",
        "        mode ='train',\n",
        "        shuffle = True):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.fold = fold\n",
        "        self.mode = mode\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.df = self.df[self.df['fold'] != self.fold]\n",
        "        elif self.mode =='val':\n",
        "            self.df = self.df[self.df['fold'] == self.fold]\n",
        "\n",
        "\n",
        "        ### Remove invalid files\n",
        "        ### https://github.com/tensorflow/models/issues/3134\n",
        "        invalid_filenames = [\n",
        "            'Egyptian_Mau_14',\n",
        "            'Egyptian_Mau_139',\n",
        "            'Egyptian_Mau_145',\n",
        "            'Egyptian_Mau_156',\n",
        "            'Egyptian_Mau_167',\n",
        "            'Egyptian_Mau_177',\n",
        "            'Egyptian_Mau_186',\n",
        "            'Egyptian_Mau_191',\n",
        "            'Abyssinian_5',\n",
        "            'Abyssinian_34',\n",
        "            'chihuahua_121',\n",
        "            'beagle_116'\n",
        "        ]\n",
        "        self.df = self.df[~self.df['filename']. \\\n",
        "                         isin(invalid_filenames)]\n",
        "\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.df)/self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        strt = idx * self.batch_size\n",
        "        fin = (idx + 1)* self.batch_size\n",
        "        data = self.df.iloc[strt:fin]\n",
        "\n",
        "        batch_x, batch_y = self.get_data(data)\n",
        "\n",
        "        return np.array(batch_x), np.array(batch_y)\n",
        "\n",
        "    def get_data(self, data):\n",
        "        batch_x =[]\n",
        "        batch_y =[]\n",
        "\n",
        "        for _, r in data.iterrows():\n",
        "            file_name = r['filename']\n",
        "\n",
        "            image = cv2.imread(f'/content/drive/MyDrive/딥러닝 정주행/P6_Ch01.이미지처리실습/data/images/{file_name}.jpg')\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # input image resize (배치로 묶기위해)\n",
        "            image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "\n",
        "            # rescaling\n",
        "            image = image /255.\n",
        "\n",
        "            # 고양이 1 강아지 2 -> 고양이 0 강아지 1\n",
        "            label = int(r['species']) -1\n",
        "\n",
        "            batch_x.append(image)\n",
        "            batch_y.append(label)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "\n",
        "\n",
        "    def on_epoch_end(self): # callback function\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac =1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ae28d1",
      "metadata": {
        "id": "49ae28d1"
      },
      "outputs": [],
      "source": [
        "csv_path = '/content/drive/MyDrive/딥러닝 정주행/P6_Ch01.이미지처리실습/kfolds.csv'\n",
        "train_generator = DataGenerator(\n",
        "    batch_size = 128,\n",
        "    csv_path =csv_path,\n",
        "    fold = 1,\n",
        "    image_size = 256,\n",
        "    mode ='train',\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "val_generator = DataGenerator(\n",
        "    batch_size = 128,\n",
        "    csv_path =csv_path,\n",
        "    fold = 1,\n",
        "    image_size = 256,\n",
        "    mode ='val',\n",
        "    shuffle = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0e2b20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0e2b20",
        "outputId": "b063bb3a-4aaa-4216-d6c5-2db1462630b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "19/46 [===========>..................] - ETA: 18:38 - loss: 1.8657 - accuracy: 0.6094"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data = val_generator,\n",
        "    epochs = 10,\n",
        "    callbacks = [\n",
        "        early_stopping,\n",
        "        reduce_on_plateau,\n",
        "        model_checkpoint\n",
        "    ],\n",
        "    verbose =1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc13c20",
      "metadata": {
        "id": "ffc13c20"
      },
      "source": [
        "## Callback functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "839e9388",
      "metadata": {
        "id": "839e9388"
      },
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss', patience = 3, verbose =1,\n",
        "    mode = 'min', restore_best_weights = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4378a1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4378a1f",
        "outputId": "bbc915a4-e6d3-45d4-e211-2b8338a1ff34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Learning rate reduction mode train is unknown, fallback to auto mode.\n"
          ]
        }
      ],
      "source": [
        "reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor = 'val_loss', factor = 0.1, patience =10, verbose = 1,\n",
        "    mode ='train', min_lr = 0.001\n",
        ") # 일종의 learning rate scheduler -> 성능이 나아지지 않으면 learning rate 1/10으로 줄임, 가장 줄인값 min_lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460f687d",
      "metadata": {
        "id": "460f687d"
      },
      "outputs": [],
      "source": [
        "# checkpoint 생성\n",
        "file_path = '{epoch:02d}-{val_loss:.2f}.hdf5'\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    file_path, monitor = 'val_loss', verbose =1, save_best_only =True,\n",
        "    save_weights_only =False, mode ='min'\n",
        ")\n",
        "# val_loss 작아질때마다 저장, save_weights_only =False 모델 구조까지 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6004b5f",
      "metadata": {
        "id": "b6004b5f"
      },
      "outputs": [],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history = history.history\n",
        "\n",
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history['loss'], label ='train')\n",
        "plt.plot(history['val_loss'], label = 'val')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history['accuracy'], label ='train')\n",
        "plt.plot(history['val_accuracy'], label = 'val')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.show\n"
      ],
      "metadata": {
        "id": "3t6cp-w2TDyX"
      },
      "id": "3t6cp-w2TDyX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S_ZSE40HYAWK"
      },
      "id": "S_ZSE40HYAWK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}