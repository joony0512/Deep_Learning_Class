## Deep Reinforcement Learning - 2. Reinforcement Learning의 핵심 개념 - Part 2
-> Expected Return을 최대화 하는 policy을 고르는 것!  

예를 들어, 만약 transition과 policy가 둘다 stochastic하여, T-step의 trajectory가 아래 식과 같이 정리 될때,
  
<img width="300" alt="스크린샷 2023-08-29 오후 6 28 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c42d0c41-0cf3-4878-a6d9-79e9cabb847e">

expected return 𝐽(𝜋)는 아래와 같이 정의된다.  <img width="312" alt="스크린샷 2023-08-29 오후 6 29 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ca991ec5-5a5d-44c8-9182-95035e9a5ab7">

이때 RL의 보편적인 최적화 문제는 아래와 같이 정리될 수 있다.

<img width="150" alt="스크린샷 2023-08-29 오후 6 29 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8500680e-af2c-4658-8fe1-9b64af46ff1f">

로 표현될 수 있고, $𝜋^*$ 을 optimal policy이라고 한다!

## 강화 학습(reinforcement learning; RL) - 가치 함수(value function)와 그 종류
State 또는 state-action 쌍의 value을 아는 것은 RL 문제에서 유용!   
Value란? 
  - state 또는 state-action 쌍에서 시작하여 이후 계속 특정 policy에 따라 action을 할 경우 expected return을 의미!

Value function은 대부분의 RL 알고리즘에서 널리 사용!
  - 대표적인 4종류의 value function은 아래와 같다.
1. On-policy value function: $𝑉^\pi (𝑠)$
2. On-policy action-value function: $𝑄^\pi (𝑠,𝑎)$
3. Optimal value function: $𝑉^* (𝑠)$
4. Optimal action-value function: $𝑄^* (𝑠,𝑎)$
