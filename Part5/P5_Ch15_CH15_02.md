## Deep Reinforcement Learning - 2. Reinforcement Learning의 핵심 개념 - Part 2
-> Expected Return을 최대화 하는 policy을 고르는 것!  

예를 들어, 만약 transition과 policy가 둘다 stochastic하여, T-step의 trajectory가 아래 식과 같이 정리 될때,
  
<img width="300" alt="스크린샷 2023-08-29 오후 6 28 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c42d0c41-0cf3-4878-a6d9-79e9cabb847e">

expected return 𝐽(𝜋)는 아래와 같이 정의된다.  <img width="312" alt="스크린샷 2023-08-29 오후 6 29 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ca991ec5-5a5d-44c8-9182-95035e9a5ab7">

이때 RL의 보편적인 최적화 문제는 아래와 같이 정리될 수 있다.

<img width="150" alt="스크린샷 2023-08-29 오후 6 29 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8500680e-af2c-4658-8fe1-9b64af46ff1f">

로 표현될 수 있고, $𝜋^*$ 을 optimal policy이라고 한다!

## 강화 학습(reinforcement learning; RL) - 가치 함수(value function)와 그 종류
State 또는 state-action 쌍의 value을 아는 것은 RL 문제에서 유용!   
Value란? 
  - state 또는 state-action 쌍에서 시작하여 이후 계속 특정 policy에 따라 action을 할 경우 expected return을 의미!

Value function은 대부분의 RL 알고리즘에서 널리 사용!
  - 대표적인 4종류의 value function은 아래와 같다.
1. On-policy value function: $𝑉^\pi (𝑠)$
2. On-policy action-value function: $𝑄^\pi (𝑠,𝑎)$
3. Optimal value function: $𝑉^* (𝑠)$
4. Optimal action-value function: $𝑄^* (𝑠,𝑎)$

## 강화 학습(reinforcement learning; RL) - 가치 함수(value function)와 그 종류
On-policy value function: $𝑉^\pi (𝑠) = 𝐸_{𝜏 \sim \pi} [𝑅 (𝜏) | 𝑠_0 = 𝑠] $
  - State(s)에서 시작하여 항상 policy(𝜋)에 따라 action을 고르기 위한 expected return을 제공할 경우
    
On-policy action-value function: $𝑄^\pi (𝑠, 𝑎) = 𝐸_{𝜏 \sim \pi} [𝑅(𝜏)| 𝑠_0 = 𝑠, 𝑎_0 = 𝑎]$
  - State(s)에서 시작하고, 임의의 action(a)을 고르고(policy에 안 따를 수도 있음), 이후로는 policy에 따라 action을 고르기 위한 expected return을 제공할 경우  

Optimal value function: $𝑉^* (𝑠) = max_\pi 𝐸_{𝜏 \sim \pi} [𝑅 (𝜏) | 𝑠_0 = 𝑠]$
  - 상태 s에서 시작하고 항상 환경에서의 optimal한 policy에 따라 action 을 고르기 위한 expected return을 제공할 경우
    
Optimal action-value function: $𝑄^* (𝑠, 𝑎 )= max 𝐸_{𝜏 \sim \pi} [𝑅(𝜏)|𝑠_0 = 𝑠, 𝑎_0 = 𝑎]$
  - State(s)에서 시작하고, 임의의 action(a)을 고르고(policy에 안 따를 수도 있음), 이후로는 optimal한 policy 에 따라 action을 고르기 위한 expected return을 제공할 경우


## 강화 학습(reinforcement learning; RL) - Optimal Q-function과 optimal action
Optimal action-value function: $𝑄^* (𝑠, 𝑎 )= max 𝐸_{𝜏 \sim \pi} [𝑅(𝜏)|𝑠_0 = 𝑠, 𝑎_0 = 𝑎]$ 와 optimal policy에 의해 선택된 Action은 서로 깊은 관련이 있다!
  - $𝑄^* (𝑠, 𝑎)$ 는 state(s)에서 시작하고, 임의의 action(a)을 고르고(policy에 안 따를 수도 있음), 이후로는 optimal한 policy 에 따라 action을 고르기 위한 expected return을 제공할 경우를 나타낸다.
  - 이때, state s에서의 optimal policy는 s에서 시작하여 예상되는 수익(expected return)을 최대화하는 작업을 선택한다.
  - 즉 , 만약  Q* 가 있다면 아래의 과정으로 직접 최적의 action인 $𝑎*(𝑠)$  를 아래의 식으로 얻을 수 있다!
      - 𝑎∗ 𝑠 = argmax 𝑄∗ (𝑠,𝑎)
  - 참고: 𝑄∗ (𝑠, 𝑎) 을 최대화하는 방법은 여러 가지 action이 될 수 있으며, 그 경우, random으로 하나를 고른다. 그러나, deterministic하게 action을 고르는 optimal policy는 보통 유일하다.

## 강화 학습(reinforcement learning; RL) - 잠깐! Policy에 안 따를 수 있다는 말은?
- 𝑄∗ (𝑠, 𝑎) 는 state(s)에서 시작하고, 임의의 action(a)을 고르고(policy에 안 따를 수도 있음)
- 보통 exploration을 촉진하기 위함 !
  - I.e. epsilon-greedy : 일정 확률로, 랜덤 action을 고른다!
  
  <img width="200" alt="스크린샷 2023-08-29 오후 7 37 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bb8131b9-fca2-48ab-b9f5-504f384e9d64">

  <img width="200" alt="스크린샷 2023-08-29 오후 7 37 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/eea7b705-44aa-49ca-b68c-04c22336a145">

## 강화 학습(reinforcement learning; RL) - Bellman equations
- 앞서 소개한 value function들을 최적화 하는 것은 Bellman equation의 형태로 optimal한 해답이 존재한다!
- Bellman equation의 기본 아이디어:
  - “출발점의 가치”는 그 곳에 있음으로써 얻을 수 있는 reward와 다음에 이동하여 착륙하는 곳의 가치의 합일것이다.
  - (현재와 바로 다음 state을 고려하자!)
    
- On-policy 가치함수들로 bellman equation을 써보면,

  <img width="350" alt="스크린샷 2023-08-29 오후 7 47 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/632508ec-ff28-4084-9f48-dad482209522">

  - 여기서, $𝑠' \sim 𝑃( \centerdot |𝑠, 𝑎)$ 는 environment transition rule에 의해 sampling 된 next state를 의미한다.
 
## 강화 학습(reinforcement learning; RL) - Bellman equations
On-policy 가치 함수들의 Bellman equation <img width="350" alt="스크린샷 2023-08-29 오후 7 47 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/632508ec-ff28-4084-9f48-dad482209522">

에서, optimal 가치 함수에 대한 bellman equation은 <img width="350" alt="스크린샷 2023-08-29 오후 7 49 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c41bfe89-4a49-4578-8116-1b468332bb6f">
이다.

On-policy 가치 함수와 optimal 가치 함수에 대한 bellman equation의 가장 큰 차이는 max action term의 존재 유무이다!
  - max action: 에이전트가 자신의 행동을 선택하게 될 때마다, 최적으로 행동하기 위해서, 가장 높은 가치로 이끄는 행동을 선택해야 한다는 사실을 반영한다.

## 강화 학습(reinforcement learning; RL) - Bellman backup
Bellman backup: Bellman equation의 오른쪽 부분(reward + next value) 를 의미한다.(파란 부분) 
- On-policy 가치 함수들의 Bellman equation <img width="350" alt="스크린샷 2023-08-29 오후 7 50 57" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/da3e191d-7fd4-493e-8a90-7704c059370d">

- Optimal 가치 함수에 대한 Bellman equation <img width="350" alt="스크린샷 2023-08-29 오후 7 51 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9f126bca-baff-433d-bd05-646d233e81f9">

## 강화 학습(reinforcement learning; RL) - Advantage function
RL에서 특정 action 이 얼마나 좋은지를 정량적으로 나타내려면 ? 
  - Advantage function! (절대적이기보다, 상대적인 방법!)

정책𝜋에 해당하는 advantage function $𝐴^\pi (𝑠,𝑎)$ : 현재 state(s)에서 무작위로 행동을 선택하는 것에 비해, 특정한 action(a)를 취하는 것이 얼마나 더 나은지를 나타낸다.  
  - Policy gradient 방법들에서 필요!

Advantage function는 다음과 같이 정의된다.
  - $𝐴^\pi (𝑠,𝑎) = Q^\pi (s,a) - V^\pi (s)$

## 강화 학습(reinforcement learning; RL) - Markov decision process과 RL의 formalization
RL에 엄밀한 정의는 Markov decision process(MDP)을 통해 정의된다 ! 
  - 현재의 정보에 집중하는 것만으로도 decision 이 가능(decision 이 포함된 Markov reward process)
Markov property 는 아래를 만족할 때를 말한다. (RNN에서 autoregressive model을 압축하는 가정과 비슷)
  - $𝑃[𝑆_{t+1}| S_t] = 𝑃[𝑆_{t+1}|𝑆_1, ... , S_t]$
    
Markov process (Markov chain)은 <S, P>의 tuple로 구성되고,
  - S: state의 유한한 집합(set)
  - P: S -> P(S) state transition probability function로, 𝑃′ = $𝑃[𝑆_{t+1} = s' | S_t = s]$ 을 의미한다.
    
Markov reward process(MRP)는 Markov chain의 value(가치)를 추가한 것 <S, P, R, 𝛾> 
  - R:S->R(S) reward function로, 𝑅 = 𝐸[ $𝑅_{t+1} | 𝑆_t = 𝑠$ ]
  - 𝛾: discount factor로, 𝛾 ∈ [0, 1] 이다.

## 강화 학습(reinforcement learning; RL) - Markov decision process과 RL의 formalization
RL에 엄밀한 정의는 Markov decision process(MDP)을 통해 정의된다 !
- 현재의 정보에 집중하는 것만으로도 decision 이 가능(decision 이 포함된 Markov reward process)
Markov Decision Process는 <S, P, A, R, 𝛾>로,
  - S: 모든 유효한 state들에 대한 집합
  - A: 모든 유효한 action들에 대한 집합
  - R:S X A X S -> R : 은 reward함수로 , 𝑅 = 𝐸[ $𝑅_{t+1}| 𝑆_t = 𝑠,𝐴_t =𝑎, 𝑆_{t+1} =𝑠′$ ]이다.
  - P: S X A -> P(S): 는 transition probability 함수로, 𝑃′ = 𝑃[ $𝑆_{t+1} =𝑠′|𝑆_t = 𝑠, 𝐴_t = 𝑎$ ]이다.
  - 𝛾: discount factor로, 𝛾 ∈ [0, 1] 이다.

여기에 종종 start state distribution ($𝜌_0$)을 추가하여 <S, P, A, R, 𝛾, $𝜌_0$>로 표현하기도 하고, discount factor를 생략하기도 한다.

## 강화 학습(reinforcement learning; RL) - MDP의 최적화?
Dynamic programming, Monte-Carlo 방법, temporal difference 방법 등으로 최적화가 가능하다. 
  - 위 3가지 방법은 장단점이 존재한다.
    
1. Dynamic programming: 마르코프 결정 프로세스(MDP)로서 환경의 완벽한 모델이 주어진 최적의 정책을 계산하는 데 사용될 수 있는 알고리즘의 모음으로, policy iteration과 value iteration방법으로 나뉜다.
  - 수학적으로 잘 개발되었지만 환경을 포함한 MDP의 모든 정보를 알아야하고 정확한 모델이 필요 !
    
2. Monte-Carlo method (model-free control): 완전한 return들에 대해 평균을 내는 식.
  - 모델이 필요하지 않으며 개념적으로 간단하지만 step-by-step 증분(incremental) 계산에는적합하지 않다.
    
3. Temporal difference: MC와 DP를 섞은 것으로서 MC처럼 raw experience로부터 학습할 수 있지만 DP처럼 time step마다 학습할 수 있는 방법
  - 모델이 필요하지 않고 증분(incremental)이 가능하지만, 분석하기 더 복잡하다!
    
## 강화 학습(reinforcement learning; RL) - MDP의 최적화?
Dynamic programming, Monte-Carlo 방법, temporal difference 등  
자세한 내용은 https://dnddnjs.gitbooks.io/rl/content/td_control.html 의 4-8장 참고.  
혹은 Sutton 교수님의 http://incompleteideas.net/book/the-book.html 책의 4-6 장 참고.


## 강화 학습(reinforcement learning; Deep RL)의 종류
<img width="450" alt="스크린샷 2023-08-29 오후 9 03 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/82617e25-d613-481c-b6eb-2f0d7b3b234e">

## 강화 학습의 구분 - Model-free RL vs Model-based RL
RL Agent에 environment에 대한 model의 존재 여부!
- Model을 사용하는 RL을 model-based, model을 갖지 않는 방법을 model-free라고 한다!

모델을 갖는 것의 장단점은?
- 장점  
  - Agent가 미리 생각하고, 가능한 선택의 범위에 대해 어떤 일이 일어날지 보고, 그 옵션들 사이에서 명시적으로 결정함으로써 계획(Planning)을 세울 수 있게 한다는 것!
  - 에이전트는 미리 계획한 결과를 학습된 정책으로 distill 하여 샘플 효율성(sample efficiency)가 크게 향상될 수있다!

- 단점
  - 사용하는 모델이 보통 환경에 대한 실제 모델(ground-truth)이 아니다.
  - Model이 environment를 제대로 반영하지 못한다면, 오류는 그래도 agent의 오류로! à좋은 모델 학습은 좋은 agent를 만드는 것만큼 또는 더 어려울 수도 있다!
  - Model-free는 샘플 효율성을 얻을 수 있는 잠재적 이득을 포기하지만, 구현 및 조정하기가 더 쉬운 경향! 29
