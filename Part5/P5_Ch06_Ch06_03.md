# 순환 신경망 (RNN) - 3. LSTM & GRU
## Long Short-Term Memory (LSTM) - RNN의 문제를 해결하기 위해 1997년 제시
- RNN는 장기 의존성의 문제를 가진다. -> 이를 극복하기 위한 시도 중 하나가 바로 LSTM!
- 대표적인 차이점?
  
  <img width="247" alt="스크린샷 2023-07-13 오후 9 52 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/baf9a7be-0d0d-4177-b6d6-e50d0fcd5ee7">
  <img width="247" alt="스크린샷 2023-07-13 오후 9 52 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/072b0ec3-b998-4aa1-88d8-f14c1ff6bc97">

  - 오랫동안 정보를 기억하는 기능 (cell state를 통과시키는)을 갖는다.
    
    <img width="298" alt="스크린샷 2023-07-13 오후 9 52 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d209216-9217-4c9d-806d-a81cec1834c9">

  - 한편, 이 정보를 없애거나 더하는 gate의 구조 역시 가지고 있다.
    - sigmoid 함수로 이루어져 있고, 곱하는 연산으로 수행한다.
    - forget, input, output gate가 있다.

1. Forget Gate
<img width="573" alt="스크린샷 2023-07-13 오후 9 49 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6b8e02c5-534c-470c-883e-c979a23ac754">

- 이전 Cell state로 부터 정보를 버릴지 말지 결정한다.
  - 이전상태의 hidden state (h_t-1) 와 현재상태의 input (x_t)에 sigmoid함수를 거쳐(0~1)의 값을 뽑는다
  - 0에 가까울 수록 정보가 삭제된것이고 1에 가까울수록 정보를 온전히 기억한다.
 
2. input gate
   
<img width="592" alt="스크린샷 2023-07-13 오후 9 53 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e379c70f-bdbc-409f-850f-a31fe1202662">

- 현재의 새 정보를 cell state에 더한다.
- tanh : -1 ~1의 값을 가지며, 현재 cell state를 나타낸다.
- sigmoid : 0~1의 값을 가지며, gate의 역할 (input)

3. State update
   - 1단계 forget gate의 값 , 2단계의 현재 cell state값, input gate의 값으로 cell state의 업데이트를 한다.
   - forget gate와 input gate의 출력 값이 모두 1이 된다면, 이전 시점의 정보는 완전히 유지되고 현재 시점의 입력도 모두 전달되게 됩니다.
   - 두 게이트의 출력이 모두 0이라면, 이전 시점의 정보는 삭제되고, 현재 시점에서의 입력은 전달되지 않게 됩니다.
  
## Gated Recurrent Unit (GRU; 2014) - 간소화된 LSTM
<img width="447" alt="스크린샷 2023-07-13 오후 10 05 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6a5591ba-950b-4149-af0e-4eac08a508b3">

- GRU에서는 LSTM과 다르게 게이트가 2개로, Reset Gate(r)과 Update Gate(z) 가 있다.
  - LSTM에서의 C(cell state)와 h(hidden state)가 hidden state로 통합
  - Update Gate(z)가 LSTM에서의 forget, input gate를 제어
  - Output gate가 없다.

- Reset Gate
  - Hidden state를 받아 sigmoid처리하고 그 값을 얼마나 반영할지 선택한다.

- Update Gate
  - 이전 상태의 hidden state와 입력 값(x)을 sigmoid처리.
  - -1 연산: 이전 상태를 얼마나 잊을지 (LSTM의 forget gate와 같은 역할)
  - tanh: 현재 상태(cell state)를 얼마나 반영할지 (LSTM의 input gate와 같은 역할)
  - 최종 결과를 다음 상태의 hidden state로 업데이트 한다.

