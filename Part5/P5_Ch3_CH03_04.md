## CH03_04-Regularization-Dropout

### Dropout
![Untitled (17)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/241e953c-a562-4ef2-a6e0-6d9ab4efe2ed)

![Untitled (18)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cb0381fe-b63b-4ee5-ac45-e7e543f77880)


- 학습시, 임의로 미리 정의된 확률대로, 뉴런(노드)을 드롭(작동시키지않음)시킨다.
- 각각의 Layer는 베르누이 확률분포(Beroulli distribution)을 따르는 확률변수 r에따라, 그 뉴런의 결과 값을 덮어씌운다. (키거나 끈다.)
- 드랍아웃이 뉴런들을 지우고 학습함에 따라, 결과적으로 ‘weight’를 공유하는 수많은 네트워크의 조합이 생긴다.
    - n개의 뉴런으로 이루어진 네트워크에서 $2^n$개의 조합이 생긴다
    - 많은 다른 ANN을 한번의 학습으로부터 Bagging하는 효과와 비슷하다
        - overfitting을 막는 효과

### Dropout vs Bagging

1. Bagging은 각 모델이 파라미터를 share하지 않는 반면, Dropout은 모든 모델이 파라미터를 공유(share)한다.
2. Bagging의 경우, 각 모델이 학습됨에 따라 하나의 모델로 수렴하지만, Dropout의 경우, 각각의 모델이 수렴하도록 학습되지 않는다.
3. Bagging은 모델 결과 값의 산술평균을 취할 수 있는것에 비해, Dropout은 지수계수로 증가하는 많은 모델의 평균을 내는 것이기 때문에, 산술평균으로는 계산이 어려워진다.
    
    → 빠른추론을 위해 dropout은 산술평균보다 기하평균(geometric)을 추론하여 근사하는 것에 가깝다.
    

### Dropout의 장점

1. Dropout은 다른 정규화 방법보다 성능이 좋은것으로 알려져 있다.
2. 컴퓨팅계산이 작다. O(n)(n은 노드의 개수)
3. 다른 regularization에 비해, dropout은 모델과 학습과정에서의 종류를 제한하지 않는다→ SGD로 학습하는 거의 그대로 학습시키면된다.

![Untitled (19)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cd968cba-87c6-4b40-92e0-3b16fef2d90c)
![Untitled (20)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/45a510b1-fab8-4768-bd68-e573b208955c)


- Dropout은 피처들끼리 상호적응(co-adaptation)을 하는 것을 막아준다
    - 뉴런들은 다른 뉴런이 그 자리에 없더라도 정상작동해야한다
- Dropout은 결과적으로, 네트워크에 sparse한 활성화를 시켜준다.

### Dropout과 Dropout rate

- 적절히 실험을 돌리며 최적의 Test Error를 찾는다.
- p=0.0에 가까울때 : 너무 과한 Dropout
- p=1.0에 가까울때 : no dropout → overfit
![Untitled (21)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/420b9d62-5aca-4e33-a14f-d8dac7d148d9)


### Dropconnect

- weight를 drop시키는 방법
- 종종 dropout보다 나은경우가 있지만, 항상결과를 보장하지는 않고, 느리고, 상대적으로 튜닝이 어렵다.

![Untitled (22)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e1616297-dd1c-4cc7-84fc-eb8b46b2c07f)
