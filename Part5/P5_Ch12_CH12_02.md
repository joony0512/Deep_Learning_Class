# <Generative model series #4> - Implicit Models - 2. Generative Moment Matching Network 
## Generative Moment Matching Network (GMMN) - Moment matching
Data / model 분포의 모멘트(moment; 적률)을 일치(matching)시켜 두 분포를 가깝게 하면 어떨까?
- 참고: 통계에서 모멘트의 정의: https://dowhati1.tistory.com/17
  - 0에대한n차모멘트:𝐸(𝑋"), 평균에대한n차모멘트:𝐸( 𝑋−𝜇 ")
  - 0에 대한 1차 모멘트: 평균, 평균에 대한 2차 모멘트: 분산, 평균에 대한 3차 모멘트: 비대칭도(skewness)...
    - 가설 검증 실험(hypothesis test)에서 two-sample test방법이라고도 한다.
   
즉,𝑋= { $x_i$ } $\^N_{i=1}$ from𝑃,𝑌= { $y_i$ } $\^M_{j=1}$ from $𝑃_Y$ 에서,
$𝑃_X, 𝑃_Y$ 을 각각 $P_{data}, P_{model}$ 이라고 하면 hypothesis test을 하듯 두 분포의 차이를 통계적으로 알아낼 수 있을
것이다 ! 이것의 제곱을 loss로 쓸 수 있지 않을까. 

<img width="266" alt="스크린샷 2023-08-18 오후 4 26 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/980381f3-4d2b-477f-9060-eacc037e2920">

- 그러나, 만약 high-dimension에서 moment을 계산한다면?
  - 너무 많은 샘플이 필요하다à학습이 거의 되지 않아 못 사용할 가능성이 높다. -> Kernel Trick!
 
## Generative Moment Matching Network (GMMN) - Recap: Kernel Trick
머신 러닝에서의 Kernel Trick ?
- 원공간(input space)의 데이터를 선형 분류가 가능한 고차원 공간(feature space)로 매핑한 뒤 두 범주를 분류하는 초평면을 찾는다.
  - 효과: 고차원 매핑과 내적(inner product) 연산을 한번에 하여 효율적으로 계산. (따로 한다면 연산량이 폭증!)

<img width="245" alt="스크린샷 2023-08-18 오후 4 31 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/11f53142-fd05-455e-8fd3-59c6333920bf">

- $\phi (x) = Ax$  
- $\phi$ 는 input space에서 feature space를 매핑하는 함수

  <img width="208" alt="스크린샷 2023-08-18 오후 6 15 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7672a4b8-0600-408c-aeb0-fdea04688dbf">

-  Kernel trick 은 머신 러닝 문제에서, 높은 차원에서 mean discrepancy(평균 불일치)을 최적화 할 수 있게 한다!
- 커널함수가될수있는예시

  <img width="400" alt="스크린샷 2023-08-18 오후 6 16 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/99b0461a-5e08-45fc-8f27-f5bb99467688">

## Generative Moment Matching Network (GMMN) - 목적함수
Moment matching + kernel trick
- 𝑋= { $x_i$ } $\^N_{i=1}$ from𝑃,𝑌= { $y_i$ } $\^M_{j=1}$ from $𝑃_Y$ 에서,
- $𝑃_X, 𝑃_Y$ 을 각각 $P_{data}, P_{model}$ 이라고 하면 hypothesis test을 하듯 두 분포의 차이를 통계적으로 알아내자.
- + kernel trick (𝜙: 고차원으로의 mapping 함수, k: kernel 함수)
  
  <img width="400" alt="스크린샷 2023-08-20 오후 4 35 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2d79d2cb-9197-45a9-a284-46710328230b">

## 암시적 모델 (Implicit Models)의 종류 - Generative Moment Matching Network (GMMN)
<img width="500" alt="스크린샷 2023-08-20 오후 4 36 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9ae62da7-9b54-4060-bca7-8d7ba5310bf7">  
  
<img width="500" alt="스크린샷 2023-08-20 오후 4 43 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/55fe99cb-9811-45d5-84fb-07a4d50eec25">  
  
<img width="200" alt="스크린샷 2023-08-20 오후 4 44 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/250c9de6-4a7f-4470-8c8a-30b208f8df86">  

- 한계는?
  - Discrepancy measure의 평균을 위해 좋은 kernel이 필요하다. Kernel trick은 만능이 아니다 !
  - kernel이 높은 차원으로 보내는 만큼 overfit이 될 가능성은 항상 존재) MNIST, TFD 데이터 등과 같이 간단한 데이터 외에는 잘 작동하지 않는다.
  - Auto-encoder나 큰 minibatch, mixture of kernel 등 다른 테크닉 동반 필요!
