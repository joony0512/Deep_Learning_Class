# Research Topics for Productions - 1. Model Compression (Quantization, Distillation, Prunning)
## 모델을 최적화 해야하는 이유 - 크기축소
더 작은 저장 크기
  - 작은 모델은 사용자의 기기에서 저장 공간을 적게 차지한다. -> 저장 공간이 작은 모바일 기기에서의 활용성을 높일 수 있다
    
더 작은 다운로드 크기
  - 모델을 사용자의 기기에 다운로드할 때의 소요시간과 대역폭을 줄일 수 있다

메모리 사용량 감소
  - 모델이 작을수록 실행시 더적은 RAM을 사용하므로 애플리케이션의 다른부분에서 사용할 수 있는 메모리를 확보할 수 있고, 성능과 안정성을 향상시킬 수 있다

## 모델을 최적화 해야하는 이유 - 지연 시간(latency) 감소
Research 환경에서는 일반적으로 모델의 성능 평가에 속도(speed)가 들어가지 않는 경우가 많다.  
Real-time production 상황에서는 그러나, 아무리 성능이 좋아도 inference time이 나오지 않으면 production에 나갈 수 없다.  
  -> AI 모델의 속도 문제는 production 상황에서 언제나 중요하다.

## Inference 속도를 높이는 일반적인 방법
<img width="350" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/23ff3ad6-68d9-4d5c-b67b-710b9e0b9d7e">

<img width="350" alt="스크린샷 2023-09-06 오후 7 35 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ca81bdb7-33d8-4dcf-9ea8-1bbc40293fdb">

1. Network의 Forward 로직과 전후 처리 로직이 비효율적인지 점검한다. (https://github.com/rkern/line_profiler)
2. 모델 아키텍처를 교체한다. (x ?배)
   - 더 얕은 모델, Auto-regressiveàSelf-attention 기반의 feed-forward 방식 등  
3. 모델을 서빙하는 Framework / Infra를 바꿔본다.
   - Tensorflow/Pytorch -> ONNX/TensorRT
   - Python -> C++
   - CPU-inference -> GPU-inference pod의 replica 증가

## 모델 용량 감소 + Inference 속도를 높이는 방법 - Model Compression
모델 아키텍처에 대한 아이디어를 최대한 유지하며 압축한다. (model compression)
- Quantization / Half-precision
- Knowledge-distillation (모델 아키텍처가 변경될 수도 있음)
- Pruning (제한적인 환경에서만 적용 가능 ‒ sparse tensor 연산이 가능한 HW, SW가 보편화 되어있지 않음

## Model Compression - 양자화(quantization)
<img width="500" alt="스크린샷 2023-09-06 오후 7 48 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/210609c9-f505-405a-af5a-8a050865a1a4">

- 모델의 파라미터를 lower bit로 표현함으로서 계산과 메모리 access
- 속도를 높이는 경량화 기법
- 모델 weight는 일반적으로 32bit(fp32)의 부동 소수점 자리로 나타나 있다.
  - 16bit float로 줄인다. (half-precision, mixed precision)
  - 8bit 정수로 변환한다. (-127 ~ 127 혹은 0~255)
    - 32bit -> 16bit: 2x memory 감소
    - 32bit -> 8bit: 4x memory 감소

- 단점? 모델 퍼포먼스가 저하될 수 있고, 실제 GPU/TPU inference 에서 적용 불가능한 경우가 존재함.
- Post Training 된 모델을 quantization 하는 Post Training Quantization
  - Training한 후에 quantize를 적용하는 기법.
  - 파라미터 수가 적을 수록 정확도 하락 폭이 크므로, small model 에는 부적합하다.
  - Dynamic Quantization, Static Quantization 방법이 속한다.
  - 학습 이후에 적용가능하여, ML Engineer가 사후 적용가능.
    
- 학습을 통한 quantization을 simulate 하는 Quantization Aware Training
  - 모델의 weight와 activation을 training 중에 양자화한다.
  - Fake quantization module/node를 첨가하여 quantize되었을 시 어떻게 동작할지 시뮬레이션.
    - Clamping(데이터 범위 제한)과 rounding(반올림)을 수행한다!
  - Post-training Dynamic / Static quantization보다 높은 accuracy 활용 가능.
  - 학습 중 적용하여야 하기에, 사후 적용이 불가, 보통 연구자가 수행하여야 함.

- 동적 양자화(dynamic quantization)
  - 데이터의 범위에 따라 활성화 함수에 적용할 양자화의 배율을 동적으로 결정한다.
  - 모델의 weight에 대해서만 양자화 진행 (training 후에 quantize)하고, activation은 inference할 때 동적으로 양자화
  - 모델을 메모리에 로딩하는 속도는 개선할 수 있으나, 연산속도 향상 효과는 미비함
  - Weight가 많은 큰 large-scale transformer와 같은 모델에 효과적
- 정적 양자화(static quantization)
  - 모델의 weight와 activation을 사전에 정량화한다.
  - 가능한 경우 activation을 선행 레이어(batch norm, convolutional layer 등)에 fusion한다.
    - Sequential 한 연산을 병렬화 할 수 있다! -> 연산 속도 향상
  - 활성화를 위한 최적의 정량화 매개 변수를 결정하려면 대표적인 dataset을 사용한 보정이 필요하다.
    - Calibration이 필요.
  - 일반적으로 메모리와 컴퓨팅 자원 절약이 모두 중요할 때 사용되며 CNN은 일반적인 사용 사례

<img width="500" alt="스크린샷 2023-09-06 오후 8 01 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0b9f1c4a-e63a-407c-b750-f9bc0580b3b8">

- [튜토리얼]
  - Pytorch: https://pytorch.org/docs/stable/quantization.html
  - Tensorflow: https://www.tensorflow.org/lite/performance/model_optimization
- [심화]
  - ONNX: https://onnxruntime.ai/docs/performance/quantization.html
  - TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#working-with-int8
- [연구 레벨] ‒ 리뷰 논문 참고
  - AWESOME curation: https://github.com/htqin/awesome-model-quantization
  - Gholami, Amir, et al. "A survey of quantization methods for efficient neural network inference." arXiv preprint arXiv:2103.13630 (2021).

## Model Compression - Knowledge Distillation 
- Knowledge distillation 의 목적은 "미리 잘 학습된 큰 네트워크(Teacher network)의 지식을 실제로 사용하고자 하는 작은 네트워크(Student network) 에게 전달하는 것"
- Teacher -> Student는 soft prediction들로 비교 (distillation loss)를 구하고, student모델은 hard label, 즉 실제 값과 비교 (student loss) 값을 구하여, loss에 반영!
- Softer softmax 적용하여 증류 (T: temperature) – Why? Soft label로 더 많은 정보를 갖기 때문! <img width="120" alt="스크린샷 2023-09-06 오후 8 13 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e2139b8f-8038-432e-9635-88d8675df895">
- Hard label (student loss)일 때 T=1, distillation loss일 때에는 T가 높게 설정.

<img width="500" alt="스크린샷 2023-09-06 오후 8 14 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4704deee-6f25-4c68-b65a-b4200da34f06">

## Model Compression - Knowledge Distillation
최종 목적 함수 <img width="350" alt="스크린샷 2023-09-06 오후 8 16 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2969155e-e4c7-40a2-b8f0-979dcab8c570">

- 𝐻 : cross entropy, $𝐷_{KL}$: KL-divergence, σ: softer softmax, α: coefficient hyperparameter, $z_s, z_t$ 는 student와 teacher의 logits을 각각 의미!
- 첫번째 항: student model 출력 값과 실제 값과 비교
- 두번째 항: teacher model에서 soft-label 계산하고, 이 soft label과 동일하게 결과 값을 추론하도록 한다.

## Model Compression - Knowledge Distillation의 종류
<img width="500" alt="스크린샷 2023-09-06 오후 8 19 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4cdcd61f-a41c-4eb2-b27b-1f0ace723ca7">

<img width="500" alt="스크린샷 2023-09-06 오후 8 20 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b16a3202-4503-47c2-b7ee-b5212915d490">

- 하나의 연구주제가 되어, 수많은 distillation 종류가 나왔다!

<img width="500" alt="스크린샷 2023-09-06 오후 8 20 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/aec24112-3aab-4941-a5e6-b8b4b9490249">

## Model Compression - Knowledge Distillation의 응용의 예 ‒ Parallel WaveNet
<img width="500" alt="스크린샷 2023-09-06 오후 8 21 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/617d776d-f441-4786-9a5a-83c0c7317093">

Auto-regressive ->Inverse auto-regressive flow

## Model Compression - Pruning
<img width="400" alt="스크린샷 2023-09-06 오후 8 28 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/edc10685-974e-44da-b05e-b38f85dce3e6">

<img width="400" alt="스크린샷 2023-09-06 오후 8 29 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f4a39d46-41b7-4774-bdd8-3af7c57e06f6">

- 처음부터 모델 구조를 찾는 것과 비교하여 몇 가지 장점! Hyperparameter을 조절하는 것 대비 모델 재활용이 가능!
- Pruning은 기존과 달리 이전에 한 번 학습되었던 가중치를 초기값으로 두고 fine-tuning을 진행하기에, 일반적인 fine-tuning 에 비하여 빠르게 수렴하며 훨씬 적은 자원을 소모한다!
- 그러나, 연산 속도 등의 차원에서 cost 를 줄이는 것이 쉽지 않다.
  - sparse matrix의 부재, 단순 0으로 바꾼다 하더라도 matrix의 연산이 바뀌지 않음
  - 아직, production 부분보다 연구적으로 접근하는 경우가 많다.

## Model Compression - Pruning의 종류
Unstructured pruning
  - 신경망의 weight가 가지치기 프로세스에서 개별적으로 가지치기되는 경우
  - Parameter을 임의로 0로 설정(zeroing out)하면 메모리 효율성이 제공되지만(sparse matrix에 저장된 모델) 이전과 동일한 수의 행렬 곱셈을 수행하게 되므로 항상 더 나은 계산 성능을 제공하는 것은 아니다.
    - 밀도가 높은 행렬 곱셈을 희소 행렬 곱셈으로 대체하여 계산상의 이점을 얻을 수 있지만, 기존 GPU/TPU에서 희소 연산을 가속하는 것은 간단하지 않다는 단점을 가진다.
  - 보통 weight 제거가 일반적이다.  

Structured pruning
  - 필요한 전체 계산을 줄이기 위해 “구조화된 방법”으로 parameter을 제거한다.
  - 예를 들어, CNN의 채널 중 일부 또는 피드포워드 계층의 뉴런이 제거되어 계산이 직접적으로 감소한다.
  - 보통 neuron 제거가 일반적이다.

Parameter의 Scoring
  - 가지치기 방법은 중요한 parameter를 고르는 scoring 기법에 따라 나뉠 수 있다.
  - Absolute magnitude scoring 방법이 표준이지만, 새로운 채점 방법을 고안할 수 있다.
    
One-shot pruning vs Iterative Pruning
- 원하는 양을 한 번에 가지치기(One-shot Pruning)하기보다는 원하는 가지치기 정도에 도달할 때까지 네트워크를 어느 정도 가지치기하고 재훈련하는 과정을 반복하는 접근법이 있는데, 이를 Iterative pruning이라고 한다. 
  
그밖에 Train/fine-tuning의 scheduling 기법 등에 따라 Pruning알고리즘의 종류가 나뉠 수 있다

## Model Compression - Pruning을 실습해보고 싶다면?
- Tensorflow: https://www.tensorflow.org/model_optimization/guide/pruning?hl=ko
- Pytorch: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#pruning-tutorial
- NNI: https://nni.readthedocs.io/en/latest/Compression/pruning.html
