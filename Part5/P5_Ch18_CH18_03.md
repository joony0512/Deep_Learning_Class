# Research Topics for Productions - 3. Auto-ML
## Back to the ML pipeline - Why Auto-ML?
<img width="500" alt="image" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/90775b55-b02f-4c1f-8a89-298b2d3d08e6">

Train 파이프라인을 다 만들고, 튜닝없이 데이터만 꾸준히 업데이트하여 모델 weight을 업데이트하는 상황을 가정해보자.   
우리의 기존 모델은 여전히 SOTA일까?   

## Auto-ML - Auto-ML의 종류
<img width="500" alt="스크린샷 2023-09-07 오후 5 04 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c447116f-6cff-4361-b47a-eb92d150259d">

<img width="500" alt="스크린샷 2023-09-07 오후 5 04 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0fd69f3b-01e0-479a-8ee8-06aa2de251ad">

최근의 연구토픽은 Hyper Parameter Optimization (HPO), Neural Architecture Search (NAS) 이 주를 이루고,  
넓은 의미에서 meta-learning 등도 포함시키기도 한다.

## Auto-ML - Hyper-Parameter Optimization (HPO)
동일한 ML 모델에서 Hyperparameter 를 최적화하여 튜닝하는 기법.   
크게, Grid / Random Search(model free approach)와 Bayesian Optimization (Bol model based approach)가 주를 이룬다  
- Grid search: search spaced을 나누고 일정 간격으로 search를 진행한다. 
- Random search: 랜덤으로 hyper parameter를 search한다.

<img width="400" alt="스크린샷 2023-09-07 오후 5 11 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1e1edb37-58e1-4093-a63b-5aab0e0ede37">

- Bayesian optimization: sequential model-based optimization(SMBO)로 validation set에 대해 확률론적으로 hyperparameter를 뽑는다.
이때 뽑는 기준은, valid set에 대해 진행을 한다.
- Evaluation 함수를 f, search space를 Θ, 습득 함수를 𝑆, 확률적인 모델을 M, Record dataset 을 𝐷이고
- 각 샘플을 $(𝜃_i,𝑦_i)$ ∈ 𝐷이고 $𝜃_i ∈ \Theta$을 샘플링된 neural architecture, $𝑦_i$ 을 평가된 방법이라 한다면 SMBO는 다음과 같다.
  
  <img width="487" alt="스크린샷 2023-09-07 오후 5 13 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1d4c1987-3065-4688-9522-444726a34faa">

- 각 방법을 풀면, 아래와 같고 T번 반복한다.
  1. record dataset(D) 에 probabilistic model M을 fit하기 위해 tune한다.
  2. 습득 함수 S는 probabilistic model M으로부터 다음 neural network를 뽑고, $𝜃_i$ 를 추출한다.
  3. f를 통해 neural network를 학습/평가한다.
  4. record dataset을 $(𝜃_i,𝑦_i)$ 을 update한다.

## Auto-ML - Hyper-Parameter Optimization (HPO)
실무에서는?  
직접 구현하지 않아도, 다양한 open-source 라이브러리를 사용 가능! 
- Ray Tune, Optuna, HyperOpt, NNI Tuner, Mango 등

## Auto-ML - Neural Architecture Search (NAS)
<img width="350" alt="스크린샷 2023-09-07 오후 5 22 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cc8263df-1e4d-4372-9193-7541d039cdf9">

<img width="350" alt="스크린샷 2023-09-07 오후 5 22 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ca833169-a79f-41c0-a5aa-0987ff72f7e4">

Search space를 어떻게 설정했느냐, Architecture optimization을 어떤 방법으로 했느냐, 어떤 estimation으로 했느냐에 따라 NAS의 종류가 달라진다!
- Model generation부분 위주로 알아보자

## Auto-ML - Neural Architecture Search (NAS) ‒ Search space: Sequential layer-wise operation
<img width="450" alt="스크린샷 2023-09-07 오후 5 24 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5bab9ef7-c287-4496-ae49-979e6cd2e3d8">

- NAS의 search space을 설계하는 가장 단순한 방법은 초기 연구에서 볼 수 있는 순차적 계층별 운영 목록을 사용하여 network의 위상 배치를 묘사하는 것  
- 네트워크 표현의 serialization은 각 운영이 서로 다른 계층별 parameter와 연관되어 있고 그러한 연관성은 하드 코딩될 필요가 있기 때문에 상당한 양의 전문가 지식이 필요!
  - (i.e. convolution 작업을 예측한 후 모델은 커널 크기, 스트라이드 크기 등을 출력해야하며, FC 작업을 예측한 후에는 단위 수를 다음 예측으로 보아야 한다.)
- 모델이 valid하지 않을 가능성이 있기 때문에, 검증 프로세스가 있어야 한다.

## Auto-ML - Neural Architecture Search (NAS) ‒ Search space: cell-based representation
<img width="400" alt="스크린샷 2023-09-07 오후 5 27 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2c4124f7-3a3d-4d70-b886-cfe656333378">

<img width="400" alt="스크린샷 2023-09-07 오후 5 27 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/20675560-779d-41fb-bded-ae2479d7d841">

- Normal cell과 reduction cell (motifs)로 나누고, 미리 정의해둔 내에서 cell representation을 학습하게 한다. -> 여러 개를 사용가능하게
- 2개의 type을 학습 (normal cell, reduction cell)
  - 검색 공간이 대폭 줄어든다!
  - NAS의 결과물을 다른 데이터셋으로 쉽게 전송 가능하다
  - 아키텍처 엔지니어링에서 모듈을 반복적으로 쌓는 유용한 설계 패턴의 강력한 증거

## Auto-ML - Neural Architecture Search (NAS) ‒ Search space: structured
<img width="500" alt="스크린샷 2023-09-07 오후 5 28 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ebfb0e20-ae27-4d86-bdd9-733080b1f593">

<img width="500" alt="스크린샷 2023-09-07 오후 5 28 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c7a20028-da02-40bc-9056-96dc2a908b0e">

- Direct acyclic graph 형태나 Tree구조 형태로 구조를 잡는 방식도 존재!

## Auto-ML - Neural Architecture Search (NAS) ‒ Search algorithm ‒ 최초? 강화 학습 (RL)
<img width="400" alt="스크린샷 2023-09-07 오후 5 32 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b05a19c8-5672-4dcf-8d12-916e8cb851b0">

- Action space: child network을 정의하기 위한 토큰 목록. 
- T: token의 수
- Controller (RNN): Action을 return한다!
- Reward: child network의 tuning을 마친 후에 얻어지는 accuracy을 reward로 삼는다!
- Loss: REINFORCE 알고리즘(loss)로 controller의 parameter(𝜃)을 최적화한다.
  - Expected reward(높은 정확도)을 극대화! 아래의 목적 함수의 기울기를 업데이트 한다.

    <img width="300" alt="스크린샷 2023-09-07 오후 5 31 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1a5b8294-1cad-4ba6-91f1-d3cfab8121c1">

- 참고: 그 외 어떠한 RL 알고리즘도 사용가능하다. - 예) 𝜖-greedy 기반의 Q-learning: MetaQNN

## Auto-ML - Neural Architecture Search (NAS) ‒ Search algorithm ‒ 그외 search algorithm은?
- Evolutionary algorithm(EA): 유전 알고리즘 (genetic algorithm) 등을 이용해 최적화 하는 방법.
  - i.e.) AmoebaNet(2018), HNAS (2017)  
- Progressive Decision Process: Search model로 하여금 간단한 모델에서 search 했다가, 점차적으로 복잡한 search를 할 수 있게 curriculum을 도입.
  - i.e.) Progressive NAS (PNAS; 2018)
- Gradient Descent: Child model evaluation (학습)과 같이 진행! -> One-shot 접근법
  - i.e. DARTS (2019), SNAS(2019)

## Auto-ML - 더 공부하려면?
<img width="400" alt="스크린샷 2023-09-07 오후 5 40 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4e4b22ea-a20e-42eb-856a-a79787b21b4d">

리뷰 논문
  - He, Xin, Kaiyong Zhao, and Xiaowen Chu. "AutoML: A Survey of the State-of-the-Art." Knowledge-Based Systems 212 (2021): 106622.
  - Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. "Neural architecture search: A survey." The Journal of Machine Learning Research 20.1 (2019): 1997-2017.

AutoML book
  - https://www.automl.org/book/

잘 요약된 블로그 사이트 (NAS)
  - https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search .html#one-shot-approach-search--evaluation
