## CH03_03.앙상블

![Untitled (12)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d28b5d36-dfd6-4c34-a4e1-4cbb69abe40c)


- **앙상블** : 여러개의 가설 모델들을 하나로 합쳐 결과를 얻는다
    - 1개의 모델을 학습할 때 보다, 정확도나 단순모델보다 커질것이다.
    - 많은 논문과 kaggle과 같은 Competition에서 성능을 높이기 위해 사용하는 방법이다.
- 각 모델이 독립적이라고 가정하고, 각 모델의 정확도(accuracy)가 전부80%라고 해보자.
    - 틀리는 이유는 다르다고 가정한다.
    - A,B,C가 ooo일경우와 oxo, oox, xoo일 경우(3개중 2개만일치)를 고려해보자.
- 이때 각 모델의 결과를 다수결에 따라(Voting) 하나의 결과로 종합했을때, 전체 모델의 정확도는 $(0.8)^3 + 3*(0.8)^2*0.2 =0.896$ → 각각의 accuracy였던 0.8보다 높아진다
    - 계속 모델이 많아진다면 더 높아질 것이다.

### 학습(learning)에 의한 결합방식
![Untitled (13)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60664963-421e-4860-ab73-b8bc52f4d5ee)



- **Boosting** : 성능이 약한 모델들을 순차적(sequential)으로 학습하여 강한 모델들을 만드는 과정
    
    ![Untitled (14)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/955c1e33-0d04-40d8-8183-c2df030502f6)

    
    - **ADA(adaptive) Boosting** : 반복적으로, sequential하게 아래 작업을 반복
        - 1. Train dataset을 샘플링한 새로운 dataset을 만든다. 그리고 이 dataset으로 model을 학습시킨다.
        - 2. 학습시킨 모델을 Validation한다.
        - 3. Validation과정에서 잘못 예측한 데이터를 Train dataset에 추가하고, 새로운 dataset을 샘플링한다. 즉, 오류데이터에 가중치를 부여하여 샘플링 될때 더 잘뽑히게 만들고 새 모델의 학습을 진행한다.
        - 4. 모델이 2개이상이 되었으므로 이를 Voting하여 하나의 예측값으로 하고, 다시 가중치를 계산하여 샘플링이 잘되게 만든다.
    - **Gradient Boosting**
        - 각 step에서 손실함수를 계산하고, 각 과정에서 예측값을 이용해 손실(loss)값의 gradient(미분값)을 계산하여 최적화한다.
    - 오버피팅이 될수도 있다
    - Bias를 줄인다(variance도 줄일수있음)
    - 순차적으로 모델을 연산한다.
- **Bayesian model averaging**
- **Stacking**
    - 각 모델을 병렬적으로 추론하고, 결과를 취합해 다시 학습하는 높은 계층의 Meta-Learner(메타모델)가 있어, 새로운 결과를 얻는 방법.
    - Boosting과 마찬가지로 오버피팅이 될수도 있다
        - Bias를 줄인다.(variance역시 줄일 수 있다)

### 동의(consensus)에 의한 결합방식

![Untitled (15)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/61590908-fda9-40c5-83f1-c4ef74127dba)


- **Bagging**
    - n개의 데이터샘플이 주어지고, 학습모델의 class가 주어졌을때, m개의 모델을 다른 샘플(bootstrap하고) 그것을 aggregating하여 평균한다
        - 오버피팅방지에 도움
        - variance를 줄임
        - 독립적인(parallel하게 연산 가능한)모델을 사용한다
        
        —> 정형화 효과
        
        ![Untitled (16)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b15546b4-8b4b-4320-9b27-3b00b0cbe193)

        
- **Random forest**
- **Average of model outputs**
