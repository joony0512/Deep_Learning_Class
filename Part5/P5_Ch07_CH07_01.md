# Recap: Encoder-decoder 기반 모델
## Autoregressive vs autoencoder vs embedding vs seq2seq

<img width="401" alt="스크린샷 2023-07-26 오후 6 19 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1bd17caa-79d5-4aba-9602-9acc4a5b3c1b">

- Encoder-Decoder 기반 모델에서, 인풋 x,y가 time-series이고 size(x) != size(y)일때, sequence-to-sequence라고 배웠다.
- 또한 Encoder와 decoder의 가운데에서 압축된 정보들은 Embedding된 정보라고 한다.
- Decoder의 경우, 일반적인 seq2seq모델에서는 autoregressive하게 y를 생성(generation)하고 있었다.
- 한편 size(x) = size(y)라면 어떤 모델일까?
  - Auto-Encoder라고 한다

<img width="285" alt="스크린샷 2023-07-26 오후 8 39 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c212db1c-062e-4556-b614-110cca1e0df1">

## Auto-encoder의 예: Deconvolutional network & U-Net
<img width="489" alt="스크린샷 2023-07-26 오후 8 41 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/94d688e4-8712-4c32-a884-0ef7d5c01f73">
<img width="570" alt="스크린샷 2023-07-26 오후 8 41 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/dbe5c8e8-23a6-46a3-a89d-14b1b563627d">

## Sequence-to-sequence의 인코더(embedding)과 디코더(auto-regressive)
- Sequence-to-sequence(seq2seq)모델은 인코더(encoder)와 디코더(decoder)로 구성되어 있으며, 본 논문에서의 seq2seq는 각각의 역할은 다음과 같이 정리할 수 있다.

- Encoder: 입력 값의 정보를 받아 latent variable (잠재 변수 혹은 context vector)로 embedding(압축)하는 단계
- Decoder: 인코더의 context 값과 이전까지의 값을 받아, auto-regressive하게 분포를 “생성”하는 단계. 

<img width="390" alt="스크린샷 2023-07-26 오후 8 43 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d16da5cd-5316-48ac-9d33-67f9764a45e7">

# Recap: Sequence-to-sequence에서의 encoder : 임베딩(embedding)
임베딩(embedding)이란?
수학에서 embedding(혹은 imbedding)이란 하나의 사례 안에 포함된 수학적 구조의 한 예로,
모집단의 성격을  보존하면서도 모집단과는 다른 형태의 소집단으로 매핑(mapping) 되는 것
Dimension reduction 방법 등 을 통해 얻어질 수 있다.

<img width="508" alt="스크린샷 2023-07-26 오후 8 46 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5c3fe39a-a926-4f70-b466-b5f25ffe5320">

크게 3가지 용도:

1. 가장 가까운 이웃 정보를 찾도록 해준다 🡪  유저의 관심사나 클러스터 카테고리에 대해서 추천을 하도록 도와준다.
2. 머신 러닝의 피처로서  learning을 위한 입력 값을 임베딩하여 사용할 수 있다.
3. 카테고리 간 개념과 관련 정도를 시각화 해줄 수 있다.

## Recap: Sequence-to-sequence에서의 decoder : 자가 회귀(auto-regressive) 모델
미래를 예측하는 상황을 고려해보자.
보통 시퀀스에서 timestamp t $\in Z^+$ 일때를 예측한다면 우리는 $x_1 \sim x_{t-1}$까지의 정보를 이용할 수 있을것이다.
즉 다음과 같이 구할 수 있다.
  - $x_t \sim P(x_t | x_{t-1},...,x_1)$
  - 이때 $x_{t-1} \sim P(x_{t-1} | x_{t-2},...,x_1)  ... x_3 = P(x_3|x_2,x_1), x_2 = P(x_2 |x_1), x_1 = P(x_1)$으로 순차적으로 계산 할 수 있다.
  - <img width="371" alt="스크린샷 2023-07-26 오후 8 53 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c4e315af-ef8d-4f84-8ec1-a57fadaebfab">

  - 이런모델을 자가회귀모델(auto-regressive model)이라고 한다.

## 생성모델(generatice model)이란?
생성모델의 정의 : $p_{data}(x)$와 유사한 $p_{model}(x)$ 을 학습하는것 -> 원하는 이미지를 복구하거나 새 샘플을 만들어 낼 수 있다.
Auto-regressive, autoencoder, sequence-to-sequence는 생성모델의 일종이라 볼 수 있다.
Embedding의 경우, dimension reduction과 가까우며, 위의 생성모델의 성능향상을 위한 기반 연구로써 많이 쓰인다.

## 생성 모델(generative model)의 종류 (preview)
Implicit density estimation
-> $p_{model}(x)$를 직접구하지 않아도 샘플을 생성할 수 있다.
Explicit density estimation
-> $p_{model}(x)$를 구하기 위해 loss등에 직접 반영하고 최적화 한다.
