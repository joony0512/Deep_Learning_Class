# CH02_01.Feedforward Network

## 변형함수의 선택( transformation function)의 선택

### 데이터의 표현형(representation) 혹은 피처를 알기 위한 변형 함수 (transformation function)의 선택방법

1. 직접 함수를 디자인한다 : hand -craft
    
    → 딥러닝 이전에는 일반적이었다.
    
2. 포괄적인 함수를 고른다.(i.e. kernel function)
    1. 커널방법이란?
        - 다른 차원으로 보내는 방법
            - 2차원→ 3차원으로 보내기
3. 딥러닝이 변형함수 자체를 학습하게 한다.( 이 경우 변형 함수는 비선형(non-linear하여야 한다)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ac3c4a9-4539-4a4c-beb2-cf07ee3fd087/Untitled.png)

- 비선형적 변형(non-linear transform)이 왜 필요할까?
    - i.e. XOR 문제
    - VC-dimension : VC(H) =H 라는 분류기에 의해서 최대로 부술(shatter) 수 있는 point 수
    - ‘선형분류기’의 VC차원 → n+1차원
        - 1차원에서, 3개이상 일직선으로 겹치는 점이 3개일때 → vc :1+1 =2
            - vc의 차원이 점의 수보다 작으므로 불가능한 경우가 생긴다.
        - 2차원에서, 3개이상 일직선으로 겹치지 않는 점이 3개일때→ vc : 2+1 = 3
        - 2차원에서, 3개이상 일직선으로 겹치지 않는 점이 4개일때 → vc : 2+1 =3
- 활성함수 : 인공신경망에서 이런 변형을 학습하는 방법
    - 하나의 큰 변형함수라고 볼 수 있다.
- Hidden units(숨은 유닛)
    - 앞단의 input unit, 혹은 전단계의 hidden units으로 부터 선형으로 변형(Affine transform)을 받은 뒤, 활성함수(Activation function)등으로 비선형(non-linear)변형을 한다.
        - 선형 : $Wx + b$
    - output unit도 hidden unit과 같은 역할을 수행 할 수 있음!
- 비선형(non-linear) 활성함수(activation function)
    - tanh
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1a4714ea-9caa-49da-ae3e-0741effcc046/Untitled.png)
        
        - $z =Wx + b, f(z) = tanh(z)$
        - -1~1로 압축
        - 0이 중심이다
        - Bias shift가 없다
        - sigmoid처럼 Saturation문제(gradient vanishing)
            - 미분을 할수록 변화가 없다고 인지하고 레이어가 쌓일수록 효과가 떨어지는 문제가 생긴다. Saturation(포화상태) → gradient vanishing
    - ReLU (Rectified linear unit; 정류 선형 유닛)
        - 가장 일반적인 선택!
            1. 쉽게 미분계산( gradient)이 된다 ! (0혹은 1)
            2. Sparsity: 희박하게 (sparce)활성화 시킬 수 있다.
                1. 값이 0인 부분이 많으므로 많은 연산이 줄어진다
            3. Sigmoid, Tanh등 함수는 포화상태가 되기 쉬운 반면, ReLU는 그렇지 않는다.
                1. 시그모이드 등은 layer가 쌓이면, gradient가 중첩되고 점점 gradient가 0으로 가까이 된다
                2. 즉, gradient가 사라지는 gradient vanishing문제 발생
        - 단점
            1. Unbounded하다. (무한대(nan)값이 나오게 될 때 본 현상을 막을 수 없다)
                1. 시그모이드 0~1, tanh -1~1
            2. 0이 중심이 아니다
                1. 활용도가 제한 될 수 있다
                2. weight의 업데이트가 항상 같은 방향으로 만 일어난다!
            3. Dying ReLU 문제:
                1. Layer가 아주 매우 깊거나, learning rate 등이 잘못 세팅되면 용량을 차지하거나 항상 죽어있는 문제 발생
                    
                    → 이 역시 gradient vanishing
                    
                2. Generalize 된 ReLU함수를 쓰면 조금 해결(단 퍼포먼스는 경우에 따라 더 떨어질 수 있다) i.e. leaky ReLU
                
                ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ec1e0209-2840-46f2-be60-07ba675c3e59/Untitled.png)
                
    - 일반화 된 ReLU(Rectified linear unit; 정류 선형 유닛)
        - Leaky ReLU
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c9ad0269-56f4-475a-b97f-db1f16c36474/Untitled.png)
            
            - 퍼포먼스 적으로 조금 손해를 볼 수 있으나, saturation문제나 dying ReLU문제를 조금 완화한다
            - ReLU대비 데이터가 작을때 오버피팅이 될 수 있다.
        - ELU(exponetial Linear Unit)
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c69b0d14-2e49-4e87-a9ca-0461a6c5fadf/Untitled.png)
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e8e36052-3762-4370-9766-0795fe07d994/Untitled.png)
            
            - Leaky보단 자연스럽게 바뀐다.
            - 일반적으로 효과가 가장 좋음
            - $\alpha =1$일때 미분에 대해 연속
            
    - Maxout Units
        - z를 다른 k∈ K 값으로 넣고, 각각의 유닛들의 max값을 취한다
        - Universal approximation theorem이 볼록함수(convex)로 점차 근사함을 나타낸다.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1b712e34-2081-4f64-ad3a-58bacc756f5f/Untitled.png)
        
    

### 목적함수: 비용함수 (cost function) 와 출력 유닛(output unit)

- ANN을 최적화하기 위한 목적함수, 즉 비용함수(cost function)는, 보통 가능도함수를 최대화(maximum likelihood)를 하는 방법을 선택한다.
- $x, y$~
$p_{data}$이고 $\theta$가 학습가능한 파라미터일때, 모델이 추론하는 확률분포를
    
    $q(y|x;\theta)$라고 한다면, 비용함수는 cross entropy이다.
    
    비용함수 : $J(\theta)= -E_{x,y\sim p_{data}} log q_{model}(y|x;\theta)$
    
- 크로스엔트로피(cross-entropy)
    - $H(p,q) := \Sigma_i p_i log_2 1/q_i = -\Sigma_i p_i log_2 q_i$
- 위의 MLE기반의 비용함수를 그대로 사용한다면, $p_{data}$는 데이터의 값으로 바꿀 수 없는 값이므로, 우리가 해야하는 것은 $q_{model}(y|x;\theta)$을 구하는 것으로 압축할 수 있다.
- 즉 이는 모델링을 잘하는 것이다 → 모델을 바꿔만들면, 비용함수도 따라서 같이 바뀐다.

### 출력 유닛(output unit) activation의 종류(필수)

- Linear
    - 확률분포이거나 회귀(regression)문제일때 종종사용
- Sigmoid
    - Binary classification일때 많이 사용
    - 0~1사이
- Softmax
    - Multi-class classification일때 많이 사용한다.
    - 0~1사이로 , 모든 k 에 대해 합이 1
    - K=2일때, sigmoid 와 같다

### 출력 유닛(output unit) activation의 종류(심화)

- 모델의 목적이 달라지면 아웃풋 형식도 바뀔 수 있다.
- 예) 혼합밀도 (mixture density) → 가우시안 분포의 혼합을 출력한다.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b0acb407-3fa2-4e0e-97e7-cb9e00fe61e7/Untitled.png)
    
- 일반아웃풋 q(c=i|x)뿐만아니라, 가우시안 분포를 위한, 평균 및 표준편차도 같이 예측
- 데이터의 노이즈성에 견디기 위한 디자인 중 하나
- AI모델의 불확정성 정도를 처리하기 위해서( uncertainty level도 같이 출력)→ 설명가능한 AI
- 컨트롤 가능한 AI(어느정도 모델을 조절하고 싶을때)

## 피드포워드 네트워크의 학습

1. 지도학습적인 접근 → 일반적인 경우, label이 충분할때 
    - 파라미터 랜덤초기화
    - 주어진 라벨로 loss최소화 학습
    - Stochastic gradient descent와 backpropagation이 쓰인다
    
2. 레이블 샘플이, 실제 데이터와 비교했을때 비율이 작을 때
    - 비지도학습 + 일부분(top)만 지도학습 분류
        - 각각의 layer를 비지도학습으로 학습
        - 그 이후 지도학습을 하기 위한 분류기 layer를 다른 레이어를 유지한(굳힌;fixed)채로 붙인다.
    - 비지도 학습 이후 글로벌하게 지도학습 fine-tune접근
        - 각각의 layer를 비지도 학습으로 학습한다.
        - 그 이후, 지도학습을 하기 위한 분류기 layer를 붙이고 완전히 다시 학습 시킨다.

A. Feed-forward step

- input → output생성, 레이블과 비교하여 error계산

B. Backpropagation step

- Chain rule을 이용하여 gradient들을 학습가능한 파라미터들에게 업데이트 한다

## 컴퓨테이션코스트, 메모리 코스트

- 각 코스트들은 Forward step과 Backward step을 따로 고민하자.
    - Forward step 🡪 학습 뿐만이 아니라 서비스 퍼포먼스에도 직결.
    - Backward step 🡪 Online으로 학습을 돌리는 상황이 아니라면, 학습 속도에만 영향.
- 다중 퍼셉트론(MLP)의 경우, 컴퓨테이션 코스트는 matrix 값을 곱하는 것에서 나온다.
    
    이 계산과 입력부분의 계산을 제외하고는,
    
- **Forward step에서 각 weight matrix를 더하기 위해서는 O(w)만큼의 비용이 든다.**
- **Backward step에서도 마찬가지로 같은 비용인 O(w) 만큼의 비용이 든다.**
- 메모리 코스트는 O(mh)으로, m은 minibatch의 예제 수, h는 은닉층 유닛(hidden units)의 개수이다.
