# Deep Reinforcement Learning - 4. Deep RL의 발전 ‒ Policy based Model-free RL
## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization)
Policy-based: value function이 없이 policy만을 학습하는 agent
- 명시적으로 나타낸 정책 $𝜋_\theta(𝑎|𝑠)$ 에 대해, 성능 목표 𝐽( $𝜋_\theta$ )의 최대화(직접 gradient ascent 혹은 local approximation 이용) 한다.
- 이 최적화는 on-policy에서 대부분 수행된다.
  - On-policy란:
    - RL의 각 업데이트가 최근 버전의 policy에 따라 수집된 데이터만 사용한다!
    - Policy optimization에는 일반적으로 정책을 업데이트하는 방법을 파악하는 데 사용되는 정책 값 함수 $𝑉^\pi$ 에 대한 근사치 $𝑉_\theta(𝑠)$ 의 학습(i.e. DNN 혹은 ML 모델 학습)이 포함된다.
- REINFORCE, Actor-Critic, Off-policy Policy gradient, A2C/A3C, PPO, TRPO 등이 여기에 속한다! 

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization): 목적 함수
Policy-based 알고리즘의 reward function은 다음과 같이 정의할 수 있다.

<img width="350" alt="스크린샷 2023-08-30 오후 5 04 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/10ed03a7-44e8-4145-98ab-3fcfc76ded4c">

여기서, $𝑑_\pi (𝑠)$ 는 $𝜋_\theta$ 에 대한 Markov chain의 stationary distribution (=on-policy state distribution)이다.

- policy-based 알고리즘은 continuous space에서 보다 유용하다!
  - c.f. 만약, Q-table을 무한한 state/action space에서 만든다면, 비효율적일 것이다!
    - curse of dimensionality
- 기울기 상승법(gradient ascent)으로 가장 높은 return을 내는 최적의 𝜃를 찾기 위해 기울기 $∇_\theta 𝐽(𝜃)$ 를 구한다.

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization): Policy gradient theorem과 $∇_\theta 𝐽(𝜃)$

$∇_\theta 𝐽(𝜃)$ 는 policy gradient theorem에 의해 아래와 같이 <img width="250" alt="스크린샷 2023-08-30 오후 5 09 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5e9f3d50-abc8-47af-9201-f8283350850d">
에 비례한다.

<img width="450" alt="스크린샷 2023-08-30 오후 5 10 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/44194cb7-369e-489c-ae0a-be957c67e659">

참고: Policy gradient theorem의 증명은 아래 링크 참고.
  - Sutton book 13.1
  - https://talkingaboutme.tistory.com/entry/RL-Policy-Gradient-Algorithms (한국어 블로그) • https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#reinforce
  Reinforcement Learning

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: REINFORCE (Monte-Carlo policy gradient)
REINFORCE(Monte-Carlo policy gradient)는 episode 샘플 내에서 Monte-Carlo method를 통해 구한 estimated return을 가지고 policy parameter 𝜃를 update해 나가는 기법!
  
이때, sample로 구한 gradient 의 expectation과 actual gradient 의 expectation이 동일하므로 sampling 이 유효하다.

<img width="350" alt="스크린샷 2023-08-30 오후 5 16 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea53ecbe-dec1-48f4-9bc2-8fdb43412e7d">

위 식을 통하여, real sample trajectory들로부터 total expected return( $𝐺_t$ )를 구할 수 있고, 이를 이용해 gradient의 update가 가능하다!  
이는 다만, 완전한 trajectory에 의존하기에, 효율이 조금 떨어질 수 있다.

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: REINFORCE (Monte-Carlo policy gradient)
<img width="500" alt="스크린샷 2023-08-30 오후 5 17 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12b252f9-b3db-42ad-9e4c-c73d3b263a18">

Monte-Carlo 방법은 그러나, high variance 문제가 발생할 수 있고, return을 구할 때 episode가 너무 길거나 복잡한 문제에서 오래 걸리거나 수렴하지 않을 수도 있다

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: Actor-Critic

Policy gradient의 두 가지 구성요소: policy model과 value function    
Actor-Critic의 아이디어:
- Policy를 학습할 때 value function을 학습하면 더 좋은 성능!
- Value function을 이해하는 것 자체가 vanilla policy gradient에서 gradient variance를 줄이는 등 policy를 update하는데 도움을 줄 수 있지 않을까?
  
- 2개의 모델로 구성
  - Critic 모델: Value function(Action-value $𝑄_w (𝑎,𝑠)$ or state-value $𝑉_w (𝑎|𝑠)$ )의
parameter w를 업데이트 한다.
    - 얼마나 actor의 action이 좋았는지 판단하고 actor에게 수정 방향을 알린다.
      
  - Actor 모델: Policy ( $𝜋_\theta(𝑎|𝑠)$ ) parameter 𝜃 를 critic이 제안하는 방향대로 업데이트 한다.
    - 어떤 action을 취할 것인지 결정한다.
   
## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: Actor-Critic
Value function을 이해하는 것 자체가 vanilla policy gradient에서 gradient variance를 줄이는 등
policy를 update하는데 도움을 줄 수 있지 않을까?
- Variance를 baseline 함수를 빼주어 줄여보자! REINFORCE의 gradient ascent

<img width="400" alt="스크린샷 2023-08-30 오후 5 27 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b22d2ddb-7400-4d5e-b4d8-4e97fa368bca">

에서, baseline 함수(𝑏(𝑠))를 정해서 cumulative reward( $𝐺_t$ )에서 빼자:

<img width="300" alt="스크린샷 2023-08-30 오후 5 30 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6e3a0d9a-5ee0-4591-b419-aeffbfca8f9d">

$𝐺_t$ − 𝑏(𝑠) 의 종류에 따라 종류가 달라진다!
- $𝐺_t$ : REINFORCE
- 𝛿: TD actor-critic
- $𝑄^w (𝑠, 𝑎)$ : Q actor-critic
- $𝐴^w (𝑠, 𝑎)$ : Advantage actor-critic

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: Actor-Critic (TD actor-critic)
<img width="500" alt="스크린샷 2023-08-30 오후 5 32 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/93770f29-db5b-4f02-af5b-7f7aa1088739">

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: Actor-Critic (Q actor-critic)
<img width="500" alt="스크린샷 2023-08-30 오후 5 33 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9770bccb-87c3-4ca4-a037-494ce05fd3ff">

로 표현가능하고, $𝐸_{r_{t+1}, S_{t+1}, ..., r_T, S_T} [𝐺_T]$ 는 Q-table( $𝑄_w(𝑠,𝑎)$ )을 나타내는 것으로 해석할 수 있다!

<img width="500" alt="스크린샷 2023-08-30 오후 5 35 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7029a334-ab49-454c-b3d9-fcc8cc6c22a4">

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) 알고리즘: Actor-Critic (Advantageous actor-critic)
Q - actor critic <img width="300" alt="스크린샷 2023-08-30 오후 5 37 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8c608f68-ea34-4434-8908-c3a8a83c8ca1">
에서,  
$𝑄_w(𝑠_t,𝑎_t)$ 에서 baseline function $𝑉_v (𝑠_t)$ 를 빼서 개선하면? -> Advantage value  
  - $𝐴(𝑠_t, 𝑎_t) = 𝑄_w(𝑠_t,𝑎_t) − 𝑉_\pi(s_t)$
    
Bellman optimality equation에 따라, 𝑄(s_t, a_t) = 𝐸[ $𝑟_{t+1} + 𝛾𝑉(s_{t+1})$ ]이므로,
  - $𝐴(𝑠_t, 𝑎_t) = 𝑟_{t+1} + 𝛾𝑉_\pi(s_{t+1}) − 𝑉_\pi(s_{t})$ 이다.

정리하면, update equation은 다음과 같다.
<img width="300" alt="스크린샷 2023-08-30 오후 5 43 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bcd21183-2c7c-4b9b-bf82-640ebb74361c">

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) - Actor-Critic (Advantageous actor-critic)
$𝐴_\Pi(𝑠_t, 𝑎_t) = 𝑟_{t+1} + 𝛾𝑉_\pi(s_{t+1}) − 𝑉_\pi(s_{t})$  
Advantage Actor-Critic 방법의 종류는 다음과 같다.  
  
TD advantage estimate: 즉각적인 reward 사용(기본 format)
- $𝐴_\Pi(𝑠_t, 𝑎_t) = 𝑟(𝑠,𝑎,𝑠')/ + 𝛾𝑉_\pi(s_{t+1}) − 𝑉_\pi(s_{t})$

Monte-Carlo(MC) advantage estimate: 만약 Q-value을 실제 return로 바꿀 경우. 
- $𝐴_\Pi(𝑠_t, 𝑎_t) = 𝐺_t − 𝑉_\Pi(s_t) = 𝑅(𝑠,𝑎) − 𝑉_\pi(s_{t})$
  
N-step advantage estimate: N step return을 사용 (가장 많이 사용)
- A2C/A3C
- <img width="300" alt="스크린샷 2023-08-30 오후 5 50 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/40f3e55f-3350-409e-be3a-e46449d32869">
- MC와 TD 방법의 장단점을 둘다 계승

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) - A2C/A3C
- Critic 모델: Value function(Action−value 𝑄4 𝑎 𝑠 or state−value 𝑉4(𝑎|𝑠))의 parameter w를 업데이트 한다.
  - 얼마나 actor의 action이 좋았는지 판단하고 actor에게 수정 방향을 알린다.
  - State-value 𝑽𝝓(𝒂|𝒔) 를 return
  - Loss: $(𝑅 −𝑉_\Pi (s))^2$

- Actor 모델: Policy (𝜋!(𝑎|𝑠)) parameter 𝜃 를 critic이 제안하는 방향대로 업데이트 한다.
  - 어떤 action을 취할 것인지 결정한다.
  - 𝝅𝜽(𝒂|𝒔)을 return!
  - Loss : $∇_\theta log 𝜋_\theta (𝑠,𝑎) (𝑅 − 𝑉_\Pi (𝑠))$

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) ‒ A2C/A3C
<img width="500" alt="스크린샷 2023-08-30 오후 6 19 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c287db13-d285-4ed1-9f61-06ba42e6e10d">

<img width="300" alt="스크린샷 2023-08-30 오후 6 20 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/46bf1674-f572-4050-8685-620e970c0851">

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) ‒ A3C
<img width="500" alt="스크린샷 2023-08-30 오후 6 20 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ff8688ed-1b8c-491a-bfc1-d73685a69bdd">

<img width="500" alt="스크린샷 2023-08-30 오후 6 20 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0a030731-7fff-4d57-a2d3-38b1fe5fc3a6">

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) ‒ 결과
<img width="500" alt="스크린샷 2023-08-30 오후 6 21 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/aed38f08-05d9-4456-8d38-0062875f11a1">

<img width="500" alt="스크린샷 2023-08-30 오후 6 21 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d2c89411-219b-46d6-ac76-414f29e0658a">

<img width="500" alt="스크린샷 2023-08-30 오후 6 22 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/85bbb728-0ebf-4860-9152-62ac43779da3">

<img width="300" alt="스크린샷 2023-08-30 오후 6 23 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fcbe4b32-3186-4229-8ebd-772a86ba4881">

Mnih, Volodymyr, et al. "Asynchronous methods for deep reinforcement learning." International conference on machine learning. PMLR, 2016.

## 강화 학습의 구분 - Model-free RL ‒ policy-based (policy optimization) ‒ A2C vs A3C
<img width="500" alt="스크린샷 2023-08-30 오후 6 23 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1ba181b6-4a3d-49ab-a048-5bfd5e8d6b86">

A2C는 A3C의 synchronous하고 deterministic 버전을 말한다! 
- Inconsistency문제를 해결하기 위해 coordinator를 도입
- 최적화나 알고리즘에 따라 A2C가 GPU 연산 등에서 보다 효율적이라 알려 져있다!
  - 참고: https://openai.com/blog/baselines-acktr-a2c/
