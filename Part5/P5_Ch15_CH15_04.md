# Deep Reinforcement Learning - 4. Deep RLì˜ ë°œì „ â€’ Policy based Model-free RL
## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization)
Policy-based: value functionì´ ì—†ì´ policyë§Œì„ í•™ìŠµí•˜ëŠ” agent
- ëª…ì‹œì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì •ì±… $ğœ‹_\theta(ğ‘|ğ‘ )$ ì— ëŒ€í•´, ì„±ëŠ¥ ëª©í‘œ ğ½( $ğœ‹_\theta$ )ì˜ ìµœëŒ€í™”(ì§ì ‘ gradient ascent í˜¹ì€ local approximation ì´ìš©) í•œë‹¤.
- ì´ ìµœì í™”ëŠ” on-policyì—ì„œ ëŒ€ë¶€ë¶„ ìˆ˜í–‰ëœë‹¤.
  - On-policyë€:
    - RLì˜ ê° ì—…ë°ì´íŠ¸ê°€ ìµœê·¼ ë²„ì „ì˜ policyì— ë”°ë¼ ìˆ˜ì§‘ëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•œë‹¤!
    - Policy optimizationì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì„ íŒŒì•…í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì •ì±… ê°’ í•¨ìˆ˜ $ğ‘‰^\pi$ ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ $ğ‘‰_\theta(ğ‘ )$ ì˜ í•™ìŠµ(i.e. DNN í˜¹ì€ ML ëª¨ë¸ í•™ìŠµ)ì´ í¬í•¨ëœë‹¤.
- REINFORCE, Actor-Critic, Off-policy Policy gradient, A2C/A3C, PPO, TRPO ë“±ì´ ì—¬ê¸°ì— ì†í•œë‹¤! 

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization): ëª©ì  í•¨ìˆ˜
Policy-based ì•Œê³ ë¦¬ì¦˜ì˜ reward functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

<img width="350" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 04 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/10ed03a7-44e8-4145-98ab-3fcfc76ded4c">

ì—¬ê¸°ì„œ, $ğ‘‘_\pi (ğ‘ )$ ëŠ” $ğœ‹_\theta$ ì— ëŒ€í•œ Markov chainì˜ stationary distribution (=on-policy state distribution)ì´ë‹¤.

- policy-based ì•Œê³ ë¦¬ì¦˜ì€ continuous spaceì—ì„œ ë³´ë‹¤ ìœ ìš©í•˜ë‹¤!
  - c.f. ë§Œì•½, Q-tableì„ ë¬´í•œí•œ state/action spaceì—ì„œ ë§Œë“ ë‹¤ë©´, ë¹„íš¨ìœ¨ì ì¼ ê²ƒì´ë‹¤!
    - curse of dimensionality
- ê¸°ìš¸ê¸° ìƒìŠ¹ë²•(gradient ascent)ìœ¼ë¡œ ê°€ì¥ ë†’ì€ returnì„ ë‚´ëŠ” ìµœì ì˜ ğœƒë¥¼ ì°¾ê¸° ìœ„í•´ ê¸°ìš¸ê¸° $âˆ‡_\theta ğ½(ğœƒ)$ ë¥¼ êµ¬í•œë‹¤.

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization): Policy gradient theoremê³¼ $âˆ‡_\theta ğ½(ğœƒ)$

$âˆ‡_\theta ğ½(ğœƒ)$ ëŠ” policy gradient theoremì— ì˜í•´ ì•„ë˜ì™€ ê°™ì´ <img width="250" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 09 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5e9f3d50-abc8-47af-9201-f8283350850d">
ì— ë¹„ë¡€í•œë‹¤.

<img width="450" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 10 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/44194cb7-369e-489c-ae0a-be957c67e659">

ì°¸ê³ : Policy gradient theoremì˜ ì¦ëª…ì€ ì•„ë˜ ë§í¬ ì°¸ê³ .
  - Sutton book 13.1
  - https://talkingaboutme.tistory.com/entry/RL-Policy-Gradient-Algorithms (í•œêµ­ì–´ ë¸”ë¡œê·¸) â€¢ https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#reinforce
  Reinforcement Learning

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: REINFORCE (Monte-Carlo policy gradient)
REINFORCE(Monte-Carlo policy gradient)ëŠ” episode ìƒ˜í”Œ ë‚´ì—ì„œ Monte-Carlo methodë¥¼ í†µí•´ êµ¬í•œ estimated returnì„ ê°€ì§€ê³  policy parameter ğœƒë¥¼ updateí•´ ë‚˜ê°€ëŠ” ê¸°ë²•!
  
ì´ë•Œ, sampleë¡œ êµ¬í•œ gradient ì˜ expectationê³¼ actual gradient ì˜ expectationì´ ë™ì¼í•˜ë¯€ë¡œ sampling ì´ ìœ íš¨í•˜ë‹¤.

<img width="350" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 16 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea53ecbe-dec1-48f4-9bc2-8fdb43412e7d">

ìœ„ ì‹ì„ í†µí•˜ì—¬, real sample trajectoryë“¤ë¡œë¶€í„° total expected return( $ğº_t$ )ë¥¼ êµ¬í•  ìˆ˜ ìˆê³ , ì´ë¥¼ ì´ìš©í•´ gradientì˜ updateê°€ ê°€ëŠ¥í•˜ë‹¤!  
ì´ëŠ” ë‹¤ë§Œ, ì™„ì „í•œ trajectoryì— ì˜ì¡´í•˜ê¸°ì—, íš¨ìœ¨ì´ ì¡°ê¸ˆ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: REINFORCE (Monte-Carlo policy gradient)
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 17 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12b252f9-b3db-42ad-9e4c-c73d3b263a18">

Monte-Carlo ë°©ë²•ì€ ê·¸ëŸ¬ë‚˜, high variance ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆê³ , returnì„ êµ¬í•  ë•Œ episodeê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë³µì¡í•œ ë¬¸ì œì—ì„œ ì˜¤ë˜ ê±¸ë¦¬ê±°ë‚˜ ìˆ˜ë ´í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: Actor-Critic

Policy gradientì˜ ë‘ ê°€ì§€ êµ¬ì„±ìš”ì†Œ: policy modelê³¼ value function    
Actor-Criticì˜ ì•„ì´ë””ì–´:
- Policyë¥¼ í•™ìŠµí•  ë•Œ value functionì„ í•™ìŠµí•˜ë©´ ë” ì¢‹ì€ ì„±ëŠ¥!
- Value functionì„ ì´í•´í•˜ëŠ” ê²ƒ ìì²´ê°€ vanilla policy gradientì—ì„œ gradient varianceë¥¼ ì¤„ì´ëŠ” ë“± policyë¥¼ updateí•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?
  
- 2ê°œì˜ ëª¨ë¸ë¡œ êµ¬ì„±
  - Critic ëª¨ë¸: Value function(Action-value $ğ‘„_w (ğ‘,ğ‘ )$ or state-value $ğ‘‰_w (ğ‘|ğ‘ )$ )ì˜
parameter wë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.
    - ì–¼ë§ˆë‚˜ actorì˜ actionì´ ì¢‹ì•˜ëŠ”ì§€ íŒë‹¨í•˜ê³  actorì—ê²Œ ìˆ˜ì • ë°©í–¥ì„ ì•Œë¦°ë‹¤.
      
  - Actor ëª¨ë¸: Policy ( $ğœ‹_\theta(ğ‘|ğ‘ )$ ) parameter ğœƒ ë¥¼ criticì´ ì œì•ˆí•˜ëŠ” ë°©í–¥ëŒ€ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤.
    - ì–´ë–¤ actionì„ ì·¨í•  ê²ƒì¸ì§€ ê²°ì •í•œë‹¤.
   
## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: Actor-Critic
Value functionì„ ì´í•´í•˜ëŠ” ê²ƒ ìì²´ê°€ vanilla policy gradientì—ì„œ gradient varianceë¥¼ ì¤„ì´ëŠ” ë“±
policyë¥¼ updateí•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?
- Varianceë¥¼ baseline í•¨ìˆ˜ë¥¼ ë¹¼ì£¼ì–´ ì¤„ì—¬ë³´ì! REINFORCEì˜ gradient ascent

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 27 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b22d2ddb-7400-4d5e-b4d8-4e97fa368bca">

ì—ì„œ, baseline í•¨ìˆ˜(ğ‘(ğ‘ ))ë¥¼ ì •í•´ì„œ cumulative reward( $ğº_t$ )ì—ì„œ ë¹¼ì:

<img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 30 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6e3a0d9a-5ee0-4591-b419-aeffbfca8f9d">

$ğº_t$ âˆ’ ğ‘(ğ‘ ) ì˜ ì¢…ë¥˜ì— ë”°ë¼ ì¢…ë¥˜ê°€ ë‹¬ë¼ì§„ë‹¤!
- $ğº_t$ : REINFORCE
- ğ›¿: TD actor-critic
- $ğ‘„^w (ğ‘ , ğ‘)$ : Q actor-critic
- $ğ´^w (ğ‘ , ğ‘)$ : Advantage actor-critic

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: Actor-Critic (TD actor-critic)
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 32 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/93770f29-db5b-4f02-af5b-7f7aa1088739">

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: Actor-Critic (Q actor-critic)
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 33 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9770bccb-87c3-4ca4-a037-494ce05fd3ff">

ë¡œ í‘œí˜„ê°€ëŠ¥í•˜ê³ , $ğ¸_{r_{t+1}, S_{t+1}, ..., r_T, S_T} [ğº_T]$ ëŠ” Q-table( $ğ‘„_w(ğ‘ ,ğ‘)$ )ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤!

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 35 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7029a334-ab49-454c-b3d9-fcc8cc6c22a4">

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) ì•Œê³ ë¦¬ì¦˜: Actor-Critic (Advantageous actor-critic)
Q - actor critic <img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 37 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8c608f68-ea34-4434-8908-c3a8a83c8ca1">
ì—ì„œ,  
$ğ‘„_w(ğ‘ _t,ğ‘_t)$ ì—ì„œ baseline function $ğ‘‰_v (ğ‘ _t)$ ë¥¼ ë¹¼ì„œ ê°œì„ í•˜ë©´? -> Advantage value  
  - $ğ´(ğ‘ _t, ğ‘_t) = ğ‘„_w(ğ‘ _t,ğ‘_t) âˆ’ ğ‘‰_\pi(s_t)$
    
Bellman optimality equationì— ë”°ë¼, ğ‘„(s_t, a_t) = ğ¸[ $ğ‘Ÿ_{t+1} + ğ›¾ğ‘‰(s_{t+1})$ ]ì´ë¯€ë¡œ,
  - $ğ´(ğ‘ _t, ğ‘_t) = ğ‘Ÿ_{t+1} + ğ›¾ğ‘‰_\pi(s_{t+1}) âˆ’ ğ‘‰_\pi(s_{t})$ ì´ë‹¤.

ì •ë¦¬í•˜ë©´, update equationì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
<img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 43 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bcd21183-2c7c-4b9b-bf82-640ebb74361c">

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) - Actor-Critic (Advantageous actor-critic)
$ğ´_\Pi(ğ‘ _t, ğ‘_t) = ğ‘Ÿ_{t+1} + ğ›¾ğ‘‰_\pi(s_{t+1}) âˆ’ ğ‘‰_\pi(s_{t})$  
Advantage Actor-Critic ë°©ë²•ì˜ ì¢…ë¥˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.  
  
TD advantage estimate: ì¦‰ê°ì ì¸ reward ì‚¬ìš©(ê¸°ë³¸ format)
- $ğ´_\Pi(ğ‘ _t, ğ‘_t) = ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ')/ + ğ›¾ğ‘‰_\pi(s_{t+1}) âˆ’ ğ‘‰_\pi(s_{t})$

Monte-Carlo(MC) advantage estimate: ë§Œì•½ Q-valueì„ ì‹¤ì œ returnë¡œ ë°”ê¿€ ê²½ìš°. 
- $ğ´_\Pi(ğ‘ _t, ğ‘_t) = ğº_t âˆ’ ğ‘‰_\Pi(s_t) = ğ‘…(ğ‘ ,ğ‘) âˆ’ ğ‘‰_\pi(s_{t})$
  
N-step advantage estimate: N step returnì„ ì‚¬ìš© (ê°€ì¥ ë§ì´ ì‚¬ìš©)
- A2C/A3C
- <img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 5 50 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/40f3e55f-3350-409e-be3a-e46449d32869">
- MCì™€ TD ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ë‘˜ë‹¤ ê³„ìŠ¹

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) - A2C/A3C
- Critic ëª¨ë¸: Value function(Actionâˆ’value ğ‘„4 ğ‘ ğ‘  or stateâˆ’value ğ‘‰4(ğ‘|ğ‘ ))ì˜ parameter wë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.
  - ì–¼ë§ˆë‚˜ actorì˜ actionì´ ì¢‹ì•˜ëŠ”ì§€ íŒë‹¨í•˜ê³  actorì—ê²Œ ìˆ˜ì • ë°©í–¥ì„ ì•Œë¦°ë‹¤.
  - State-value ğ‘½ğ“(ğ’‚|ğ’”) ë¥¼ return
  - Loss: $(ğ‘… âˆ’ğ‘‰_\Pi (s))^2$

- Actor ëª¨ë¸: Policy (ğœ‹!(ğ‘|ğ‘ )) parameter ğœƒ ë¥¼ criticì´ ì œì•ˆí•˜ëŠ” ë°©í–¥ëŒ€ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤.
  - ì–´ë–¤ actionì„ ì·¨í•  ê²ƒì¸ì§€ ê²°ì •í•œë‹¤.
  - ğ…ğœ½(ğ’‚|ğ’”)ì„ return!
  - Loss : $âˆ‡_\theta log ğœ‹_\theta (ğ‘ ,ğ‘) (ğ‘… âˆ’ ğ‘‰_\Pi (ğ‘ ))$

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) â€’ A2C/A3C
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 19 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c287db13-d285-4ed1-9f61-06ba42e6e10d">

<img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 20 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/46bf1674-f572-4050-8685-620e970c0851">

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) â€’ A3C
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 20 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ff8688ed-1b8c-491a-bfc1-d73685a69bdd">

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 20 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0a030731-7fff-4d57-a2d3-38b1fe5fc3a6">

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) â€’ ê²°ê³¼
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 21 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/aed38f08-05d9-4456-8d38-0062875f11a1">

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 21 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d2c89411-219b-46d6-ac76-414f29e0658a">

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 22 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/85bbb728-0ebf-4860-9152-62ac43779da3">

<img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 23 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fcbe4b32-3186-4229-8ebd-772a86ba4881">

Mnih, Volodymyr, et al. "Asynchronous methods for deep reinforcement learning." International conference on machine learning. PMLR, 2016.

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Model-free RL â€’ policy-based (policy optimization) â€’ A2C vs A3C
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 23 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1ba181b6-4a3d-49ab-a048-5bfd5e8d6b86">

A2CëŠ” A3Cì˜ synchronousí•˜ê³  deterministic ë²„ì „ì„ ë§í•œë‹¤! 
- Inconsistencyë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ coordinatorë¥¼ ë„ì…
- ìµœì í™”ë‚˜ ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ A2Cê°€ GPU ì—°ì‚° ë“±ì—ì„œ ë³´ë‹¤ íš¨ìœ¨ì ì´ë¼ ì•Œë ¤ ì ¸ìˆë‹¤!
  - ì°¸ê³ : https://openai.com/blog/baselines-acktr-a2c/
