# íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)- 2. Variations of Transformers

## Transformerì˜ ë‹¤ì–‘í•œ í›„ì† ì—°êµ¬ë“¤
<img width="435" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 55 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/02e7f10f-ba48-4117-a221-95fce05ecb5c">

ì£¼ë¡œ ì•„ë˜ ë°©í–¥ìœ¼ë¡œ ë°œì „!  
1. Complexityì˜ ê°œì„  (í•˜ë‹¨ì˜ efficient transformer êµ¬ë¶„í‘œ)
2. ì„±ëŠ¥ê°œì„   
3. ë„ë©”ì¸ í™•ì¥  
  
ê°œì„  ë°©ë²•ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ ë²”ì£¼í™” í•  ìˆ˜ ìˆë‹¤!  
- Module level
- Architecture level
- Pre-Train
- Application

  <img width="322" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 58 11" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/03caa91b-e675-4888-a95a-8dd6ac3cfa2f">

  <img width="515" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 56 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8b1bb5bf-bfc9-4c0c-81e2-4635a8392ac0">

## Transformerì˜ ë‹¤ì–‘í•œ í›„ì† ì—°êµ¬ë“¤ - Complexity / ì„±ëŠ¥ì˜ ê°œì„ 
<img width="363" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 58 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/32e9285e-2548-48d4-9859-e7655a9fd9d8">

<img width="396" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 59 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7eb7cf00-c4e7-424c-b719-4ba08b347393">

## í›„ì† ì—°êµ¬ê°€ ë˜ì–´ ë§ì€ ì¢…ë¥˜ì˜ Transformerê°€ ìˆë‹¤!- Recap: íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer) & Computational Costs
<img width="600" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/166d421e-1920-444b-a2f5-59a82c521286">

<img width="600" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 01 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b3cd850d-9c1c-4f68-a797-0ea45fda859a">

<img width="600" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 02 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8a86d111-4ef3-41a4-b4a4-98f2fcac630c">

- TransformerëŠ” RNNë“±ê³¼ ë¹„êµí–ˆì„ë•Œ, íš¨ìœ¨ì ìœ¼ë¡œ ë³‘ë ¬ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤.
- Self-attention layerì—ì„œ ì‹œí€€ìŠ¤ê¸¸ì´ë¥¼ n, input-sizeì„ dë¼ê³  í–ˆì„ë•Œ, íŒŒë¼ë¯¸í„°ìˆ˜ëŠ” $4d^2$ì´ë©° ì—°ì‚°ë³µì¡ë„(computational complexity)ëŠ” $O(n^2 * d)$ ì´ë‹¤.
- Self-attention ì´í›„ì˜ position-wise FFDì˜ ê²½ìš°, íŒŒë¼ë¯¸í„°ìˆ˜ëŠ” $8d^2$ì´ë©° ì—°ì‚°ë³µì¡ë„ëŠ” $O(n * d^2)$ì´ë‹¤.

## Variations of Transformers - Module-levelì˜ ê°œì„  - Attention
Self-attentionì€ Transformerì˜ í•µì‹¬êµ¬ì„±ìš”ì†Œì´ì§€ë§Œ, í›„ì†ì—°êµ¬ì—ì„œ í¬ê²Œ 2ê°€ì§€ ë¬¸ì œê°€ ì œê¸°ë˜ì—ˆë‹¤.  
1. Complexity(ë³µì¡ì„±)
   - $O(n^2 * d)$ì˜ ì†ë„ëŠ” ë‚˜ì˜ì§€ ì•Šì§€ë§Œ ì „ì²´ transformerì˜ ì—°ì‚°ì¤‘ bottle-neckì´ë‹¤. ë§Œì•½ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆë‹¤ë©´, ì „ì²´ì ì¸ ì†ë„ê°€ ë¹¨ë¼ì§ˆ ê²ƒì´ë‹¤.
     -> Efficient transformer
2. Structural prior(êµ¬ì¡°ì ì¸ prior)
   - Self-attentionì€ CNN,RNNê³¼ ë‹¤ë¥´ê²Œ inputì— ëŒ€í•œ ê°€ì •ì´ ì—†ë‹¤.
   - CNNì€ ì£¼ë³€ì˜ ì¸ê·¼ featureë“¤ì´ ê´€ë ¨ì´ ìˆì„ê²ƒì´ë¼ëŠ” ì •ë³´, RNNì€ Time-seriesì´ë¯€ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ timeì— overí•´ì„œ ì–»ì„ê²ƒì´ë¼ëŠ” ì •ë³´ê°€ ìˆëŠ” ë°˜ë©´, Self-attentionì€ ì—†ë‹¤.
   - ê·¸ëŸ¬ë¯€ë¡œ pre-trainingë“±ì´ ì—†ë‹¤ë©´, ì‘ê±°ë‚˜ ì ë‹¹í•œ ì‚¬ì´ì¦ˆì˜ ë°ì´í„°ì—ì„œ overfití•˜ê¸° ì‰½ë‹¤.

## Variations of Transformers - Module-levelì˜ ê°œì„  - Attention
Complexity, Structural prior ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì´ ì œì‹œë˜ì—ˆë‹¤.
1. Sparse attention: sparsity biasì„ attention mechanism ì— ë”í•´ ì—°ì‚°ì„ ì¤„ì¸ë‹¤.
2. Linearized attention: attention matrix ì—°ì‚°ì„ kernel feature mapë¡œ ë¶„ë¦¬(disentangle)í•œë‹¤. ê·¸ ì´í›„, ì„ í˜• ë³µì¡ì„±ì„ ì´ë£¨ê¸° ìœ„í•´ ì—­ìˆœìœ¼ë¡œ attentionì„ ê³„ì‚°í•œë‹¤.
3. Query prototype & memory compression: attentionì˜ key, query, value memory pairì˜ ìˆ˜ë¥¼ ì¤„ì—¬ attention matrix ì—°ì‚°ì„ ì¤„ì¸ë‹¤.
4. Low-rank self-attention: low-rank (ë‚®ì€ ê³„ìˆ˜)ë¡œ self-attention ì„ ì •ì˜í•˜ì—¬ ì—°ì‚°ì„ ì¤„ì¸ë‹¤.
5. Attention with prior: CNN/RNN ëª¨ë¸ì„ ì¶”ê°€í•˜ì—¬ prior knowledgeì„ ë³´ì™„í•˜ê±°ë‚˜, ê¸°ëŠ¥ì ìœ¼ë¡œ priorì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ê°œì„ í•œë‹¤.
6. Improved multi-head mechanism: multi-head attention ë¡œì§ ìì²´ë¥¼ ê°œì„ í•œë‹¤.

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Attention: sparse attention
- Attentionì€ ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  queryì— attendë¥¼ ì‹œë„í•œë‹¤.
- ì‹¤ì œë¡œ attentionì´ í•„ìš”í•œ ì˜ì—­ì€ ì¼ë¶€ì— ë¶ˆê³¼í•˜ë‹¤ -> ì œí•œëœ ì˜ì—­ì—ì„œë§Œ sparseí•˜ê²Œ ì§‘ì¤‘í•˜ê²Œ í•˜ë©´ ì–´ë–¨ê¹Œ?
  
  <img width="600" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 21 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d980510-2307-4039-8d92-c821938eb2e2">

- Position ê¸°ë°˜ sparse attention : ë¯¸ë¦¬ ì •ì˜ëœ íŒ¨í„´ìœ¼ë¡œ sparsityì„ ì •ì˜í•œë‹¤.
  - ì™¼ìª½ì— ìˆëŠ” predefinedëœ ëª¨í˜•ì„ ì¡°í•©í•˜ì—¬ ë§Œë“¤ ìˆ˜ìˆë‹¤.
  - star-transformer, long-transformer, big-birdë“±ì´ ì†í•œë‹¤.
    
  <img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 23 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b5d3a3e5-0b07-4d41-8cf8-aa22bedab84b">
  <img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 23 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/95d4e583-646c-4594-ab48-95a32d6c3d8f">

- Content ê¸°ë°˜ sparse attention:  input contentì— ë”°ë¼ sparse graphë¥¼ ìƒì„±í•œë‹¤. ì¦‰, inputì— ë”°ë¼ sparse attentionì˜ í˜•íƒœê°€ ë‹¬ë¼ì§„ë‹¤.
  - Routing transformer (K-means clustering ê¸°ë°˜), Reformer(locality-sensitive hashing; LSH ê¸°ë°˜)ê°€ ì´ì— ì†í•œë‹¤.
  - O(nkd + n2d/k) : n routing vectors to all k centroids in a space of size d

  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 25 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5402a902-af57-42ff-8e31-4905c59edbc8">
  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 25 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea7128a5-22ee-4987-9b50-3cc9c8adf7c0">
  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 26 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0798d9d8-aed3-4ce2-b81a-75c25b1354aa">

## Variations of Transformers- Module-levelì˜ ê°œì„  â€“ Attention: linearized attention
Linearized attention: Kernel method ë“±ì„ ì´ìš©í•˜ì—¬ bottleneckì¸ matrixì—°ì‚° ë° softmaxë¥¼ linearì—°ì‚°ìœ¼ë¡œ ì¤„ì¸ë‹¤!
- Performer, RFA(random feature attention) ë“±ì´ ì—¬ê¸° ì†í•œë‹¤.

  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 27 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/28596132-0822-445c-ac87-c3e11d8c9044">
  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 28 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9087ad16-2224-4ad2-96c6-39379fed8d47">
  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 28 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/25cd8a6f-6ec1-4d8e-8264-f6b123baad26">

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Attention: query prototype & memory compression
Query prototype & memory compression  
- query(ë””ì½”ë”ì˜ ì§‘ì¤‘í•˜ê³ ì í•˜ëŠ” ë¶€ë¶„; query prototype)ë‚˜ key-value
- (ì¸ì½”ë” ë¶€ë¶„; memory compression) pairì˜ í¬ê¸°ë¥¼ ì¤„ì—¬
- computational complexityì„ ì¤„ì´ëŠ” ë°©ë²•.  

  <img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 31 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e10e7196-9832-4d53-8184-9d9263bc5efa">
  <img width="600" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 31 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60006792-ce49-43a7-9c78-f88b499f009e">
  
Attention with query prototype  
- Clustered attention, Informer ë“±ì´ ì†í•œë‹¤.
    
Attention with compressed key-value memory
- Memory compressed attention(MCA),  Set Transformer ë“±ì´ ì†í•œë‹¤.

  <img width="509" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 33 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9f814b0c-9d8f-4f14-a1df-9d2b91d5c745">

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Attention: low-rank self-attention

Low - rank self -attention :  
self-attention ì—°ì‚°ì€ ë•Œë–„ë¡œ low-rank(Attention-matrix Aì˜ rankê°€ input length Të³´ë‹¤ ì‘ë‹¤)ì¼ìˆ˜ìˆë‹¤ê³  ì‹¤í—˜ì /ì´ë¡ ì ìœ¼ë¡œ ì•Œë ¤ì ¸ìˆë‹¤.  

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 42 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c879c4e4-1109-4b2f-a4a5-c329edf2f617">
<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 42 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60b15fa9-1830-4b0b-8464-38299f3d36f2">

ì´ëŸ¬í•œ ì„±ì§ˆì„ ì´ìš©í•˜ëŠ” 2ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.  
1. Parametrizationë¡œ low-rank propertyë¡œ ëª¨ë¸ë§ í•˜ëŠ” ë°©ë²•
   -> keyì˜ dimensionì„ ì œí•œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ low-rank propertyì„ explicití•˜ê²Œ ëª¨ë¸ë§í•˜ì—¬, íŒŒë¼ë¯¸í„°ìˆ˜ë¥¼ ì¤„ì´ê³  ê³¼ì í•©(overfitting)ì„ ë°©ì§€í•œë‹¤.
2. self-attention matrixì„ low-rank approximationë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ë²•
   -> low-rank approximationìœ¼ë¡œ self-attentionì˜ ë³µì¡ë„ë¥¼ ì¤„ì¸ë‹¤.
  ì˜ˆ) compressed self-attention for deep metric learning with low-rank approximation(CSALR), Nystromformer

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Attention: with prior knowledge
Attentionì€ ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ì˜ì—­ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ë‘ê³  í•™ìŠµì„ ì‹œì‘í•œë‹¤.  
íŠ¹ì • ë„ë©”ì¸ì˜ ë°ì´í„° ë“±ìœ¼ë¡œ ë¶€í„° ì–»ì€ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ë³¸ knowledge ë¥¼ attention ì˜ ì„ í—˜ì§€ì‹(prior)ë¡œ í™œìš©í•œë‹¤.  

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 7 44 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5be01e70-c00b-4cc2-b809-601f075eb765">

í¬ê²Œ, ì•„ë˜ì˜ ì¹´í…Œê³ ë¦¬ ë“±ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤: 
1. ë°ì´í„°ì˜ íŠ¹ì„±ì— ëŒ€í•œ localityì„ ë°˜ì˜í•˜ëŠ” priorì„ ì´ìš©í•˜ëŠ” ëª¨ë¸
- i.e. Gaussian transformer
2. CNNê³¼ ê°™ì€ lower moduleë¡œë¶€í„° priorì„ ì–»ëŠ” ëª¨ë¸
- i.e. predictive attention transformer, realformer
3. Multi-task adapterì„ ì´ìš©í•˜ëŠ” ëª¨ë¸
- ì…ë ¥ ê°„ì˜ pair-wise ìƒí˜¸ ì‘ìš©ê³¼ ê´€ë ¨ ì—†ëŠ” attentionì„ ì‚¬ìš©í•˜ì—¬ íƒìƒ‰í•˜ëŠ” ë°©ë²•
ì´ ê²½ìš°, ë³´í†µ ëª¨ë¸ì€ ì‚¬ì „ ì£¼ì˜ ë¶„í¬ë§Œ ì´ìš©í•œë‹¤

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Attention: multi-head mechanism improvement
Multi-head mechanism improvement  
- Transformerì˜ Multi-head êµ¬ì¡°ë¥¼ ë³´ì™„í•˜ì—¬ ê°œì„ í•œë‹¤.
- Head behavior modeling:
  -> Vanilla transformer ì—ì„œëŠ” ê° Headê°€ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šê³  ì„œë¡œ ë…ë¦½ì ì´ë‹¤.
    $head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)$  
    $MultiHead(Q,K,V) = Concat(head_i, ..., head_h)W^O$
  -> Headê°€ ì„œë¡œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ì§€ë¥¼ ì£¼ì–´ ì„±ëŠ¥ì„ ê°œì„ í•œë‹¤ (i.e. collaborative multi-head attention)
    (ëª¨ë“  Headê°€ ë™ì¼í•œ Wì„ ê³µìœ í•˜ë©°, mixing vector $m_i$ ë¡œ ê° headë¥¼ ë¶„ë¦¬í•œë‹¤.
    $head_i = Attention(Q W^Q_i diag(m_i), K W^K_i, V W^V_i)$  
  - ê·¸ë°–ì— Talking-heads attention, auxiliary disagreement regularization termë“±ì˜ ì—°êµ¬ê°€ ìˆë‹¤.
 
Multi-head with Refined Aggregation :  
-> $MultiHead(Q,K,V) = Concat(head_i, ..., head_h)W^O$  
  Multi-head attentionì„ headë“¤ë¡œ aggregationí• ë•Œ, projectionë“±ì„ í•˜ì—¬ ì •ì œí•œë‹¤.  
   $MultiHead(Q,K,V)$ = <img width="350" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 35 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6ad36fe2-6927-4985-a44d-19461ed06b3a">

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Attention: multi-head mechanism improvement
Multi-head mechanism improvement  
- Multi-head with Restricted Spans
  - Head ë³„ë¡œ attention ì„ ìˆ˜í–‰í•˜ëŠ” ë²”ìœ„(span)ì„ ì œí•œ -> Head ë³„ë¡œ ë‹¤ë¥¸ contextì„ ê´€ì¸¡í•  ìˆ˜ ìˆìŒ! 
- 2ê°€ì§€ ì¥ì .
  1. Locality: ì •í™•íˆ ì–´ë””ë¥¼ ë³¼ì§€ë¥¼ ì§€ì • ğŸ¡ª ìš°ë¦¬ê°€ ì–´ë–¤ localityê°€ ì¤‘ìš”í•œì§€ ë°ì´í„°ë¥¼ í†µí•´ ì•Œê³  ìˆë‹¤ë©´ ì´ìµì„ ë³¼ ìˆ˜ ìˆë‹¤!
  2. Efficiency: spanì„ ì ì ˆí•˜ê²Œ ì œí•œí•˜ì—¬, ì¶”ê°€ì ì¸ ë©”ëª¨ë¦¬/ì»´í“¨íŒ… ìì› ì†Œëª¨ ì—†ì´ long sequenceì„ ì²˜ë¦¬í•˜ë„ë¡ scaling í•  ìˆ˜ ìˆë‹¤!
     
  <img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 38 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6a60dbb4-924f-4473-8619-747e1f546c5f">

- ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ (b) ì™€ ê°™ì´ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° R ë“±ì„ ì„¸íŒ…í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ë„ ìˆë‹¤.
- ë˜í•œ, multi-scale vision longformerì˜ ê²½ìš°, layer ì™€ head ì— ë”°ë¼ window ì˜ í¬ê¸°ë§Œ ë‹¬ë¼ì§€ëŠ”
 â€œfixed attention spanâ€ì„ ì œì•ˆí•˜ì˜€ë‹¤.

## Variations of Transformers - Module-levelì˜ ê°œì„  â€“ Positional representation
Transformerì—ì„œëŠ” positionì •ë³´ë¥¼ ë„£ì–´ì£¼ëŠ”ë° ê·¸ ë„£ì–´ì£¼ëŠ” ë°©ë²•ì— ë”°ë¼ performanceê°€ ë‹¬ë¼ì§ˆ ìˆ˜ë„ ìˆë‹¤.
1. Absolute position
- ì› ë…¼ë¬¸ì€ ì‚¼ê°í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ positional encodingì„ êµ¬í˜„í•˜ì˜€ë‹¤.

  <img width="468" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 45 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/df5a2b6a-f489-4311-b1ff-0a4505250842">
  - pos: tokenì˜ ìœ„ì¹˜ index, i: ë²¡í„°ì˜ ì°¨ì› index
- ê·¸ë ‡ì§€ë§Œ, ì•ì„œ ì´ì•¼ê¸°í–ˆë“¯ì´, ë‹¤ë¥¸ í•¨ìˆ˜ë‚˜ ê·œì¹™ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤!

2. Relative position
- ì‹¤ì œ ìœ„ì¹˜ë³´ë‹¤, ê° ì „í›„ inputê°„ì˜ ê´€ê³„ê°€ ë” ì¤‘ìš”í•œ ê²½ìš°, relative positionì„ embeddingí•˜ëŠ” ê²ƒì´ ë” ì¢‹ì„ ìˆ˜ ìˆë‹¤. (i.e. Music Transformer)
- ì´ relative position representation ì„ ê³ ì •í•˜ì§€ ì•Šê³ , í•™ìŠµ ê°€ëŠ¥í•œ embedding ì •ë³´ë¡œ ì ìš©í•  ìˆ˜ë„ ìˆë‹¤.
- ê´€ë ¨ëœ transformerë“¤
	i.e. Transformer-XL, DeBERTa, Music Transformerâ€¦

3. Without explicit encoding
- ëª…í™•í•œ ì¸ì½”ë”©ì„ í•˜ì§€ì•Šê³ , ê°„ì ‘ì ìœ¼ë¡œ ë‹¤ë¥¸ ì¡°ê±´ ë¡œì§ / neural net ë“±ì„ í†µí•´ ê°„ì ‘ì ìœ¼ë¡œ ì•Œë ¤ì¤€ë‹¤.
- i.e. R-transformer, Conditional positional encoding

## Variations of Transformers - Architecture-levelì—ì„œì˜ ê°œì„ : recurrent / hierarchical
<img width="600" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 47 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/19c6c0d7-9edb-458b-9f81-1def1b8dac39">

ì‹œí€€ìŠ¤ ê¸¸ì´ì— ëŒ€í•œ self-attentionì˜ 2ì°¨ì  ë³µì¡ì„±ì€ ì¼ë¶€ down streaming taskì˜ ì„±ëŠ¥ì„ í¬ê²Œ ì œí•œí•  ìˆ˜ ìˆë‹¤.  
ì˜ˆë¥¼ ë“¤ì–´, ì–¸ì–´ ëª¨ë¸ë§ì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¥ê±°ë¦¬ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë‹¤!  
- Recurrent, hierarchical í•œ ë””ìì¸ì´ ì„±ëŠ¥í–¥ìƒì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤.
- [Examples]
  - Recurrent: Transformer-XL, Memformer
  - Hierarchical: Hi-Transformer, HIBERT, TENER

## Variations of Transformers - Applicationsì˜ í™•ì¥ (vision / video)
<img width="500" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1a103f34-db88-47d2-95a6-66d69768915a">  
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 51 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/db580b35-026a-45a0-a227-9c24d06d8a04">  
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 51 25" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/70bbff01-1310-41cc-afea-03138c68de95">  
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„Œá…¥á†« 11 51 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8d50b750-b4d8-42a1-b966-801cc16cdc51">  
- Tay, Yi, et al. "Efficient transformers: A survey." arXiv preprint arXiv:2009.06732 (2020).
- Tay, Yi, et al. "Long range arena: A benchmark for efficient transformers." arXiv preprint arXiv:2011.04006 (2020).
- Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).
- Girdhar, Rohit, et al. "Video action transformer network." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

## Variations of Transformers - Applicationsì˜ í™•ì¥ (speech synthesis(TTS) / speech recognition(STT))
<img height="400" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/da306968-3357-48da-b53b-c5a04a1bb68f">
<img height="400" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/515162cd-266d-4bd3-b1aa-8231dcd7668f">
<img height="300" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b7a239fe-0fe5-4383-91fe-aee03f4dbf01">   

- Dong, Linhao, Shuang Xu, and Bo Xu. "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition." 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.
- Li, Naihan, et al. "Neural speech synthesis with transformer network." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.

## Variations of Transformers - Applicationsì˜ í™•ì¥ (graph network / reinforcement learning)
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„’á…® 12 04 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3f4be30f-09d9-47dd-af6d-6f03c74f58c9">
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-04 á„‹á…©á„’á…® 12 04 57" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ebeef5f9-0bf8-4795-a882-b04d52d95a8a">

- Yun, Seongjun, et al. "Graph transformer networks." Advances in Neural Information Processing Systems 32 (2019): 11983-11993.
- Chen, Lili, et al. "Decision transformer: Reinforcement learning via sequence modeling." arXiv preprint arXiv:2106.01345 (2021).


