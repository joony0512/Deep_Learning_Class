# 트랜스포머(Transformer)- 2. Variations of Transformers

## Transformer의 다양한 후속 연구들
<img width="435" alt="스크린샷 2023-08-03 오후 6 55 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/02e7f10f-ba48-4117-a221-95fce05ecb5c">

주로 아래 방향으로 발전!  
1. Complexity의 개선 (하단의 efficient transformer 구분표)
2. 성능개선  
3. 도메인 확장  
  
개선 방법에 따라 아래와 같이 범주화 할 수 있다!  
- Module level
- Architecture level
- Pre-Train
- Application

  <img width="322" alt="스크린샷 2023-08-03 오후 6 58 11" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/03caa91b-e675-4888-a95a-8dd6ac3cfa2f">

  <img width="515" alt="스크린샷 2023-08-03 오후 6 56 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8b1bb5bf-bfc9-4c0c-81e2-4635a8392ac0">

## Transformer의 다양한 후속 연구들 - Complexity / 성능의 개선
<img width="363" alt="스크린샷 2023-08-03 오후 6 58 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/32e9285e-2548-48d4-9859-e7655a9fd9d8">

<img width="396" alt="스크린샷 2023-08-03 오후 6 59 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7eb7cf00-c4e7-424c-b719-4ba08b347393">

## 후속 연구가 되어 많은 종류의 Transformer가 있다!- Recap: 트랜스포머 (Transformer) & Computational Costs
<img width="600" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/166d421e-1920-444b-a2f5-59a82c521286">

<img width="600" alt="스크린샷 2023-08-03 오후 7 01 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b3cd850d-9c1c-4f68-a797-0ea45fda859a">

<img width="600" alt="스크린샷 2023-08-03 오후 7 02 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8a86d111-4ef3-41a4-b4a4-98f2fcac630c">

- Transformer는 RNN등과 비교했을때, 효율적으로 병렬계산이 가능하다.
- Self-attention layer에서 시퀀스길이를 n, input-size을 d라고 했을때, 파라미터수는 $4d^2$이며 연산복잡도(computational complexity)는 $O(n^2 * d)$ 이다.
- Self-attention 이후의 position-wise FFD의 경우, 파라미터수는 $8d^2$이며 연산복잡도는 $O(n * d^2)$이다.

## Variations of Transformers - Module-level의 개선 - Attention
Self-attention은 Transformer의 핵심구성요소이지만, 후속연구에서 크게 2가지 문제가 제기되었다.  
1. Complexity(복잡성)
   - $O(n^2 * d)$의 속도는 나쁘지 않지만 전체 transformer의 연산중 bottle-neck이다. 만약 빠르게 할 수 있다면, 전체적인 속도가 빨라질 것이다.
     -> Efficient transformer
2. Structural prior(구조적인 prior)
   - Self-attention은 CNN,RNN과 다르게 input에 대한 가정이 없다.
   - CNN은 주변의 인근 feature들이 관련이 있을것이라는 정보, RNN은 Time-series이므로 관련된 정보를 time에 over해서 얻을것이라는 정보가 있는 반면, Self-attention은 없다.
   - 그러므로 pre-training등이 없다면, 작거나 적당한 사이즈의 데이터에서 overfit하기 쉽다.

## Variations of Transformers - Module-level의 개선 - Attention
Complexity, Structural prior 문제를 해결하기 위한 다양한 방법들이 제시되었다.
1. Sparse attention: sparsity bias을 attention mechanism 에 더해 연산을 줄인다.
2. Linearized attention: attention matrix 연산을 kernel feature map로 분리(disentangle)한다. 그 이후, 선형 복잡성을 이루기 위해 역순으로 attention을 계산한다.
3. Query prototype & memory compression: attention의 key, query, value memory pair의 수를 줄여 attention matrix 연산을 줄인다.
4. Low-rank self-attention: low-rank (낮은 계수)로 self-attention 을 정의하여 연산을 줄인다.
5. Attention with prior: CNN/RNN 모델을 추가하여 prior knowledge을 보완하거나, 기능적으로 prior을 추가할 수 있도록 개선한다.
6. Improved multi-head mechanism: multi-head attention 로직 자체를 개선한다.

## Variations of Transformers - Module-level의 개선 – Attention: sparse attention
- Attention은 기본적으로 모든 query에 attend를 시도한다.
- 실제로 attention이 필요한 영역은 일부에 불과하다 -> 제한된 영역에서만 sparse하게 집중하게 하면 어떨까?
  
  <img width="600" alt="스크린샷 2023-08-03 오후 7 21 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d980510-2307-4039-8d92-c821938eb2e2">

- Position 기반 sparse attention : 미리 정의된 패턴으로 sparsity을 정의한다.
  - 왼쪽에 있는 predefined된 모형을 조합하여 만들 수있다.
  - star-transformer, long-transformer, big-bird등이 속한다.
    
  <img width="400" alt="스크린샷 2023-08-03 오후 7 23 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b5d3a3e5-0b07-4d41-8cf8-aa22bedab84b">
  <img width="400" alt="스크린샷 2023-08-03 오후 7 23 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/95d4e583-646c-4594-ab48-95a32d6c3d8f">

- Content 기반 sparse attention:  input content에 따라 sparse graph를 생성한다. 즉, input에 따라 sparse attention의 형태가 달라진다.
  - Routing transformer (K-means clustering 기반), Reformer(locality-sensitive hashing; LSH 기반)가 이에 속한다.
  - O(nkd + n2d/k) : n routing vectors to all k centroids in a space of size d

  <img width="500" alt="스크린샷 2023-08-03 오후 7 25 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5402a902-af57-42ff-8e31-4905c59edbc8">
  <img width="500" alt="스크린샷 2023-08-03 오후 7 25 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea7128a5-22ee-4987-9b50-3cc9c8adf7c0">
  <img width="500" alt="스크린샷 2023-08-03 오후 7 26 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0798d9d8-aed3-4ce2-b81a-75c25b1354aa">

## Variations of Transformers- Module-level의 개선 – Attention: linearized attention
Linearized attention: Kernel method 등을 이용하여 bottleneck인 matrix연산 및 softmax를 linear연산으로 줄인다!
- Performer, RFA(random feature attention) 등이 여기 속한다.

  <img width="500" alt="스크린샷 2023-08-03 오후 7 27 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/28596132-0822-445c-ac87-c3e11d8c9044">
  <img width="500" alt="스크린샷 2023-08-03 오후 7 28 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9087ad16-2224-4ad2-96c6-39379fed8d47">
  <img width="500" alt="스크린샷 2023-08-03 오후 7 28 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/25cd8a6f-6ec1-4d8e-8264-f6b123baad26">

## Variations of Transformers - Module-level의 개선 – Attention: query prototype & memory compression
Query prototype & memory compression  
- query(디코더의 집중하고자 하는 부분; query prototype)나 key-value
- (인코더 부분; memory compression) pair의 크기를 줄여
- computational complexity을 줄이는 방법.  

  <img width="400" alt="스크린샷 2023-08-03 오후 7 31 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e10e7196-9832-4d53-8184-9d9263bc5efa">
  <img width="600" alt="스크린샷 2023-08-03 오후 7 31 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60006792-ce49-43a7-9c78-f88b499f009e">
  
Attention with query prototype  
- Clustered attention, Informer 등이 속한다.
    
Attention with compressed key-value memory
- Memory compressed attention(MCA),  Set Transformer 등이 속한다.

  <img width="509" alt="스크린샷 2023-08-03 오후 7 33 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9f814b0c-9d8f-4f14-a1df-9d2b91d5c745">

## Variations of Transformers - Module-level의 개선 – Attention: low-rank self-attention

Low - rank self -attention :  
self-attention 연산은 때떄로 low-rank(Attention-matrix A의 rank가 input length T보다 작다)일수있다고 실험적/이론적으로 알려져있다.  

<img width="400" alt="스크린샷 2023-08-03 오후 7 42 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c879c4e4-1109-4b2f-a4a5-c329edf2f617">
<img width="400" alt="스크린샷 2023-08-03 오후 7 42 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60b15fa9-1830-4b0b-8464-38299f3d36f2">

이러한 성질을 이용하는 2가지 방법이 있다.  
1. Parametrization로 low-rank property로 모델링 하는 방법
   -> key의 dimension을 제한하는 방식으로 low-rank property을 explicit하게 모델링하여, 파라미터수를 줄이고 과적합(overfitting)을 방지한다.
2. self-attention matrix을 low-rank approximation로 대체하는 방법
   -> low-rank approximation으로 self-attention의 복잡도를 줄인다.
  예) compressed self-attention for deep metric learning with low-rank approximation(CSALR), Nystromformer

## Variations of Transformers - Module-level의 개선 – Attention: with prior knowledge
Attention은 기본적으로 모든 영역에 동일한 가중치를 두고 학습을 시작한다.  
특정 도메인의 데이터 등으로 부터 얻은 가중치에 대한 기본 knowledge 를 attention 의 선험지식(prior)로 활용한다.  

<img width="500" alt="스크린샷 2023-08-03 오후 7 44 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5be01e70-c00b-4cc2-b809-601f075eb765">

크게, 아래의 카테고리 등으로 나눌 수 있다: 
1. 데이터의 특성에 대한 locality을 반영하는 prior을 이용하는 모델
- i.e. Gaussian transformer
2. CNN과 같은 lower module로부터 prior을 얻는 모델
- i.e. predictive attention transformer, realformer
3. Multi-task adapter을 이용하는 모델
- 입력 간의 pair-wise 상호 작용과 관련 없는 attention을 사용하여 탐색하는 방법
이 경우, 보통 모델은 사전 주의 분포만 이용한다

## Variations of Transformers - Module-level의 개선 – Attention: multi-head mechanism improvement
Multi-head mechanism improvement  
- Transformer의 Multi-head 구조를 보완하여 개선한다.
- Head behavior modeling:
  -> Vanilla transformer 에서는 각 Head가 상호작용하지 않고 서로 독립적이다.
    $head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)$  
    $MultiHead(Q,K,V) = Concat(head_i, ..., head_h)W^O$
  -> Head가 서로 상호작용할 수 있는 여지를 주어 성능을 개선한다 (i.e. collaborative multi-head attention)
    (모든 Head가 동일한 W을 공유하며, mixing vector $m_i$ 로 각 head를 분리한다.
    $head_i = Attention(Q W^Q_i diag(m_i), K W^K_i, V W^V_i)$  
  - 그밖에 Talking-heads attention, auxiliary disagreement regularization term등의 연구가 있다.
 
Multi-head with Refined Aggregation :  
-> $MultiHead(Q,K,V) = Concat(head_i, ..., head_h)W^O$  
  Multi-head attention을 head들로 aggregation할때, projection등을 하여 정제한다.  
   $MultiHead(Q,K,V)$ = <img width="350" alt="스크린샷 2023-08-04 오전 11 35 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6ad36fe2-6927-4985-a44d-19461ed06b3a">

## Variations of Transformers - Module-level의 개선 – Attention: multi-head mechanism improvement
Multi-head mechanism improvement  
- Multi-head with Restricted Spans
  - Head 별로 attention 을 수행하는 범위(span)을 제한 -> Head 별로 다른 context을 관측할 수 있음! 
- 2가지 장점.
  1. Locality: 정확히 어디를 볼지를 지정 🡪 우리가 어떤 locality가 중요한지 데이터를 통해 알고 있다면 이익을 볼 수 있다!
  2. Efficiency: span을 적절하게 제한하여, 추가적인 메모리/컴퓨팅 자원 소모 없이 long sequence을 처리하도록 scaling 할 수 있다!
     
  <img width="500" alt="스크린샷 2023-08-04 오전 11 38 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6a60dbb4-924f-4473-8619-747e1f546c5f">

- 위의 그림처럼 (b) 와 같이 학습 가능한 파라미터 R 등을 세팅하여 처리할 수도 있다.
- 또한, multi-scale vision longformer의 경우, layer 와 head 에 따라 window 의 크기만 달라지는
 “fixed attention span”을 제안하였다.

## Variations of Transformers - Module-level의 개선 – Positional representation
Transformer에서는 position정보를 넣어주는데 그 넣어주는 방법에 따라 performance가 달라질 수도 있다.
1. Absolute position
- 원 논문은 삼각함수를 사용해 positional encoding을 구현하였다.

  <img width="468" alt="스크린샷 2023-08-04 오전 11 45 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/df5a2b6a-f489-4311-b1ff-0a4505250842">
  - pos: token의 위치 index, i: 벡터의 차원 index
- 그렇지만, 앞서 이야기했듯이, 다른 함수나 규칙을 사용할 수도 있다!

2. Relative position
- 실제 위치보다, 각 전후 input간의 관계가 더 중요한 경우, relative position을 embedding하는 것이 더 좋을 수 있다. (i.e. Music Transformer)
- 이 relative position representation 을 고정하지 않고, 학습 가능한 embedding 정보로 적용할 수도 있다.
- 관련된 transformer들
	i.e. Transformer-XL, DeBERTa, Music Transformer…

3. Without explicit encoding
- 명확한 인코딩을 하지않고, 간접적으로 다른 조건 로직 / neural net 등을 통해 간접적으로 알려준다.
- i.e. R-transformer, Conditional positional encoding

## Variations of Transformers - Architecture-level에서의 개선: recurrent / hierarchical
<img width="600" alt="스크린샷 2023-08-04 오전 11 47 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/19c6c0d7-9edb-458b-9f81-1def1b8dac39">

시퀀스 길이에 대한 self-attention의 2차적 복잡성은 일부 down streaming task의 성능을 크게 제한할 수 있다.  
예를 들어, 언어 모델링에는 일반적으로 장거리 컨텍스트가 필요하다!  
- Recurrent, hierarchical 한 디자인이 성능향상에 도움을 줄 수 있다.
- [Examples]
  - Recurrent: Transformer-XL, Memformer
  - Hierarchical: Hi-Transformer, HIBERT, TENER

## Variations of Transformers - Applications의 확장 (vision / video)
<img width="500" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1a103f34-db88-47d2-95a6-66d69768915a">  
<img width="500" alt="스크린샷 2023-08-04 오전 11 51 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/db580b35-026a-45a0-a227-9c24d06d8a04">  
<img width="500" alt="스크린샷 2023-08-04 오전 11 51 25" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/70bbff01-1310-41cc-afea-03138c68de95">  
<img width="500" alt="스크린샷 2023-08-04 오전 11 51 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8d50b750-b4d8-42a1-b966-801cc16cdc51">  
- Tay, Yi, et al. "Efficient transformers: A survey." arXiv preprint arXiv:2009.06732 (2020).
- Tay, Yi, et al. "Long range arena: A benchmark for efficient transformers." arXiv preprint arXiv:2011.04006 (2020).
- Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).
- Girdhar, Rohit, et al. "Video action transformer network." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.

## Variations of Transformers - Applications의 확장 (speech synthesis(TTS) / speech recognition(STT))
<img height="400" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/da306968-3357-48da-b53b-c5a04a1bb68f">
<img height="400" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/515162cd-266d-4bd3-b1aa-8231dcd7668f">
<img height="300" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b7a239fe-0fe5-4383-91fe-aee03f4dbf01">   

- Dong, Linhao, Shuang Xu, and Bo Xu. "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition." 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.
- Li, Naihan, et al. "Neural speech synthesis with transformer network." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.

## Variations of Transformers - Applications의 확장 (graph network / reinforcement learning)
<img width="500" alt="스크린샷 2023-08-04 오후 12 04 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3f4be30f-09d9-47dd-af6d-6f03c74f58c9">
<img width="500" alt="스크린샷 2023-08-04 오후 12 04 57" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ebeef5f9-0bf8-4795-a882-b04d52d95a8a">

- Yun, Seongjun, et al. "Graph transformer networks." Advances in Neural Information Processing Systems 32 (2019): 11983-11993.
- Chen, Lili, et al. "Decision transformer: Reinforcement learning via sequence modeling." arXiv preprint arXiv:2106.01345 (2021).


