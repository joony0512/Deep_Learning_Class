# 트랜스포머(Transformer)- 2. Variations of Transformers

## Transformer의 다양한 후속 연구들
<img width="435" alt="스크린샷 2023-08-03 오후 6 55 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/02e7f10f-ba48-4117-a221-95fce05ecb5c">

주로 아래 방향으로 발전!  
1. Complexity의 개선 (하단의 efficient transformer 구분표)
2. 성능개선  
3. 도메인 확장  
  
개선 방법에 따라 아래와 같이 범주화 할 수 있다!  
- Module level
- Architecture level
- Pre-Train
- Application

  <img width="322" alt="스크린샷 2023-08-03 오후 6 58 11" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/03caa91b-e675-4888-a95a-8dd6ac3cfa2f">

  <img width="515" alt="스크린샷 2023-08-03 오후 6 56 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8b1bb5bf-bfc9-4c0c-81e2-4635a8392ac0">

## Transformer의 다양한 후속 연구들 - Complexity / 성능의 개선
<img width="363" alt="스크린샷 2023-08-03 오후 6 58 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/32e9285e-2548-48d4-9859-e7655a9fd9d8">

<img width="396" alt="스크린샷 2023-08-03 오후 6 59 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7eb7cf00-c4e7-424c-b719-4ba08b347393">

## 후속 연구가 되어 많은 종류의 Transformer가 있다!- Recap: 트랜스포머 (Transformer) & Computational Costs
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/166d421e-1920-444b-a2f5-59a82c521286)  

<img width="809" alt="스크린샷 2023-08-03 오후 7 01 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b3cd850d-9c1c-4f68-a797-0ea45fda859a">

<img width="545" alt="스크린샷 2023-08-03 오후 7 02 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8a86d111-4ef3-41a4-b4a4-98f2fcac630c">

- Transformer는 RNN등과 비교했을때, 효율적으로 병렬계산이 가능하다.
- Self-attention layer에서 시퀀스길이를 n, input-size을 d라고 했을때, 파라미터수는 $4d^2$이며 연산복잡도(computational complexity)는 $O(n^2 * d)$ 이다.
- Self-attention 이후의 position-wise FFD의 경우, 파라미터수는 $8d^2$이며 연산복잡도는 $O(n * d^2)$이다.

## Variations of Transformers - Module-level의 개선 - Attention
Self-attention은 Transformer의 핵심구성요소이지만, 후속연구에서 크게 2가지 문제가 제기되었다.  
1. Complexity(복잡성)
   - $O(n^2 * d)$의 속도는 나쁘지 않지만 전체 transformer의 연산중 bottle-neck이다. 만약 빠르게 할 수 있다면, 전체적인 속도가 빨라질 것이다.
     -> Efficient transformer
2. Structural prior(구조적인 prior)
   - Self-attention은 CNN,RNN과 다르게 input에 대한 가정이 없다.
   - CNN은 주변의 인근 feature들이 관련이 있을것이라는 정보, RNN은 Time-series이므로 관련된 정보를 time에 over해서 얻을것이라는 정보가 있는 반면, Self-attention은 없다.
   - 그러므로 pre-training등이 없다면, 작거나 적당한 사이즈의 데이터에서 overfit하기 쉽다.

## Variations of Transformers - Module-level의 개선 - Attention
Complexity, Structural prior 문제를 해결하기 위한 다양한 방법들이 제시되었다.
1. Sparse attention: sparsity bias을 attention mechanism 에 더해 연산을 줄인다.
2. Linearized attention: attention matrix 연산을 kernel feature map로 분리(disentangle)한다. 그 이후, 선형 복잡성을 이루기 위해 역순으로 attention을 계산한다.
3. Query prototype & memory compression: attention의 key, query, value memory pair의 수를 줄여 attention matrix 연산을 줄인다.
4. Low-rank self-attention: low-rank (낮은 계수)로 self-attention 을 정의하여 연산을 줄인다.
5. Attention with prior: CNN/RNN 모델을 추가하여 prior knowledge을 보완하거나, 기능적으로 prior을 추가할 수 있도록 개선한다.
6. Improved multi-head mechanism: multi-head attention 로직 자체를 개선한다.

## Variations of Transformers - Module-level의 개선 – Attention: sparse attention
- Attention은 기본적으로 모든 query에 attend를 시도한다.
- 실제로 attention이 필요한 영역은 일부에 불과하다 -> 제한된 영역에서만 sparse하게 집중하게 하면 어떨까?
  
  <img width="912" alt="스크린샷 2023-08-03 오후 7 21 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d980510-2307-4039-8d92-c821938eb2e2">

- Position 기반 sparse attention : 미리 정의된 패턴으로 sparsity을 정의한다.
  - 왼쪽에 있는 predefined된 모형을 조합하여 만들 수있다.
  - star-transformer, long-transformer, big-bird등이 속한다.
    
<img width="400" alt="스크린샷 2023-08-03 오후 7 23 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b5d3a3e5-0b07-4d41-8cf8-aa22bedab84b">
<img width="400" alt="스크린샷 2023-08-03 오후 7 23 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/95d4e583-646c-4594-ab48-95a32d6c3d8f">

- Content 기반 sparse attention:  input content에 따라 sparse graph를 생성한다. 즉, input에 따라 sparse attention의 형태가 달라진다.
  - Routing transformer (K-means clustering 기반), Reformer(locality-sensitive hashing; LSH 기반)가 이에 속한다.
  - O(nkd + n2d/k) : n routing vectors to all k centroids in a space of size d

  <img width="539" alt="스크린샷 2023-08-03 오후 7 25 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5402a902-af57-42ff-8e31-4905c59edbc8">
  <img width="665" alt="스크린샷 2023-08-03 오후 7 25 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea7128a5-22ee-4987-9b50-3cc9c8adf7c0">
  <img width="477" alt="스크린샷 2023-08-03 오후 7 26 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0798d9d8-aed3-4ce2-b81a-75c25b1354aa">

## Variations of Transformers- Module-level의 개선 – Attention: linearized attention
Linearized attention: Kernel method 등을 이용하여 bottleneck인 matrix연산 및 softmax를 linear연산으로 줄인다!
- Performer, RFA(random feature attention) 등이 여기 속한다.

  <img width="906" alt="스크린샷 2023-08-03 오후 7 27 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/28596132-0822-445c-ac87-c3e11d8c9044">
  <img width="500" alt="스크린샷 2023-08-03 오후 7 28 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9087ad16-2224-4ad2-96c6-39379fed8d47">
  <img width="500" alt="스크린샷 2023-08-03 오후 7 28 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/25cd8a6f-6ec1-4d8e-8264-f6b123baad26">

## Variations of Transformers - Module-level의 개선 – Attention: query prototype & memory compression
Query prototype & memory compression  
- query(디코더의 집중하고자 하는 부분; query prototype)나 key-value
- (인코더 부분; memory compression) pair의 크기를 줄여
- computational complexity을 줄이는 방법.  

  <img width="400" alt="스크린샷 2023-08-03 오후 7 31 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e10e7196-9832-4d53-8184-9d9263bc5efa">
  <img width="600" alt="스크린샷 2023-08-03 오후 7 31 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60006792-ce49-43a7-9c78-f88b499f009e">
  
Attention with query prototype  
- Clustered attention, Informer 등이 속한다.
    
Attention with compressed key-value memory
- Memory compressed attention(MCA),  Set Transformer 등이 속한다.

  <img width="509" alt="스크린샷 2023-08-03 오후 7 33 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9f814b0c-9d8f-4f14-a1df-9d2b91d5c745">

## Variations of Transformers - Module-level의 개선 – Attention: low-rank self-attention

Low - rank self -attention :  
self-attention 연산은 때떄로 low-rank(Attention-matrix A의 rank가 input length T보다 작다)일수있다고 실험적/이론적으로 알려져있다.  

<img width="400" alt="스크린샷 2023-08-03 오후 7 42 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c879c4e4-1109-4b2f-a4a5-c329edf2f617">
<img width="400" alt="스크린샷 2023-08-03 오후 7 42 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60b15fa9-1830-4b0b-8464-38299f3d36f2">

이러한 성질을 이용하는 2가지 방법이 있다.  
1. Parametrization로 low-rank property로 모델링 하는 방법
   -> key의 dimension을 제한하는 방식으로 low-rank property을 explicit하게 모델링하여, 파라미터수를 줄이고 과적합(overfitting)을 방지한다.
2. self-attention matrix을 low-rank approximation로 대체하는 방법
   -> low-rank approximation으로 self-attention의 복잡도를 줄인다.
  예) compressed self-attention for deep metric learning with low-rank approximation(CSALR), Nystromformer

## Variations of Transformers - Module-level의 개선 – Attention: with prior knowledge
Attention은 기본적으로 모든 영역에 동일한 가중치를 두고 학습을 시작한다.  
특정 도메인의 데이터 등으로 부터 얻은 가중치에 대한 기본 knowledge 를 attention 의 선험지식(prior)로 활용한다.  

<img width="683" alt="스크린샷 2023-08-03 오후 7 44 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5be01e70-c00b-4cc2-b809-601f075eb765">

크게, 아래의 카테고리 등으로 나눌 수 있다: 
1. 데이터의 특성에 대한 locality을 반영하는 prior을 이용하는 모델
- i.e. Gaussian transformer
2. CNN과 같은 lower module로부터 prior을 얻는 모델
- i.e. predictive attention transformer, realformer
3. Multi-task adapter을 이용하는 모델
- 입력 간의 pair-wise 상호 작용과 관련 없는 attention을 사용하여 탐색하는 방법
이 경우, 보통 모델은 사전 주의 분포만 이용한다

## Variations of Transformers
Module-level의 개선 – Attention: multi-head mechanism improvement

