# 합성곱 신경망(CNN) - Recent trends of CNN models
## CNN의 발전 방향 - Self-training(자가 학습 기법)
- 자가 학습(self-training)

  - 준지도 학습 (Semi-supervised learning)
    - 레이블된 데이터와 레이블 되지 않는 데이터 모두 사용되는 학습.
    - 일반적으로, 다수의 레이블되지 않는 데이터와 레이블 된 골드 레이블 (gold label) 데이터로 구성.

  - 자기 지도 학습(Self-supervised learning)
    - 다량의 레이블이 없는 원데이터로부터 데이터 부분들의 관계를 통해 레이블을 자동으로 생성하여 지도 학습에 이용
    - Pretext task를 설정(i.e. 데이터의 일부분을 은닉하여 모델이 그 부분을 예측하도록 학습)하여 이를 학습한 후, downstream task로 전이 학습 (transfer learning) 하여 다른 테스크에 적용.
    - Pretext task:  데이터 내의 semantic한 정보를 이해할 수 있도록 새로 정의한 임의의 문제
   
- 자가 학습(self-training) : 셀프 트레이닝의 원리는 높은 확률 값이 나오는 데이터 위주로 가져가, 먼저 학습을 진행한다 ! 

<img width="464" alt="스크린샷 2023-07-01 오후 6 32 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/dcc44872-7350-4294-9253-0b9343b5b93f">

## CNN의 발전 방향 - Self-training(자가 학습 기법)과 Noisy Student (2020) 
- 순서

1. EfficientNet 모델을 teacher 모델로, ImageNet데이터(약 1천만 이미지)로 학습시킨다.

2. 학습 된 teacher 모델로 라벨이 매겨져 있지 않은 3 억장의 이미지에 라벨을 매긴다.

3. teacher 모델보다 크기가 같거나 큰 student network를 생성한 뒤, ImageNet 데이터와 라벨을 매긴 데이터를 합쳐서 학습시킨다.
  - 이때, data augmentation, dropout, stochastic depth 등을 적용한다!

4. 학습된 student 모델을 teacher 모델로 바꾸고, 2-4 과정을 반복한다.

- 원 논문의 설명
<img width="334" alt="스크린샷 2023-07-01 오후 6 34 14" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/47e51b47-a391-4ffa-9d8f-bfe8caea745a">

<img width="265" alt="스크린샷 2023-07-01 오후 6 34 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12e84767-3e2f-4431-a16a-743047f2b494">

- 결과
  
<img width="434" alt="스크린샷 2023-07-01 오후 6 36 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f99faca8-054d-4c89-9da5-5eaeb8f519a8">

<img width="181" alt="스크린샷 2023-07-01 오후 6 36 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/52165525-8bf0-4498-afc1-6704e3b6a6c2">

- 노이즈와 adversarial attack(적대적 공격)에 튼튼하다.

<img width="261" alt="스크린샷 2023-07-01 오후 6 38 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5bb0b372-bfa4-4ac6-a23c-8f826eb9caa1">

- 참고: ImageNet-A,C,P
  - 모델의 robustness을 테스트하기 위해 ImageNet의 corruption을 담은 데이터
  - burring, fogging, rotation, scaling 등이 들어있음
    
<img width="195" alt="스크린샷 2023-07-01 오후 6 39 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6fae9440-224c-4fb4-853b-7c6395b12e7e">

## CNN의 발전 방향 - Pseudo Labeling
<img width="378" alt="스크린샷 2023-07-01 오후 6 40 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/67226e2b-936c-4eb8-b1d3-e6291c0b9bff">

1. Labeled Data로 Model을 먼저 학습시킨다.
2. 그렇게 학습된 모델을 사용하여, Unlabeled data를 예측하고 그 결과를 Label로 사용하는 pseudo-labeled data를 만든다.
3. Pseudo-labeled data와 Labeled data를 모두 사용하여 다시 그 모델을 학습시킨다.

- Pseudo-labeled data는?
  - 각 샘플에 대하여, 예측된 확률이 가장 높은것.
  - <img width="195" alt="스크린샷 2023-07-01 오후 6 42 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d35486c1-d6bb-4f3d-874c-df965dc79e4c">
- 단점
  - 예측된 확률이 가장 높은 것을 label로 선정한다고 했을때, 제대로 학습을 마치지 못한 모델로 하였을 경우, 모델의 학습을 저해하는 데이터를 만들뿐
- 이 두개의 작업을 따로 하는 것이 아닌 동시에 적용하여야 한다.

- Pseudo-labeling의 손실함수
  - 다음과 같이 원래학습과 Pseudo-labeling된 데이터의 학습을 동시에 진행한다.<img width="339" alt="스크린샷 2023-07-01 오후 6 46 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d4cc183a-8a20-448a-93f2-436f95a0e3cb">
    - n : labeled data의 배치 사이즈
    - $f_i^m$ : m번째 labeled data 샘플을 input으로 넣은 모델의 output
    - $y_i^m$ : 위의 input에 매칭되는 label
    - n' : unlabeled data의 배치 사이즈
    - $f_i^{'m}$ : m번째 unlabeled data 샘플을 input으로 넣은 모델의 output
    - $y_i^{'m}$ : 위의 unlabeled input에 매칭되는 현재의 Pseudo-label
    - $\alpha (t)$ : 두식을 balancing하는 coefficient -> 매우중요
- Pseudo labeling 이 작동하는 이유?
  - 모델의 분류과정에서 decision boundary를 결정할 때, 그 경계를 구분하는 지점의 데이터가 몰려있는 밀도가 낮으면 낮을수록, 더 미세한 차이점도 구별한다고 생각할 수 있기 때문에, 전체적인 성능을 높일 수 있다! 
  - :다른 semi-supervised learning방법에서도 통용되는 장점
  - Entropy 정형화(regularization) 효과

## CNN의 발전 방향 - Meta Pseudo Labels (2021)
<img width="553" alt="스크린샷 2023-07-01 오후 7 05 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c8105655-441c-4d59-b52f-ca06204f3d5d">

- Meta Pseudo Labels는 teacher와 student를 동시에 학습!
  - 이때, student의 labeled data에 대한 performance가 teacher에 학습에 영향을 준다! (일종의 reward signal)
 
  <img width="553" alt="스크린샷 2023-07-01 오후 7 07 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/369ca03a-0ae9-45df-8408-b0b205e95691">

  1. Teacher의 x결과로 Pseudo-label($q_\psi$(x))을 만들고 student에게 주고 student의 weight를 supervised learning(cross entropy)로 업데이트 한다.
  2. student의 validation loss 를 재사용하여, teacher의 weight역시 업데이트한다.<img width="400" alt="스크린샷 2023-07-01 오후 7 13 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/43ef118a-f074-478a-879c-50758ef43f8d">
  3. 부가적으로 UDA(unsupervied data augmentation for consistency training)에서 제안된, UDA objective을 조금 더해 보완하여 완성

- Noisy student보다 나은 성능
  
<img width="261" alt="스크린샷 2023-07-01 오후 7 15 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b744c28c-0884-453a-ae0a-fecfe4d3e956">

<img width="288" alt="스크린샷 2023-07-01 오후 7 16 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2ca54bfa-b577-42eb-ad75-37f8c4df11e3">
