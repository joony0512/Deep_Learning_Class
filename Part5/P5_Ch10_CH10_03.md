 # <Generative model series #2> Flow Models - 3. Variations of Flow Models
## Affine flows
다변수 가우시안(multivariate gaussian) flow라고도 하고, affine변환을 하는 flow이다. 
- 아핀변환 : 선형변환 + 평행이동변환의 합성
- 파라미터 : 역변환 가능한 matrix A와 벡터 b로 이루어진다. 즉 아래를 만족한다.
  $f(x) = A^{-1}(x-b)$

샘플링은 x~N(0,1)일때, 아래와 같이 generation한다.  x = Az + b   
이때 행렬 형태로 인해 차원이 커질수록 log-likelihood의 컴퓨팅 코스트가 급격하게 증가한다. 
- f의 Jacobian은 $A^{-1}$ 이고 log-likelihood에 포함되는 det(A)계산이 필요하다.
- 행렬의 역함수 연산은 비싸다 (O(N^3)) -> 결과적으로 매우 비싼 연산이 필요하다.

## Elementwise flows
flow연산을 각 element별로 따로 적용하여 연산량을 획기적으로 줄일 수 있다. <img width="234" alt="스크린샷 2023-08-09 오후 5 44 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2da5a818-e6d9-458c-b5bb-9da2d32cd2dc">  
- elementwise affine함수 , cdf flow등을 사용할 수 있다.

Jacobian이 diagonal(대각선값만가짐)하므로 determinant를 계산하기 쉬워진다.  <img width="104" alt="스크린샷 2023-08-09 오후 5 47 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/10dec254-413e-452c-a8a5-94219ff71f3e">

<img width="234" alt="스크린샷 2023-08-09 오후 5 46 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fb2ddba2-68ce-4569-807a-82c57cb3b671">  
이고 이는 아래와 같이 정리된다.  
<img width="148" alt="스크린샷 2023-08-09 오후 5 46 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d2fc7e3-ddea-447f-91d7-fbbbdbb07ba8">

## Normalizing flows 기반 모델 - NICE/RealNVP
<img width="402" alt="스크린샷 2023-08-09 오후 5 48 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea6b83ad-aa8a-446d-9542-dcbbc7d6506a">

## Normalizing flows 기반 모델 - NICE (Non-linear independent component estimation) / RealNVP(Real-valued Non-Volume Preserving)
아핀 커플링 레이어(affine coupling layer)을 정의  

<img width="426" alt="스크린샷 2023-08-09 오후 5 58 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/be7d1c03-059c-42ef-aea0-941f019fc2d6">

- variable을 2파트로 나눈다. 전체가 D일때, d<D인 지점으로 $x_{1:d}, x_{d+1 : D}$ 로 분리한다.
- RealNVP의 경우 s,t을 각각 scale과 translation을 하는 함수($R^d -> R^{D-d}$ 이고 $\odot$ 을 element-wise곱이라고 했을때,
  <img width="562" alt="스크린샷 2023-08-09 오후 5 55 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ff22c0a0-dcaf-4624-b431-52147ac1ce2f"> 이다.
  - x, z는 $s_\theta , t_\theta$ 의 역함수를 구하지 않고도 invertible하다.
    - $s_\theta , t_\theta$ 는 제한없는 임의의 함수 (neural network)로 나타낼 수 있다.
    - data-parameterized된 element-wise flow로 생각할 수도 있다.
    - 이때 위 변환의 Jacobian은 아래와 같다 .  
      <img width="447" alt="스크린샷 2023-08-09 오후 5 57 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b63cfd53-9910-4884-8506-b709e76b5adc">
## Normalizing flows 기반 모델 - NICE (Non-linear independent component estimation)는?
NICE에서는 coupling layer를 사용하고 내부의 g function으로 additive function을 사용했다. 
덕분에 Inversion이 가능하고 Jacobian의 Determinant를 구하기 쉬워 Normalizing Flow를 구현할 수 있다. 
하지만 내부 함수 g를 단순하게 +를 사용했기에 복잡한 데이터를 표현하기 어렵다는 한계가 있었다. -> scale term이 없다.  
<img width="368" alt="스크린샷 2023-08-09 오후 6 26 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9dbb2eca-11e1-4fd0-9f37-af38f1d75f23">


## Normalizing flows 기반 모델 - NICE / RealNVP의 partitioning (affine coupling layers을 위한 masking schema)
<img width="536" alt="스크린샷 2023-08-09 오후 5 59 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8a533fc0-c5d3-4e72-8734-3670b7694e32">

## Normalizing flows 기반 모델 - RealNVP(Real-valued Non-Volume Preserving)
Triangular Matrix의 Determinant는 대각 성분의 곱으로 표현된다. 따라서 Affine Transform으로 표현되는 y의 Jacobian을 구해보면 다음과 같다.  
<img width="256" alt="스크린샷 2023-08-09 오후 6 05 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b74bdc2f-a451-4df3-b6a4-9ec631d47021">

따라서 det(J)은 다음과 같다.  
<img width="336" alt="스크린샷 2023-08-09 오후 6 19 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5192378c-7af9-4875-9bd8-f2b2c1146de3">  
심지어, Jacobian의 Determinant를 구할 때 함수 s, t의 Jacobian을 구하지 않아도 된다. 따라서 s,t는 복잡한 함수를 사용할 수 있다.
- 그 말은 s, t는 DNN으로 표현할 수 있다는 의미.  
  
<img width="599" alt="스크린샷 2023-08-09 오후 6 20 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c78930ae-1977-402f-9580-868e8a004f0a">
- s, t의 역을 계산하는 것을 필요로 하지않는다.

## Normalizing flows 기반 모델 - RealNVP(Real-valued Non-Volume Preserving)의 결과
<img width="600" alt="스크린샷 2023-08-09 오후 6 23 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1f908959-6474-4dcb-b563-5912b071ae5d">
<img width="600" alt="스크린샷 2023-08-09 오후 6 23 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/396e75cb-9a64-44e1-8f70-1257bea92a56">

## Normalizing flows 기반 모델 - Directed graphical models & autoregressive flows
- Bayes nets에 inspired되어 flow를 sequential하게만 구성하는게 아닌, directed acyclic graph(DAG)형태로 구성할 수 도 있을것이다.   
- Bayes nets의 sampling process가 flow이고, 이 graph구성이 auto-regressive하다면, 'auto-regressive flow'라고 한다.  
  <img width="368" alt="스크린샷 2023-08-09 오후 6 31 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3b64abc1-1b2a-4250-8035-caaeb465018f">
- z에 invertible한 mapping($f_\theta^{-1}$)을 반복하여 x를 sampling한다.
  - f를 사용하는 autoregressive log-likelihood 계산보다 시간이 더 많이 소요된다.
 
## Normalizing flows 기반 모델 - Autoregressive flows
- x->z을 매핑하는 과정을 전부 병렬화(fully parallelizable)하여 fitting할 수 있다.<img width="236" alt="스크린샷 2023-08-09 오후 6 39 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bc0a678c-dea2-4735-ba67-2ed3155482e5">
- Autoregressive flow에서
  - x->z는 autoregressive model의 log-likelihood의 계산과 같다.
  - z->x는 autoregressive model의 'sampling'방법과 같다.  
    <img width="280" alt="스크린샷 2023-08-09 오후 6 40 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/226bb8bb-4c74-4b8e-8cae-cb90f70d69f9">

## Normalizing flows 기반 모델 - Masked Autoregressive Flows (MAF)
- 2개의 랜덤 variable z~$\pi(z) and x \sim p(x)$ 이 있을때, pdf $\pi(z)$이 알려져 있으면, 
  MAF는 p(x)을 학습하는 것을 원칙으로 한다.
- MAF는 AF처럼 과거생성데이터($x_{1: i-1}$)을 condition된 input로 넣어 각각의 $x_i$ 을 생성한다.
- Data generation(새 x을 생성 ; inference)
  - <img width="737" alt="스크린샷 2023-08-09 오후 6 58 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6f1d5035-6560-4f12-923e-8e5649333cfc">
- Density estimation (x가 알려져 있을때 ; train)
  - <img width="289" alt="스크린샷 2023-08-09 오후 6 59 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/def9f1fd-9aee-4b7f-9a2e-92738aa5aa1e">
- Generation process는 sequential하기에 매우 느리다.
- 반면 density estimation은 Masked Autoregressive Model(i.e.  MADE, Pixel CNN, Transformer Decoder)을 사용하면 한번만 inference하면 되기에 빠르다.
  - Transformation함수(scale, shift)는 엳변환되기 쉽고, def(Jacobian)도 쉽게 컴퓨팅 될 수 있다. 

## Normalizing flows 기반 모델 - Inverse Autoregressive Flows (IAF)
- Autoregressive의 역(inverse) 역시, flow다. 이를 inverse autoregressive flow(IAF)라고 한다.
- x->z는 autoregressive model의 'sampling' 방법과 같다. 
- z->x는 autoregressive model의 'log-likelihood'의 계산과 같다.
  - 즉 생성과정에서 IAF sampling은 빠르다!

    <img width="289" alt="스크린샷 2023-08-10 오후 3 20 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/13ab30ea-e6a7-48ed-895c-67bb03a0dad0">  
- Parallel WaveNet, IAF-VAE등이 IAF의 빠른 sampling을 활용한다.

## Normalizing flows 기반 모델 - Inverse Autoregressive Flows (IAF)모델의 예: Parallel WaveNet
- 기존의 WaveNet은 음성 데이터의 학습은 빠른 반면 sampling(generation)은 순차적으로 이루어져야 하기 때문에 매우 속도가 느렸다.  
- Auto-regressive flow와 Inverse Autoregressive flow는 모두 Normalizing Flow을 통하여 Autoregressive구조의 likelihood을 예측한다는 점에서는 같지만 정반대의 특성을 가지고 있다!
- WaveNet이 가진 구조인 AF는 데이터의 학습은 빠르게 이루어지는 반면 Sampling은 느리다. 반면 IAF는 데이터의 학습은 느린 반면 Sampling은 빠르다.
- Parallel WaveNet: 둘다 이용!
  - Teacher로 AF구조인 WaveNet을 빠르게학습하고,
  - IAF구조를 가진 student network를 distillation 시킨다!
    - Student는 noise 로부터 샘플링 하는 방법 학습하고 IAF특성으로 빠르게 generation한다.

<img width="350" alt="스크린샷 2023-08-10 오후 3 54 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f1e03450-b776-4906-a50a-795d7880ed9d">

## Normalizing flows 기반 모델
Inverse Autoregressive Flows (IAF)모델의 예: IAF-VAE  

<img width="499" alt="스크린샷 2023-08-10 오후 4 01 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c719eead-bed2-401c-8f61-46703e191217">  
<img width="345" alt="스크린샷 2023-08-10 오후 4 01 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/99866598-4871-4083-a7cd-db780db2ccca">

## Normalizing flows 기반 모델 - AF vs IAF
Autoregressive flow
- 알려진 임의의 x에 대하여 p(x)의 evaluation이 빠르다! : 학습 속도가 빠르다!
- 다만, 생성 과정(z -> x)이 sampling로 속도가 느리다. (real-time 서비스에 부적합하다)

Inverse autoregressive flow
- 알려진 임의의 x에 대하여 p(x)의 evaluation이 느리다! 즉, Maximum likelihood에 의해 직접 학습하는 것이 느리다.
- 빠른 샘플링 속도! (real-time 서비스에 적합하다)
- 만약 x가 샘플이라면, p(x)의 evaluation은 빠르다!

## Normalizing flows 기반 모델 - Masked AF vs IAF
<img width="505" alt="스크린샷 2023-08-10 오후 4 16 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/78224633-c453-4bf2-8f6f-56ccc1bf1729">  
<img width="397" alt="스크린샷 2023-08-10 오후 4 16 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a704a46b-45ca-4f3e-8451-b4bb1ac53b97">

## Normalizing flows 기반 모델 - Glow
Nice 및 RealNVP을 확장 -> reverse permutation operation연산을 Invertible한 fixed 1x1 변환으로 대체 (Nice 및 RealNVP는 reverse permutation하는 flow을 사용했다)
1. Activation Normalization (actnorm)
   - (batch norm 대체) 이는 activation함수의 normalization을 뜻한다. 이 step에서는 각 채널에 scale과 bias parameter를 이용해서 affine transform을 한다.
   - 이는 batch normalization과 유사하지만 mini-batch size1에서 동작한다. scale,bias parameter는 학습하는 파라미터이지만 act norm 이후에 첫번때 미니배치는 평균0, 표준편차1이되게 initialize된다.
2. Invertible 1X1 convolution
   -  1X1 convolution은 input channel과 output channel수가 같으면 permutation연산의 generalization이다.
   -  이 permutation을 learnable하게 만들어 버린 것.
   -  이 말은 결국 1X1 convolution이 output channel을 input channel과 같게만 해주면, x1:d를 어떻게 잡을 것이냐를 shuffle해주면서 channel을 해치지 않게 된다는 것이다.
   -  이 연산을 하는 함수를 $f = conv2d(h;W)$라고 하고 normalizing flow를 사용하기 위해 log determinant가 필요하다.  
      <img width="309" alt="스크린샷 2023-08-10 오후 4 56 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a69782e5-11b4-4efd-af50-4c6a9adbe289">

3. Affine coupling layer
   - RealNVP와 동일
     1)처음 d까지의 차원은 그대로 가져간다.
     2)d+1~D 차원까지는 scale-and shift를 하는 affine transform을 취한다.
     - 여기서 scale, shift parameter들은 처음 d차원의 function들을 이용한다.

<img width="147" alt="스크린샷 2023-08-10 오후 4 58 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0a520803-cae8-4bed-a5d6-e879f865afcf"> 
<img width="601" alt="스크린샷 2023-08-10 오후 4 59 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bc2fabd6-2d45-43b8-87cb-d98735f98527">

## Normalizing flows 기반 모델 - Glow (결과) 
<img width="500" alt="스크린샷 2023-08-10 오후 5 00 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/273a84a4-11f9-4bb0-907d-b7d40aa2533f">

<img width="500" alt="스크린샷 2023-08-10 오후 5 00 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9b768e77-21e5-49fa-8ba4-85beacb40f5a">  
<img width="500" alt="스크린샷 2023-08-10 오후 5 01 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/50097023-a5ab-4096-96fd-230cd6825d9c">  
<img width="500" alt="스크린샷 2023-08-10 오후 5 01 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f2b21e74-c470-4928-9390-0e8bc0bee649">  
<img width="500" alt="스크린샷 2023-08-10 오후 5 01 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ec7bf550-4a04-4c97-a76a-c8197985f890">

## Normalizing flows 기반 모델 - Wave-Glow (결과)
<img width="268" alt="스크린샷 2023-08-10 오후 5 02 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0b147ce0-1efd-4fe1-8808-051cc1ef83ad">    

Glow의 역연산가능한 1x1 Conv + affine coupling layer를 “mel-spectrogram -> audio” generation에 적용.
