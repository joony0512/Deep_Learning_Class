# <Generative model series #2> Flow Models - 3. Variations of Flow Models
## Affine flows
다변수 가우시안(multivariate gaussian) flow라고도 하고, affine변환을 하는 flow이다. 
- 아핀변환 : 선형변환 + 평행이동변환의 합성
- 파라미터 : 역변환 가능한 matrix A와 벡터 b로 이루어진다. 즉 아래를 만족한다.
  $f(x) = A^{-1}(x-b)$

샘플링은 x~N(0,1)일때, 아래와 같이 generation한다.  x = Az + b   
이때 행렬 형태로 인해 차원이 커질수록 log-likelihood의 컴퓨팅 코스트가 급격하게 증가한다. 
- f의 Jacobian은 $A^{-1}$ 이고 log-likelihood에 포함되는 det(A)계산이 필요하다.
- 행렬의 역함수 연산은 비싸다 (O(N^3)) -> 결과적으로 매우 비싼 연산이 필요하다.

## Elementwise flows
flow연산을 각 element별로 따로 적용하여 연산량을 획기적으로 줄일 수 있다. <img width="234" alt="스크린샷 2023-08-09 오후 5 44 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2da5a818-e6d9-458c-b5bb-9da2d32cd2dc">  
- elementwise affine함수 , cdf flow등을 사용할 수 있다.

Jacobian이 diagonal(대각선값만가짐)하므로 determinant를 계산하기 쉬워진다.  <img width="104" alt="스크린샷 2023-08-09 오후 5 47 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/10dec254-413e-452c-a8a5-94219ff71f3e">

<img width="234" alt="스크린샷 2023-08-09 오후 5 46 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fb2ddba2-68ce-4569-807a-82c57cb3b671">  
이고 이는 아래와 같이 정리된다.  
<img width="148" alt="스크린샷 2023-08-09 오후 5 46 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d2fc7e3-ddea-447f-91d7-fbbbdbb07ba8">

## Normalizing flows 기반 모델 - NICE/RealNVP
<img width="402" alt="스크린샷 2023-08-09 오후 5 48 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ea6b83ad-aa8a-446d-9542-dcbbc7d6506a">

## Normalizing flows 기반 모델 - NICE (Non-linear independent component estimation) / RealNVP(Real-valued Non-Volume Preserving)
아핀 커플링 레이어(affine coupling layer)을 정의  

<img width="426" alt="스크린샷 2023-08-09 오후 5 58 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/be7d1c03-059c-42ef-aea0-941f019fc2d6">

- variable을 2파트로 나눈다. 전체가 D일때, d<D인 지점으로 $x_{1:d}, x_{d+1 : D}$ 로 분리한다.
- RealNVP의 경우 s,t을 각각 scale과 translation을 하는 함수($R^d -> R^{D-d}$ 이고 $\odot$ 을 element-wise곱이라고 했을때,
  <img width="562" alt="스크린샷 2023-08-09 오후 5 55 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ff22c0a0-dcaf-4624-b431-52147ac1ce2f"> 이다.
  - x, z는 $s_\theta , t_\theta$ 의 역함수를 구하지 않고도 invertible하다.
    - $s_\theta , t_\theta$ 는 제한없는 임의의 함수 (neural network)로 나타낼 수 있다.
    - data-parameterized된 element-wise flow로 생각할 수도 있다.
    - 이때 위 변환의 Jacobian은 아래와 같다 .  
      <img width="447" alt="스크린샷 2023-08-09 오후 5 57 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b63cfd53-9910-4884-8506-b709e76b5adc">
## Normalizing flows 기반 모델 - NICE (Non-linear independent component estimation)는?
NICE에서는 coupling layer를 사용하고 내부의 g function으로 additive function을 사용했다. 
덕분에 Inversion이 가능하고 Jacobian의 Determinant를 구하기 쉬워 Normalizing Flow를 구현할 수 있다. 
하지만 내부 함수 g를 단순하게 +를 사용했기에 복잡한 데이터를 표현하기 어렵다는 한계가 있었다. -> scale term이 없다.  
<img width="368" alt="스크린샷 2023-08-09 오후 6 26 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9dbb2eca-11e1-4fd0-9f37-af38f1d75f23">


## Normalizing flows 기반 모델 - NICE / RealNVP의 partitioning (affine coupling layers을 위한 masking schema)
<img width="536" alt="스크린샷 2023-08-09 오후 5 59 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8a533fc0-c5d3-4e72-8734-3670b7694e32">

## Normalizing flows 기반 모델 - RealNVP(Real-valued Non-Volume Preserving)
Triangular Matrix의 Determinant는 대각 성분의 곱으로 표현된다. 따라서 Affine Transform으로 표현되는 y의 Jacobian을 구해보면 다음과 같다.  
<img width="256" alt="스크린샷 2023-08-09 오후 6 05 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b74bdc2f-a451-4df3-b6a4-9ec631d47021">

따라서 det(J)은 다음과 같다.  
<img width="336" alt="스크린샷 2023-08-09 오후 6 19 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5192378c-7af9-4875-9bd8-f2b2c1146de3">  
심지어, Jacobian의 Determinant를 구할 때 함수 s, t의 Jacobian을 구하지 않아도 된다. 따라서 s,t는 복잡한 함수를 사용할 수 있다.
- 그 말은 s, t는 DNN으로 표현할 수 있다는 의미.  
  
<img width="599" alt="스크린샷 2023-08-09 오후 6 20 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c78930ae-1977-402f-9580-868e8a004f0a">
- s, t의 역을 계산하는 것을 필요로 하지않는다.

## Normalizing flows 기반 모델 - RealNVP(Real-valued Non-Volume Preserving)의 결과
<img width="600" alt="스크린샷 2023-08-09 오후 6 23 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1f908959-6474-4dcb-b563-5912b071ae5d">
<img width="600" alt="스크린샷 2023-08-09 오후 6 23 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/396e75cb-9a64-44e1-8f70-1257bea92a56">

## Normalizing flows 기반 모델 - Directed graphical models & autoregressive flows
- Bayes nets에 inspired되어 flow를 sequential하게만 구성하는게 아닌, directed acyclic graph(DAG)형태로 구성할 수 도 있을것이다.   
- Bayes nets의 sampling process가 flow이고, 이 graph구성이 auto-regressive하다면, 'auto-regressive flow'라고 한다.  
  <img width="368" alt="스크린샷 2023-08-09 오후 6 31 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3b64abc1-1b2a-4250-8035-caaeb465018f">
- z에 invertible한 mapping($f_\theta^{-1}$)을 반복하여 x를 sampling한다.
  - f를 사용하는 autoregressive log-likelihood 계산보다 시간이 더 많이 소요된다.
 
## Normalizing flows 기반 모델 - Autoregressive flows
- x->z을 매핑하는 과정을 전부 병렬화(fully parallelizable)하여 fitting할 수 있다.<img width="236" alt="스크린샷 2023-08-09 오후 6 39 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bc0a678c-dea2-4735-ba67-2ed3155482e5">
- Autoregressive flow에서
  - x->z는 autoregressive model의 log-likelihood의 계산과 같다.
  - z->x는 autoregressive model의 'sampling'방법과 같다.  
    <img width="280" alt="스크린샷 2023-08-09 오후 6 40 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/226bb8bb-4c74-4b8e-8cae-cb90f70d69f9">

## Normalizing flows 기반 모델 - Masked Autoregressive Flows (MAF)
- 2개의 랜덤 variable z~$\pi(z) and x \sim p(x)$ 이 있을때, pdf $\pi(z)$이 알려져 있으면, 
  MAF는 p(x)을 학습하는 것을 원칙으로 한다.
- MAF는 AF처럼 과거생성데이터($x_{1: i-1}$)을 condition된 input로 넣어 각각의 $x_i$ 을 생성한다.
- Data generation(새 x을 생성 ; inference)
  - <img width="737" alt="스크린샷 2023-08-09 오후 6 58 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6f1d5035-6560-4f12-923e-8e5649333cfc">
- Density estimation (x가 알려져 있을때 ; train)
  - <img width="289" alt="스크린샷 2023-08-09 오후 6 59 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/def9f1fd-9aee-4b7f-9a2e-92738aa5aa1e">
- Generation process는 sequential하기에 매우 느리다.
- 반면 density estimation은 Masked Autoregressive Model(i.e.  MADE, Pixel CNN, Transformer Decoder)을 사용하면 한번만 inference하면 되기에 빠르다.
  - Transformation함수(scale, shift)는 엳변환되기 쉽고, def(Jacobian)도 쉽게 컴퓨팅 될 수 있다. 
