## CH04_04_최적화의난제들

## ****경사하강법의 난제****

- 실제 minibatch 등을 이용한 경사 하강법은 목적함수를 최적화할 때, 실제로 어떤 종류의 임계점(critical point)에 도달하지 못할 때가 있다!
🡺 이런 상황을 ill-conditioned 되었다고 한다.
- 그러나, 역설적이게도, 이와 관계없이 네트워크의 학습 프로세스는 성공적일 수 있다.
    ![Untitled (61)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/474c02a0-16f1-46c9-bdf2-90f33049fc6f)

    
- 위 그래프처럼 Gradient norm이 커지더라도 classification error (분류 오차)는 매우 낮게 내려갈 수 있다.

### ****국소 최저/최대치 (local minima/maxima)****

- 많은 딥러닝 문제들은 실제 convex가 아닐수도 있고, 이는 곧 global minima가 아닌 많은 많은 local minima값을 가지게 한다.
→DNN이 많은 local minima를 가지는 주요한 이유는 은닉층(hidden layers)들을 처리하는 경우의 조합이 엄청 많기 때문인데, 만약 k개의 layer와 각각n개의 노드가 있다면, → $n!^k$만큼 가능한 경우의 수가 나온다.

### Saddle Point(안장점), Plateaus(고원형태) 등

- ****3차원 이상에서는, 특정 축에서는 convex스러우나, 실제로는 convex가 아닌 경우가 있다.****
    ![Untitled (62)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ae2df2ef-e3d5-4fb2-adf0-27c5c4089d29)

    
- 🡪 특히, 높은 차원의 non-convex 함수에서, saddle point는 특정 축에서는 local minima 를 가지나, 다른 축에서는 local maximum을 가지는 지점이다.
- SGD를 할 때, saddle point 등에 빠져, global optima를 찾는데 실패하게 하는 요인이 될 수 있다.
- 또한, 수렴 속도를 늦추는 범인 중에 하나다.

### 절벽문제(Cliff) 문제

- DNN은 많은 레이어가 있어 종종 절벽과 비슷한 부분 역시 존재한다.
    - Gradient가 갑자기 커지면서 업데이트가 튈수 있다.
    - 이는 gradient clipping으로 막을 수 있다.
        
        ![Untitled (63)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f413dbb9-57aa-4112-9abd-1df76db1db54)


### ****긴 구간 의존성 (long-term dependencies)****
![Untitled (64)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2452327a-b02b-4bac-828d-2ba3d15b4d43)


- 만약, 컴퓨팅 그래프가 극도로 깊다면 (i.e. RNN), gradient가 매우 작은 값이 되어 사라지거나, 무한대로 발산하여 터질 수 있다. (vanishing gradient descent & exploding gradient)
- Vanishing gradient 🡪 어떤 방향으로 파라미터들을 업데이트할지 모르게 됨.
Exploding gradient 🡪 학습과정을 불안정하게 만든다.
