# 합성곱 신경망 (CNN) - 3 CNN, going deeper

## CNN의 발전 방향
- 2012~2015년까지는, ‘깊게’ CNN을 잘 쌓는 역사 였다!
- 그러나, 깊게 쌓을 수록 gradient vanishing문제가 있어 점점 갈수록 gradient가 더 작아졌다.

<img width="222" alt="스크린샷 2023-06-30 오후 6 07 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d615a20-e35b-4e0b-9a8f-c13c92fd3da9">

## CNN의 발전 방향 - VGGNet (2014)
- 최대로 많이 깊게 들어간 모델은 VGGNet19로 19개의 layer
- 그러나, 깊게 쌓을 수록 gradient vanishing문제가 있었다…!
- Top 5 error 는 7.3% 
  - 점점 갈수록 gradient가 더 작아졌다. 그래서 더 깊게 못 쌓았음.

<img width="246" alt="스크린샷 2023-06-30 오후 6 09 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/11c5f403-73f5-482b-9700-c80999cd18bd">

## CNN의 발전 방향 - GoogLeNet (2014)
<img width="113" alt="스크린샷 2023-06-30 오후 6 12 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/faa5a70b-66b4-4150-b2c2-a1d6dc3c1983">

- Pooling layer를 사용하여 27개 layer를 쌓았다.
- 계산량을 Top5 error 6.7%로 개선!
- Inception 모듈에서 여러개의 filter size와 pooling을 결합하는 아이디어 사용 : 다양한 scale의 feature를 추출할 수 있게 !
  - 그러나 이 경우 비싸다.

- network in network 개념 사용.
  - 1x1 conv를 network in network의 개념처럼 사이에 넣어 , 연산량을 줄였다!
    
<img width="624" alt="스크린샷 2023-06-30 오후 6 10 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9a4b0e79-aa1b-4e0d-9a54-849dee8c376b">

## CNN의 발전 방향 - ResNet (2015)
- 그러나, 그 이후의 아무 과정없이 layer를 쌓는 것은 큰 성과를 쌓지 못했다.
  - Why? :Gradient vanishing !

- 그러나, ResNet(2015)에서 residual connection(skip connection) 개념을 사용하여 본 문제를 해결하였다.
  - 사람 뉴런의 pyramidal cell의 결합과 유사하다 !

<img width="234" alt="스크린샷 2023-06-30 오후 6 14 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/97211b6b-ff27-4f67-bb5d-2d5b275973d3">

- 왜 Residual Connection 은 gradient vanishing에도 괜찮을까?
- l번째에 있는 original block의 연산을 f(x)라 하고, skip connection의 연산을 h(x)라고 하자.
- 만약 h(x) = x로 identical 하다면, $x_{l+1} = x_l + f(x_l)$ 이고 $x_{l+2} = x_{l+1} + f(x_{l}) + f(x_{l+1})$ 이고  일반항을 구하면 <img width="151" alt="스크린샷 2023-06-30 오후 6 38 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/20988c24-cc3f-4486-91ff-91d4fa0711bf"> 를 만족한다. 즉, 여러번 쌓여도 $x_l$은 남아있고, original block의 연산을 더할 뿐이다.
- <img width="151" alt="스크린샷 2023-06-30 오후 6 38 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/20988c24-cc3f-4486-91ff-91d4fa0711bf"> 에서 backpropagation을 해도, <img width="258" alt="스크린샷 2023-06-30 오후 6 43 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fd16ab48-8319-45e6-953a-4c9ed2bf17c2">를 만족한다. 즉, 여러번 쌓여도 $x_l$은 남아있고, original block의 gradient연산을 더할 뿐이다.
  - gradient vanishing 해결하고 심지어 연산이 크게 추가되지도 않는다.

- 한편, 1x1 연산을 전후로 더하는 bottleneck 구조를 취해 퍼포먼스를 높였다.
<img width="493" alt="스크린샷 2023-06-30 오후 6 46 02"   src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2430cde1-a9ca-45ea-a417-051b894b7274">

- 3.57% 의 top5 error를 보였다 !
 <img width="245" alt="스크린샷 2023-06-30 오후 6 46 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b2750b9c-3cde-4b18-a66d-d9a2a6886653">


## CNN의 발전 방향 - DenseNet (2017)
- ResNet의 skip connection을 보다 일반화 하여, 옆의 붙어있는 layer 뿐만 아닌, 모든 layer간에 skip connection을 붙였다.

- 더 gradient vanishing문제를 완화한다. 그러나 down sampling (pooling)이 필요하므로 그 구간별로 다르게 DenseNet block을 취한다!

<img width="318" alt="스크린샷 2023-06-30 오후 6 49 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/33c588fa-b110-49ff-a355-f38c791fd7c2">

- 일반 ResNet보다 성능이 개선됨

<img width="522" alt="스크린샷 2023-06-30 오후 6 49 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/10fc387c-0770-45a0-a9d6-22fb53bf11a2">
