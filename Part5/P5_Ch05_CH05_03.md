# 합성곱 신경망 (CNN) - 3 CNN, going deeper

## CNN의 발전 방향
- 2012~2015년까지는, ‘깊게’ CNN을 잘 쌓는 역사 였다!
- 그러나, 깊게 쌓을 수록 gradient vanishing문제가 있어 점점 갈수록 gradient가 더 작아졌다.

<img width="222" alt="스크린샷 2023-06-30 오후 6 07 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4d615a20-e35b-4e0b-9a8f-c13c92fd3da9">

## CNN의 발전 방향 - VGGNet (2014)
- 최대로 많이 깊게 들어간 모델은 VGGNet19로 19개의 layer
- 그러나, 깊게 쌓을 수록 gradient vanishing문제가 있었다…!
- Top 5 error 는 7.3% 
  - 점점 갈수록 gradient가 더 작아졌다. 그래서 더 깊게 못 쌓았음.

<img width="246" alt="스크린샷 2023-06-30 오후 6 09 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/11c5f403-73f5-482b-9700-c80999cd18bd">

## CNN의 발전 방향 - GoogLeNet (2014)
<img width="113" alt="스크린샷 2023-06-30 오후 6 12 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/faa5a70b-66b4-4150-b2c2-a1d6dc3c1983">

- Pooling layer를 사용하여 27개 layer를 쌓았다.
- 계산량을 Top5 error 6.7%로 개선!
- Inception 모듈에서 여러개의 filter size와 pooling을 결합하는 아이디어 사용 : 다양한 scale의 feature를 추출할 수 있게 !
  - 그러나 이 경우 비싸다.

- network in network 개념 사용.
  - 1x1 conv를 network in network의 개념처럼 사이에 넣어 , 연산량을 줄였다!
    
<img width="624" alt="스크린샷 2023-06-30 오후 6 10 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9a4b0e79-aa1b-4e0d-9a54-849dee8c376b">

## CNN의 발전 방향 - ResNet (2015)
- 그러나, 그 이후의 아무 과정없이 layer를 쌓는 것은 큰 성과를 쌓지 못했다.
  - Why? :Gradient vanishing !

- 그러나, ResNet(2015)에서 residual connection(skip connection) 개념을 사용하여 본 문제를 해결하였다.
  - 사람 뉴런의 pyramidal cell의 결합과 유사하다 !

<img width="234" alt="스크린샷 2023-06-30 오후 6 14 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/97211b6b-ff27-4f67-bb5d-2d5b275973d3">

- 왜 Residual Connection 은 gradient vanishing에도 괜찮을까?
- l번째에 있는 original block
