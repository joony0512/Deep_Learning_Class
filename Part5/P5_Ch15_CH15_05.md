# Deep Reinforcement Learning - 5. Deep RLì˜ ë°œì „ - Off-policy Policy-based Model-free RL
## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Policy Optimizationê³¼ Q-learningì„ ì„ëŠ”ë‹¤ë©´?

- Policy optimization ë°©ë²•(i.e. REINFORCE, actor-critic)ì˜ ì£¼ìš” ê°•ì ì€ ì›í•˜ëŠ” ê²ƒì„ ì§ì ‘ ìµœì í™”!
  - ìµœì í™” ê²°ê³¼ë¬¼ì´ ì•ˆì •ì ì´ê³  ì‹ ë¢°ë„ê°€ ë†’ë‹¤!
- ë°˜ëŒ€ë¡œ Q-learningê³¼ ê°™ì€ value optimizationì€ $ğ‘„_\theta$ ì„ í›ˆë ¨í•˜ì—¬ self-consistentí•œ ë°©ì •ì‹ì„ ë§Œì¡±ì‹œí‚´ìœ¼ë¡œì¨ ì—ì´ì „íŠ¸ ì„±ëŠ¥ì— ëŒ€í•´ ê°„ì ‘ì ìœ¼ë¡œë§Œ ìµœì í™”!
  - Value functionì˜ ë§ì€ ì‹¤íŒ¨ ê°’ì´ ì¡´ì¬í•˜ë¯€ë¡œ ì•ˆì •ì„±ì´ ë–¨ì–´ì§€ëŠ” ê²½í–¥ì´ ìˆìœ¼ë‚˜, policy optimization ê¸°ë²•ë³´ë‹¤ ë°ì´í„°ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì‘ì—… ì‹œ ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ìƒë‹¹íˆ ë†’ë‹¤!
- ìƒí˜¸ ë³´í™˜ì ì¸ ë‘ ëª¨ë¸ì„ ì„œë¡œ ì„ì„ ìˆ˜ ìˆì„ê¹Œ?

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Off-policy policy gradient ë°©ë²•
- Policy gradientì™€ Q-learningì€ ì–‘ë¦½í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì´ ì¡´ì¬í•˜ë©°(ì–´ë–¤ ìƒí™©ì—ì„œëŠ” ë™ë“±) ë‘ ê·¹ë‹¨ ì‚¬ì´ì— ì¡´ì¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ë²”ìœ„ê°€ ì¡´ì¬
  - Off-policy policy gradient!
  - ì–‘ìª½ì˜ ì¥ë‹¨ì ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.
- Off-policy ì ‘ê·¼ ë°©ì‹ì€ ì „ì²´ trajectoryì„ í•„ìš”ë¡œ í•˜ì§€ ì•Šìœ¼ë©° ê³¼ê±° episodic memory("ì²´í—˜ ì¬ìƒ")ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ í›¨ì”¬ ë” ë‚˜ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ì–»ì„ ìˆ˜ ìˆë‹¤
- Sample collectionì€ target policyì™€ ë‹¤ë¥¸ behavior policyë¥¼ ë”°ë¥´ë¯€ë¡œ ë” ë‚˜ì€ explorationì„ í•œë‹¤

## ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Off-policy policy gradient ë°©ë²•
- ìƒ˜í”Œì„ ëª¨ìœ¼ëŠ” behavior policyëŠ” hyperparameterì²˜ëŸ¼ ë¯¸ë¦¬ ì •ì˜ í•´ë‘ê³  ì‹œì‘! - ğ›½(ğ‘|ğ‘ )  
- Policy gradient ì˜ Objective function

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 38 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8c55a362-0c77-44bf-a805-58476dfad236">
ì€ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œì •ë¦¬ë  ìˆ˜ ìˆë‹¤.

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 39 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/830a8c33-37e6-4fa8-8e1e-4c4403cb6a62">

ì´ë•Œ, $ğ‘‘^\beta (ğ‘ )$ ì€ behavior policy ğ›½ì˜ stationary distributionì´ë‹¤.  
ì´ë•Œ, <img width="200" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 40 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/75ea02c6-1460-4dda-9659-adb6bdc29f03">
ì´ê³ , $ğ‘„^\pi$ ëŠ” target policy ğœ‹ì— ê´€í•œ estimatedëœ action-value functionì´ë‹¤.  

Training observationì„ ğ‘ ~ ğ›½(ğ‘|ğ‘ ) ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ êµ¬í•œë‹¤ í•˜ê³ , Update rule( $âˆ‡_\theta J(Î¸)$ )ì„ êµ¬í•˜ë©´,

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 42 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/468bc235-62cd-4fd6-8c92-5df54122fa38">

ì´ë‹¤. ì´ë•Œ, $ğœ‹_\theta (ğ‘|ğ‘ ) âˆ‡_\theta ğ‘„^\pi (ğ‘ ,ğ‘)$ ì„ ê³„ì‚°í•˜ê¸° ì–´ë µë‹¤. ëŒ€ì‹ , ë¬´ì‹œí•˜ì—¬ ê·¼ì‚¬í•˜ì—¬ë„ [Degris, Thomas, Martha White, and Richard S. Sutton. "Off-policy actor-critic." (2012)]ì— ì˜í•´ true local minimumì— ë„ë‹¬ í•˜ë¯€ë¡œ,

<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 44 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e7395aa8-db47-4b56-bbc9-a55dc1f71f0e">

ë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤! ( $ğœ‹_\theta (ğ‘|s)$ : target density, ğ›½(ğ‘|ğ‘ ) : proposal density)
- ì°¸ê³ : $ğœ‹_\theta(ğ‘|ğ‘ )$ ë¥¼ samplingí•˜ê¸° í˜ë“œë¯€ë¡œ importance samplingì´ìš©!
- $ğœ‹_\theta(ğ‘|ğ‘ ) \over \beta(ğ‘|s)$ ëŠ” importance samplingì—ì„œì˜ importance weight ì´ë‹¤.

ìš”ì•½í•˜ë©´, policy gradientë¥¼ off-policy settingì— ì ìš©í•˜ë ¤ë©´, ì•„ë˜ì˜ ì‹

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 46 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2bccec80-73ff-48f1-8e00-c13a356a5cc8">
ì„ ìµœì í™” í•˜ë©´ ëœë‹¤!
  
ì¦‰, $ğ‘„^\pi (ğ‘ ,ğ‘) âˆ‡_\theta log ğœ‹_\theta (a|s)$ ì˜ weighted sumì„ í†µí•´ ìµœì í™”í•˜ë©´ ë˜ë©°,  
ì´ë•Œ weightëŠ” $ğœ‹_\theta(ğ‘|ğ‘ ) \over \beta(ğ‘|s)$ ìœ¼ë¡œ target policy( $ğœ‹_\theta (ğ‘|s)$ )ì™€ behavior policy(ğ›½(ğ‘|ğ‘ ) )ì˜ ë¹„ìœ¨ì´ë‹¤! 

##  ê°•í™” í•™ìŠµì˜ êµ¬ë¶„ - Off-policy policy gradient ì˜ ì¢…ë¥˜
DDPG(Deep deterministic policy gradient)
 - Deterministic policy (for target update)ì™€ Q-function (stochastic behavior policy for exploration)ì„ ì„œë¡œì‚¬ìš©í•˜ì—¬ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜! (Q-functionë¡œ behavior policyë¥¼ ì–»ê¸°ì— off-policy)
 - ì°¸ê³ : ë™ì‹œì— actor-critic ì•Œê³ ë¦¬ì¦˜ì´ê¸°ë„ í•˜ë‹¤.

SAC(Soft actor-critic)
  - Stochastic policyì™€ entropy regularization ë“±ì„ ì‚¬ìš©í•˜ì—¬ DDPGë¥¼ ê°œì„ .

<img width="600" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-30 á„‹á…©á„’á…® 6 51 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2545677a-386d-4a6d-970f-839d8a5871fa">
