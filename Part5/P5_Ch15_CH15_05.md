# Deep Reinforcement Learning - 5. Deep RL의 발전 - Off-policy Policy-based Model-free RL
## 강화 학습의 구분 - Policy Optimization과 Q-learning을 섞는다면?

- Policy optimization 방법(i.e. REINFORCE, actor-critic)의 주요 강점은 원하는 것을 직접 최적화!
  - 최적화 결과물이 안정적이고 신뢰도가 높다!
- 반대로 Q-learning과 같은 value optimization은 $𝑄_\theta$ 을 훈련하여 self-consistent한 방정식을 만족시킴으로써 에이전트 성능에 대해 간접적으로만 최적화!
  - Value function의 많은 실패 값이 존재하므로 안정성이 떨어지는 경향이 있으나, policy optimization 기법보다 데이터를 더 효과적으로 재사용할 수 있기 때문에 작업 시 샘플 효율성이 상당히 높다!
- 상호 보환적인 두 모델을 서로 섞을 수 있을까?

## 강화 학습의 구분 - Off-policy policy gradient 방법
- Policy gradient와 Q-learning은 양립할 수 있는 조건이 존재하며(어떤 상황에서는 동등) 두 극단 사이에 존재하는 알고리즘의 범위가 존재
  - Off-policy policy gradient!
  - 양쪽의 장단점을 가질 수 있다.
- Off-policy 접근 방식은 전체 trajectory을 필요로 하지 않으며 과거 episodic memory("체험 재생")를 재사용하여 훨씬 더 나은 샘플 효율성을 얻을 수 있다
- Sample collection은 target policy와 다른 behavior policy를 따르므로 더 나은 exploration을 한다

## 강화 학습의 구분 - Off-policy policy gradient 방법
- 샘플을 모으는 behavior policy는 hyperparameter처럼 미리 정의 해두고 시작! - 𝛽(𝑎|𝑠)  
- Policy gradient 의 Objective function

<img width="400" alt="스크린샷 2023-08-30 오후 6 38 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8c55a362-0c77-44bf-a805-58476dfad236">
은 아래와 같이 다시정리될 수 있다.

<img width="400" alt="스크린샷 2023-08-30 오후 6 39 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/830a8c33-37e6-4fa8-8e1e-4c4403cb6a62">

이때, $𝑑^\beta (𝑠)$ 은 behavior policy 𝛽의 stationary distribution이다.  
이때, <img width="200" alt="스크린샷 2023-08-30 오후 6 40 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/75ea02c6-1460-4dda-9659-adb6bdc29f03">
이고, $𝑄^\pi$ 는 target policy 𝜋에 관한 estimated된 action-value function이다.  

Training observation을 𝑎 ~ 𝛽(𝑎|𝑠) 를 샘플링하여 구한다 하고, Update rule( $∇_\theta J(θ)$ )을 구하면,

<img width="500" alt="스크린샷 2023-08-30 오후 6 42 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/468bc235-62cd-4fd6-8c92-5df54122fa38">

이다. 이때, $𝜋_\theta (𝑎|𝑠) ∇_\theta 𝑄^\pi (𝑠,𝑎)$ 을 계산하기 어렵다. 대신, 무시하여 근사하여도 [Degris, Thomas, Martha White, and Richard S. Sutton. "Off-policy actor-critic." (2012)]에 의해 true local minimum에 도달 하므로,

<img width="500" alt="스크린샷 2023-08-30 오후 6 44 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e7395aa8-db47-4b56-bbc9-a55dc1f71f0e">

로 근사할 수 있다! ( $𝜋_\theta (𝑎|s)$ : target density, 𝛽(𝑎|𝑠) : proposal density)
- 참고: $𝜋_\theta(𝑎|𝑠)$ 를 sampling하기 힘드므로 importance sampling이용!
- $𝜋_\theta(𝑎|𝑠) \over \beta(𝑎|s)$ 는 importance sampling에서의 importance weight 이다.

요약하면, policy gradient를 off-policy setting에 적용하려면, 아래의 식

<img width="400" alt="스크린샷 2023-08-30 오후 6 46 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2bccec80-73ff-48f1-8e00-c13a356a5cc8">
을 최적화 하면 된다!
  
즉, $𝑄^\pi (𝑠,𝑎) ∇_\theta log 𝜋_\theta (a|s)$ 의 weighted sum을 통해 최적화하면 되며,  
이때 weight는 $𝜋_\theta(𝑎|𝑠) \over \beta(𝑎|s)$ 으로 target policy( $𝜋_\theta (𝑎|s)$ )와 behavior policy(𝛽(𝑎|𝑠) )의 비율이다! 

##  강화 학습의 구분 - Off-policy policy gradient 의 종류
DDPG(Deep deterministic policy gradient)
 - Deterministic policy (for target update)와 Q-function (stochastic behavior policy for exploration)을 서로사용하여 동시에 학습하는 알고리즘! (Q-function로 behavior policy를 얻기에 off-policy)
 - 참고: 동시에 actor-critic 알고리즘이기도 하다.

SAC(Soft actor-critic)
  - Stochastic policy와 entropy regularization 등을 사용하여 DDPG를 개선.

<img width="600" alt="스크린샷 2023-08-30 오후 6 51 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2545677a-386d-4a6d-970f-839d8a5871fa">
