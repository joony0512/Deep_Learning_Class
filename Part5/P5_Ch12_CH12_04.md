# <Generative model series #4> - Implicit Models - 4. GAN의 발전 - part 1
## Deep Convolutional GAN(DCGAN; 2015-2016) - Convolutional Net + GAN 

- 기본 GAN의 fully-connected layer을 CNN로 바꾸면 어떨까?
	- 이때 generator는 up-sampling을 위해 de-convolution layer를 쓰자! 
- Spatial max pooling을 지우고, 대신 strided convolution을 쓰자!
- 또한, Batch-normalization을 각 layer 뒤에 넣어보자! (그러나, generator의  output, discriminator의 input 에는 X)
- Generator에 ReLU를 hidden layer에 쓰고, Tanh를 output layer에 쓰자!
- Discriminator에는 Leaky ReLU를 쓰고, sigmoid output을 쓰자.

  <img width="407" alt="스크린샷 2023-08-20 오후 6 33 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/164aabfc-c3a3-41d5-88ae-2f6d6091c87d">

## Deep Convolutional GAN(DCGAN; 2015-2016) - 결과 – Vector arithmetic, Interpolation, representation learning
<img width="300" alt="스크린샷 2023-08-20 오후 6 34 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/93f13ea3-8f47-47d9-9647-d7db13417c14">

<img width="200" alt="스크린샷 2023-08-20 오후 6 34 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b04c93f3-8e29-4814-9bf0-1a3198c67d8f">

<img width="250" alt="스크린샷 2023-08-20 오후 6 35 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fad4493f-ac93-4dbc-9c36-9a2e1ca08dd2">

## Deep Convolutional GAN(DCGAN; 2015-2016) - 의의와 남은 문제
GAN이 좋은 품질로 생성되고 interpolation도 가능함을 보임!

그러나, 아래 문제들은 개선될 여지가 많음!
- 불안정한 학습
- 아키텍처에 따라 성능이 많이 달라진다!
- 하이퍼 파라미터의 변경에 의해 성능이 많이 달라진다!

## Improved techniques for training GAN (2016) - OpenAI팀에 의해 GAN의 추가 발전 방향이 제시됨!
- Feature matching
- Historical averaging
- Minibatch discrimination
- Virtual batch-normalization
- One-side label smoothing
- Semi-supervised learning
- Inception Score(exp($𝐸_x 𝐷_{KL}$ (𝑝(𝑦|𝑥)||𝑝(𝑦)))도 여기서 제안되었다.
  - 모델의 성능과 사람의 판단을 correlate 시킨다!
  - 생성물의 다양성을 보장할 수 있다!

## Improved techniques for training GAN (2016) - Feature matching
- Discriminator에 새로운 목표를 지정하여 over-training을 방지하고, GAN의 불안정성 (i.e. discriminator saturation, module collapsed)을 완화
- Generator에서 생성한 분포와 실제 데이터의 분포를 matching 시키기 위해 Discriminator 중간층의 activation 함수를 이용
  -  진짜 가짜를 나누는 것이 아닌, 진짜와 같은 feature을 가지고 있는지를 파악하는 방식으로 학습을 진행
- 𝑓(𝑥) : discriminator의 중간 layer의 activation 함수라면, 새로운 손실 함수는 다음이 된다!

  <img width="200" alt="스크린샷 2023-08-20 오후 7 07 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c55d0a8c-448f-45ca-80e2-814aaf3f900a">

- Discriminator 중간층의 output이 생성에 필요한 하나의 특징(feature)이며, random sampling된 z에 대해 분포가 비슷한지(matching) 살펴보는 것
- G가 목표하는 부분까지 도달하는 지 확인할 수 없지만, GAN 학습의 불안정성 완화에 효과적임을 실험적으로 확인!

## Improved techniques for training GAN (2016) - Historical averaging
- Discriminator와 generator의 손실 함수에 아래의 식을 추가.
- 𝜃가 모델 파라미터, 𝜃[𝑖]가 i번째 학습 과정에서 파라미터를 의미하면,

  <img width="130" alt="스크린샷 2023-08-20 오후 7 11 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4ae796ba-9580-4ee8-b375-9d79f846d655">

- Long-time series에서도 잘 작동하게 도와줌!

## Improved techniques for training GAN (2016) - Minibatch discrimination
- GAN이 실패하는 경우 중 하나는 Generator가 동일한(유사한) 출력을 하게 parameter가 학습되는 경우! 
	- Generator는 discriminator을 속이기만 하면 되기 때문에 발생!
- Vanilla GAN 등에서 정교하게 세팅하지 않고 실험하면 초기 noise에 따라 Loss가 급격하게 줄어들고 결과를 확인하면 이상한 noise만 출력하는 경우가 있다!
	- Discriminator가 각 example을 개별로 처리하기 때문에 출력 간의 관계를 고려하지 않기 때문 즉, 동일한 example 을 동일한 것으로 인식하지 못한다.
	- 배치(batch) 안에서 다른 데이터 간의 관계를 고려하도록 설계하자!

## Improved techniques for training GAN (2016) - Minibatch discrimination
-> Discriminator가 배치(batch) 안에서 다른 데이터 간의 관계를 고려하도록 설계하자!

 <img width="200" alt="스크린샷 2023-08-20 오후 7 27 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3db8d24e-037d-4225-aa25-ed00d24d9e56">
	
- $f(x_i) \in R^A$ : input $x_i$ 에 대한 feature vecture (latent vector)
  - discriminator의 중간 레이어에 의해 생성
  - $f(x_i)$ 에 𝑇 ∈ $R^{AXBXC}$ 텐서를 곱해 Matrix M( $M_i \in R^{BXC}$ )을계산!
  - 그리고 본 matrix의 row 간 L1-distance을 계산한다.

    <img width="200" alt="스크린샷 2023-08-20 오후 7 24 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/84aca648-359a-4019-a5ec-9ad16bf3ea81">

샘플 $x_i$ 가 있을 때 Minibatch layer에 대한 output layer 𝑜( $x_i$ ) 는 아래와 같이 정의하고 discriminator가 이를 고려하게 한다!

<img width="200" alt="스크린샷 2023-08-20 오후 7 25 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5e6ce2c6-8e30-47ba-9e2d-5e6001061530">

- 시각적으로 좋은 샘플을 보다 잘 생성!
  
  <img width="200" alt="스크린샷 2023-08-20 오후 7 27 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8d61a3c5-cca2-4684-be48-ef2e1638351a">

## Improved techniques for training GAN (2016) - One-sided label smoothing
- 타겟 레이블로 0(Fake)/1(Real) 대신 0.1/0.9 등의 smoothed value로 Discriminator을 훈련.
- Label smoothing이라고도 한다.
- CE: cross entropy라면,
- Cross entropy을 사용할 때 Discriminator의 비용?
  - 기존: CE(1, discriminator(data) + CE(0, discriminator(samples))
  - 제안: CE(.9, discriminator(data) + CE(0., discriminator(samples))
 
## Improved techniques for training GAN (2016) - One-sided label smoothing
왜 one-side라 할까?
- 𝛼 : positive label (real data) 𝛽: negative label (generator output) 이 주어질 때,
- 최적의 discriminator는 아래와 같다.

  <img width="200" alt="스크린샷 2023-08-20 오후 7 31 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b400b68e-ca00-4cf7-bfeb-ebd8968e73e7">

- 분자의 𝛽항이 0이아니라면, discriminator의 최적값이 변하게 만든다!
- Discriminator가 generator가 p_data(x)에 가까워지는것을 오히려방해한다.
  - 𝜷 $𝒑_{𝒎𝒐𝒅𝒆𝒍}(𝒙)$ 을 0로 하여 positive label만을 smoothing한다.

## Improved techniques for training GAN (2016) - Virtual batch normalization
Batch normalization로 인해 Mini batch 의 값들이 Discriminator 출력에 서로 영향을 줌. 
- 고정된 배치(reference batch)을 이용하여 해결
Reference batch는 학습 초기에 한번 선별되어 학습이 진행되는 동안 변하지 않는다!
- 이 값을 이용하여 normalize!
generator에서만 진행!
- 2개의 minibatch을 forward propagation하는 것은 느리기 때문

## Improved techniques for training GAN (2016) - Semi-supervised learning
- Discriminator에서 fake/true label(y)을 단순히 predict하는 것이 아닌, p(x, y)을 approximate하도록
즉, semi-supervised로 모델링할 수 있다!
- K개 class에서 generator가 만들어내는 값을 다른 class(K+1)로 잡아, supervised와 unsupervised로 만든다.
- 반면, Generator는 조건 p(x | y)을 만들지 않기에, 특별한 업데이트가 필요하지 않다.
  - 그러므로 semi-supervised환경에서는 Generator에 비해 discriminator에 보다 깊은 architecture을 쓰면 좋다.
  
  <img width="300" alt="스크린샷 2023-08-20 오후 7 57 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f1e14ade-ac5d-41a1-9a67-8afbb651c80e">
  
  <img width="500" alt="스크린샷 2023-08-20 오후 7 58 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3543b034-20ce-40fc-81d3-d0fa09482999">
