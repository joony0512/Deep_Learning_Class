# Meta Learning - 5. Meta Reinforcement Learning
## Meta Reinforcement Learning
- ë©”íƒ€ ê°•í™” í•™ìŠµì€ ê°•í™” í•™ìŠµ ë¶„ì•¼ì—ì„œ ë©”íƒ€ í•™ìŠµì„ í•˜ëŠ” ê²ƒ  
- ì¼ë°˜ì ìœ¼ë¡œ trainê³¼ test taskê°€ ë‹¤ë¥´ì§€ë§Œ ë™ì¼í•œ ë¬¸ì œ ê³„ì—´ì—ì„œ ì§„í–‰ëœë‹¤.
- i.e. ë³´ìƒ í™•ë¥ ì´ ë‹¤ë¥¸ multi-armed bandit, ë ˆì´ì•„ì›ƒì´ ë‹¤ë¥¸ ë¯¸ë¡œ, ì‹œë®¬ë ˆì´í„°ì—ì„œ ë¬¼ë¦¬ì  ë§¤ê°œë³€ìˆ˜ê°€ ë‹¤ë¥¸ ë¡œë´‡
  - + ì‚¬ëŒì˜ ë‡ŒëŠ” meta-RL ê¸°ë°˜ìœ¼ë¡œ taskë¥¼ ìˆ˜í–‰ í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìˆë‹¤.

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-09-01 á„‹á…©á„’á…® 7 06 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b757ae6d-bee2-4c53-8f40-3d09a5457102">

## Meta Reinforcement Learning - RL vs Meta-RL
- Meta-RLì€ RLê³¼ í° í‹€ì—ì„œ ë¹„ìŠ·í•˜ì§€ë§Œ, last reward( $ğ‘Ÿ_{t-1}$ )ì™€ last action( $a_{t-1}$ )ì´ í˜„ì¬ state( $s_{t}$ )ì— policy observationì„ ê²°í•©í•˜ì—¬ ì •í•´ì§„ë‹¤.
- ì¦‰, RLì—ì„œ ì•„ë˜ì™€ ê°™ì•˜ë‹¤ë©´, <img width="130" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-09-01 á„‹á…©á„’á…® 7 11 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d13483b1-3700-4e58-beeb-ad8ca709629a">
- meta-RLì—ì„œëŠ” ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤. <img width="150" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-09-01 á„‹á…©á„’á…® 7 12 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/79e63aa8-6416-4e21-8c62-7c03139ebf53">
- History ì •ë³´ë¥¼ ëª¨ë¸ì— ë„£ê³ , policyê°€ state, reward, actionì—ì„œì˜ dynamicì— ë‚´ë©´í™”ê°€ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤!
- ëŒ€í‘œì ì¸ ëª¨ë¸ì€, LSTM-A3C, LSTM-A2C, RL^2 ê°€ ìˆë‹¤.

## Meta Reinforcement Learning - Meta-RLì˜ í•™ìŠµ ìˆœì„œ
1. ìƒˆë¡œìš´ MDP ë¥¼ ìƒ˜í”Œë§ í•œë‹¤. $ğ‘€_i \sim ğ‘€$
2. ëª¨ë¸ì˜ hidden stateë¥¼ resetí•œë‹¤.
3. ë‹¤ì–‘í•œ trajectoryë¥¼ ëª¨ìœ¼ê³ , modelì˜ weightë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.
4. 1-3ì„ ë°˜ë³µí•œë‹¤.

## Meta Reinforcement Learning - LSTM-A2C/LSTM-A3C
<img width="800" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-09-01 á„‹á…©á„’á…® 7 24 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3aaf4b8a-3030-4cad-95d8-9c669d33236a">

A3Cì— ë¹„í•´, LSTM-A3CëŠ” backpropagationì„ ìœ„í•´ì„œë¼ê¸° ë³´ë‹¤, hidden stateì„ ë°”ê¾¸ê¸°ë¥¼ ìŠ¤ìŠ¤ë¡œ ì¡°ì ˆí•˜ëŠ” ê²ƒì„ ë°°ìš°ê¸° ìœ„í•´ RNNì„ í•™ìŠµí•œë‹¤.  
ë˜í•œ, policy learning moduleì—ì„œ $ğ‘_{t-1}:, ğ‘Ÿ_{t-1}$ :ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë°›ëŠ”ë‹¤! 
  - í™˜ê²½ì˜ ë³€í™”ê°€ ê°€ëŠ¥í•˜ê²Œ! $ğœ‹_\theta (ğ‘_{t-1}, ğ‘Ÿ_{t-1}, ğ‘ _t) â†’ ğ‘ âˆˆ ğ´$

## Meta Reinforcement Learning - RL^2: Fast Reinforcement Learning via Slow reinforcement learning
- ì—¬ëŸ¬ episodeë¥¼ ë‹´ì„ ìˆ˜ ìˆëŠ” â€œtrialâ€ì„ ì •ì˜
- Agent(policy)ëŠ” episodeê°„ ì •ë³´ë¥¼ hiddenì„ ë°›ì•„ íŒŒì•…í•œë‹¤.   
- Hiddenì„ prediction í•  ë•Œì—ëŠ”, ì´ì „ hidden ê°’, reward, termination flagë¥¼ ë°›ëŠ”ë‹¤.
- PolicyëŠ” recurrent neural networkì´ë©°, ğœ™(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘‘) ë¥¼ inputìœ¼ë¡œ ë°›ëŠ”ë‹¤!
