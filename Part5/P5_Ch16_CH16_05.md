# Meta Learning - 5. Meta Reinforcement Learning
## Meta Reinforcement Learning
- 메타 강화 학습은 강화 학습 분야에서 메타 학습을 하는 것  
- 일반적으로 train과 test task가 다르지만 동일한 문제 계열에서 진행된다.
- i.e. 보상 확률이 다른 multi-armed bandit, 레이아웃이 다른 미로, 시뮬레이터에서 물리적 매개변수가 다른 로봇
  - + 사람의 뇌는 meta-RL 기반으로 task를 수행 하는 것으로 알려져있다.

<img width="400" alt="스크린샷 2023-09-01 오후 7 06 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b757ae6d-bee2-4c53-8f40-3d09a5457102">

## Meta Reinforcement Learning - RL vs Meta-RL
- Meta-RL은 RL과 큰 틀에서 비슷하지만, last reward( $𝑟_{t-1}$ )와 last action( $a_{t-1}$ )이 현재 state( $s_{t}$ )에 policy observation을 결합하여 정해진다.
- 즉, RL에서 아래와 같았다면, <img width="130" alt="스크린샷 2023-09-01 오후 7 11 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d13483b1-3700-4e58-beeb-ad8ca709629a">
- meta-RL에서는 다음을 만족한다. <img width="150" alt="스크린샷 2023-09-01 오후 7 12 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/79e63aa8-6416-4e21-8c62-7c03139ebf53">
- History 정보를 모델에 넣고, policy가 state, reward, action에서의 dynamic에 내면화가 가능하게 한다!
- 대표적인 모델은, LSTM-A3C, LSTM-A2C, RL^2 가 있다.

## Meta Reinforcement Learning - Meta-RL의 학습 순서
1. 새로운 MDP 를 샘플링 한다. $𝑀_i \sim 𝑀$
2. 모델의 hidden state를 reset한다.
3. 다양한 trajectory를 모으고, model의 weight를 업데이트 한다.
4. 1-3을 반복한다.

## Meta Reinforcement Learning - LSTM-A2C/LSTM-A3C
<img width="800" alt="스크린샷 2023-09-01 오후 7 24 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3aaf4b8a-3030-4cad-95d8-9c669d33236a">

A3C에 비해, LSTM-A3C는 backpropagation을 위해서라기 보다, hidden state을 바꾸기를 스스로 조절하는 것을 배우기 위해 RNN을 학습한다.  
또한, policy learning module에서 $𝑎_{t-1}:, 𝑟_{t-1}$ :를 추가적으로 받는다! 
  - 환경의 변화가 가능하게! $𝜋_\theta (𝑎_{t-1}, 𝑟_{t-1}, 𝑠_t) → 𝑎 ∈ 𝐴$

## Meta Reinforcement Learning - RL^2: Fast Reinforcement Learning via Slow reinforcement learning
- 여러 episode를 담을 수 있는 “trial”을 정의
- Agent(policy)는 episode간 정보를 hidden을 받아 파악한다.   
- Hidden을 prediction 할 때에는, 이전 hidden 값, reward, termination flag를 받는다.
- Policy는 recurrent neural network이며, 𝜙(𝑠, 𝑎, 𝑟, 𝑑) 를 input으로 받는다!
