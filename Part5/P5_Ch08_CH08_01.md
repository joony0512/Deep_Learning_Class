# íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)- 1. Transformer

## íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer)
RNNì€ sequence-to-sequenceì—ì„œ í•„ìˆ˜ì ì´ì§€ ì•Šë‹¤?  
ì˜¤íˆë ¤, RNN êµ¬ì¡° ë“±ì€ long-term dependencyë¥¼ ì•¼ê¸°í•  ë¿ì´ë‹¤.  
ìš°ë¦¬ê°€ í•„ìš”í•œ ê²ƒì€ â€œattentionâ€ ë¿ì´ë‹¤.  -> â€˜attention is all you needâ€™  
2017ë…„ Vaswani, Ashish ë“± Google ì—°êµ¬íŒ€ì´ ë°œí‘œ.

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/59c69661-6ac1-4cf7-99ad-ec883de16b24)

## Recap: 2013-2014ë…„ RNNê¸°ë°˜ì˜ Sequence-to-sequence
ì¸ì½”ë”-ë””ì½”ë” ê¸°ë°˜ì˜ ê¸°ê³„ ë²ˆì—­(seq2seq)ì€, ì…ë ¥ ë¬¸ì¥ì—ì„œ í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ê³ ì •ëœ ê¸¸ì´ì˜ vectorë¡œ ë³€í™˜í•˜ì˜€ë‹¤.  
ê·¸ëŸ¬ë‚˜, ì´ëŠ” ì…ë ¥ ë¬¸ì¥ì´ ê¸¸ì–´ì§ì— ë”°ë¼ í¼í¬ë¨¼ìŠ¤ê°€ ì‹¬ê°í•˜ê²Œ ê°ì†Œí•˜ëŠ” ë¬¸ì œë¥¼ ë°œìƒì‹œì¼°ë‹¤.  
- LSTM, GRU ë“±ë„ ì™„ì „íˆ long-term dependencyë¥¼ í•´ê²°í•˜ì§€ ëª»í•˜ì˜€ë‹¤.

<img width="303" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 29 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f7c56e04-adb7-4b5d-bc50-d2bf35881b2b">
<img width="189" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 30 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/01385f2c-2a6a-4e5b-a72d-48d32c6ea858">

## Recap: 2014-2015ë…„ Attentional RNN
<img width="166" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 31 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/91993f4c-89dd-4064-95b3-b706827583e2">

Attentionêµ¬ì¡°ëŠ” RNNì˜ long-term dependency ë¬¸ì œë¥¼ ì–´ëŠì •ë„ í•´ê²°í•´ì¤¬ë‹¤.  
í•˜ì§€ë§Œ, ì™„í™”í–ˆì„ ë¿, ì™„ì „íˆ í•´ê²°í–ˆë˜ ê²ƒì€ ì•„ë‹ˆë‹¤.  
â€œh1 - h2 ì™€ h2 - h3ì´ ê³µìœ ë˜ëŠ” êµ¬ì¡°â€ëŠ” ì •ë³´ì˜ íš¨ìœ¨ì„ ì¶”êµ¬í•œ ê²ƒì´ë‚˜, ì™„ì „í•˜ì§„ ì•Šë‹¤.  
ê°œì„ í•  ì—¬ì§€ê°€ ì•„ì§ ìˆë‹¤!  

<img width="298" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 32 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/eab3d887-d16a-4655-911d-c06e7897a2fa">

## Recap: Self-attention
Attentionì´ encoder decoderêµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” seq2seqì—ì„œ ì •ì˜ë˜ì—ˆì§€ë§Œ, ë‹¤ë¥¸ ë¶„ì•¼ì—ì„œë„ ì“¸ ìˆ˜ ìˆë‹¤. 
- How?
  - Attention êµ¬ì¡°ì—ì„œ key, value, queryì„ ëª¨ë‘ ê°™ì€ ì†ŒìŠ¤ì— ê³„ì‚°í•˜ëŠ” ê²½ìš°ë¥¼ "Self-attention" ì´ë¼ ì •ì˜í–ˆì—ˆë‹¤!

  <img width="279" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 33 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0bec7fdb-d7d5-49ac-b5e4-e9042b18c90b">
  
  - dot-product attentionì¼ë•Œ <img width="216" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 40 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c177bfb4-43b7-4a0a-8efd-a7348cf0b31b">

(self-)attentionì€?   
<img width="825" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 5 42 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c455b418-d303-4b2c-bfec-cef960980dfb">

- ë¬´ì œí•œì˜ receptive field : CNNì— ë¹„í•´ ì¥ì 
- ë°ì´í„° ì‚¬ì´ì¦ˆì— ë”°ë¼ O(1) parameter scalingì„ ê°€ì§„ë‹¤. ë°ì´í„°ê°€ ì»¤ì ¸ë„ ê´œì°®ì€ í¸! : MLPì— ë¹„í•´ ì¥ì 
- RNNë“±ì— ë¹„í•´ parallel computingì´ ì‰½ë‹¤

## Self-attention VS CNN VS RNN
k : CNN kernel size, n : sequence length , d : input/output channel(size) ë¼ í• ë•Œ
- CNN
  - Conv layer computational cost : $O(knd^2)$
  - Sequence operation : O(1)
  - Maximum path length : $O(log_k n)$
- RNN
  - RNN cellë‹¹ Rnn hidden state ì—…ë°ì´íŠ¸ì‹œ ê³„ì‚° : O(d^2) (d x d matrixì—°ì‚°)
  - Recurrent layer computational cost : O(nd^2)
  - Sequence operation : O(n) ìœ¼ë¡œ parallel computation ë¶ˆê°€
  - Maximum path length : O(n)
- Self attention
  - Query, key, value ë“¤ì€ n x d matrix
  - ë§Œì•½ dot attentionì´ë¼ë©´ , [nxd] matrixê°€ [dxn]ì™€ ê³±í•´ì ¸, [nxn] matrixë¥¼ ë§Œë“ ë‹¤.
  - Self-attention layer computation cost : O(n^2d)
  - Sequence operation layer computational cost : O(n^2d)
  - Sequence operation : O(1)ëª¨ë“  í† í°ì´ ë™ì‹œì— ì‘ë™í•˜ê¸° ë•Œë¬¸
  - Maximum path length : O(1)
    
<img width="564" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 25 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b975aa5e-2ab4-409b-839b-b6a8054b6e9f">

<img width="605" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 25 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/79e74df1-936c-4259-816d-4cb2f7f7a3d6">

## íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer)
<img width="440" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 26 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c321e9bc-c247-4089-9841-fbe91aa568f7">

RNN ì„ self-attentionë¡œ êµì²´í•œë‹¤!  
auto-regressive í•œ decoder ë„ ê·¸ëŒ€ë¡œ RNNì—ì„œ self-attentionë¡œ êµì²´í•  ìˆ˜ ìˆì„ê¹Œ?  
-> recap : masked attentionë¡œ autoregressive í•œ êµ¬ì¡°ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆë‹¤!  

ì£¼ìš” íŒŒë¼ë¯¸í„° :
Attention  
K : key, Q : query, V : value  
$d_k, d_q, d_p$ : key, query, valueì˜ ì°¨ì›í¬ê¸°  
$d_{ff}$=2048 : Transformerë‚´ feedforward netì˜ ì€ë‹‰ì¸µì˜ ì°¨ì›í¬ê¸°    
$d_{model}$=512 : ì¸ì½”ë” ë””ì½”ë” ì°¨ì›ì˜ í¬ê¸°  
num_layers(N) =6 : transformer layerì˜ ì´ ì¸µì˜ ê°œìˆ˜  
num_headers(h) =8 : Attentionì„ ë³‘ë ¬ë¡œ ë¶„í• í•  ìˆ«ì (ë¶„í• í›„ í•©ì¹˜ëŠ” ë°©ì‹)  
ì¸ì½”ë” self-attention (k,q,v ëª¨ë‘ ê°™ì€ encoderì´ source)  
ë””ì½”ë” masked self-attention (k,q,v ëª¨ë‘ ê°™ì€ decoderì´ source)   
ë””ì½”ë”ì˜ ì¸ì½”ë”-ë””ì½”ë” attention (k,v: encoder, q: decoder)  

## íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer) â€“ ì£¼ìš” êµ¬í˜„ ë””í…Œì¼ - Scaled Dot-product attention / Multi-head attention
ì•ì„œì„œ ë°°ìš´ scaled dot-product attentionì€ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œ!  
Attention(Q,K,V) = softmax($Q K^T \over \sqrt d_k$)V  

Multi-head Attention : ê°ê°ì˜ headì—ì„œ êµ¬í•œ ë…ë¦½ì ì¸ attention representationì„ ì¢…í•©í•œë‹¤ -> ì§‘ì¤‘ì´ í•„ìš”í•œ ë¶€ë¶„ì„ ë¶„ì‚°í•´ì„œ íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. 
- $MultiHead(Q,K,V) = Concat(head_1, ... head_h) W^O $
- $head_i =Attention(Q W_i^Q, K W_i^K, V W_i^V)$

<img width="657" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 41 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/91bbfaff-7411-49c8-9527-9839ef90a667">

## íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer) â€“ ì£¼ìš” êµ¬í˜„ ë””í…Œì¼ - Positional Encoding
RNNê³¼ ë‹¤ë¥´ê²Œ, attentionìì²´ëŠ” ì‹œí€€ìŠ¤ì˜ ìˆœì„œë¥¼ ì¸ì§€í•˜ì§€ ëª»í•œë‹¤!  
ì´ë¥¼ ìœ„í•´ position ì •ë³´ë¥¼ inputê³¼ í•¨ê»˜ ë„£ì–´ì¤˜ì•¼ í•œë‹¤!   
ì—¬ëŸ¬ê°€ì§€ë¥¼ ë„£ì–´ì¤„ ìˆ˜ ìˆê² ì§€ë§Œ, ì› ë…¼ë¬¸ì—ì„œëŠ” ì‚¼ê° í•¨ìˆ˜(sin, cos)ì„ ì´ìš©í•´ì„œ ë„£ì–´ì£¼ì—ˆë‹¤.  
- pos: tokenì˜ ìœ„ì¹˜ index
- i: ë²¡í„°ì˜ ì°¨ì› index

  <img width="264" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 42 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d1e85b36-8d7d-4eb2-8244-a6485ea151f9">

## íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer) â€“ ê²°ê³¼
<img width="733" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 44 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/56706b3b-9c3a-4716-8a28-46b61a736600">

<img width="369" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-03 á„‹á…©á„’á…® 6 46 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d593a318-3c0a-4937-abbb-08c6fc6d1d6b">
- ëª¨ë¸ì´ í¬ë©´ í´ ìˆ˜ë¡ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë‚˜ì•„ì§€ëŠ” ë°©í–¥ ğŸ¡ª ì¶”í›„: GPT / BERT ë“± big transformerë¡œì˜ ê°€ëŠ¥ì„±â€¦ (TBD)

