# 트랜스포머(Transformer)- 1. Transformer

## 트랜스포머 (Transformer)
RNN은 sequence-to-sequence에서 필수적이지 않다?  
오히려, RNN 구조 등은 long-term dependency를 야기할 뿐이다.  
우리가 필요한 것은 “attention” 뿐이다.  -> ‘attention is all you need’  
2017년 Vaswani, Ashish 등 Google 연구팀이 발표.

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/59c69661-6ac1-4cf7-99ad-ec883de16b24)

## Recap: 2013-2014년 RNN기반의 Sequence-to-sequence
인코더-디코더 기반의 기계 번역(seq2seq)은, 입력 문장에서 필요한 모든 정보를 고정된 길이의 vector로 변환하였다.  
그러나, 이는 입력 문장이 길어짐에 따라 퍼포먼스가 심각하게 감소하는 문제를 발생시켰다.  
- LSTM, GRU 등도 완전히 long-term dependency를 해결하지 못하였다.

<img width="303" alt="스크린샷 2023-08-03 오후 5 29 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f7c56e04-adb7-4b5d-bc50-d2bf35881b2b">
<img width="189" alt="스크린샷 2023-08-03 오후 5 30 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/01385f2c-2a6a-4e5b-a72d-48d32c6ea858">

## Recap: 2014-2015년 Attentional RNN
<img width="166" alt="스크린샷 2023-08-03 오후 5 31 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/91993f4c-89dd-4064-95b3-b706827583e2">

Attention구조는 RNN의 long-term dependency 문제를 어느정도 해결해줬다.  
하지만, 완화했을 뿐, 완전히 해결했던 것은 아니다.  
“h1 - h2 와 h2 - h3이 공유되는 구조”는 정보의 효율을 추구한 것이나, 완전하진 않다.  
개선할 여지가 아직 있다!  

<img width="298" alt="스크린샷 2023-08-03 오후 5 32 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/eab3d887-d16a-4655-911d-c06e7897a2fa">

## Recap: Self-attention
Attention이 encoder decoder구조를 가지는 seq2seq에서 정의되었지만, 다른 분야에서도 쓸 수 있다. 
- How?
  - Attention 구조에서 key, value, query을 모두 같은 소스에 계산하는 경우를 "Self-attention" 이라 정의했었다!

  <img width="279" alt="스크린샷 2023-08-03 오후 5 33 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0bec7fdb-d7d5-49ac-b5e4-e9042b18c90b">
  
  - dot-product attention일때 <img width="216" alt="스크린샷 2023-08-03 오후 5 40 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c177bfb4-43b7-4a0a-8efd-a7348cf0b31b">

(self-)attention은?   
<img width="825" alt="스크린샷 2023-08-03 오후 5 42 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c455b418-d303-4b2c-bfec-cef960980dfb">

- 무제한의 receptive field : CNN에 비해 장점
- 데이터 사이즈에 따라 O(1) parameter scaling을 가진다. 데이터가 커져도 괜찮은 편! : MLP에 비해 장점
- RNN등에 비해 parallel computing이 쉽다

## Self-attention VS CNN VS RNN
k : CNN kernel size, n : sequence length , d : input/output channel(size) 라 할때
- CNN
  - Conv layer computational cost : $O(knd^2)$
  - Sequence operation : O(1)
  - Maximum path length : $O(log_k n)$
- RNN
  - RNN cell당 Rnn hidden state 업데이트시 계산 : O(d^2) (d x d matrix연산)
  - Recurrent layer computational cost : O(nd^2)
  - Sequence operation : O(n) 으로 parallel computation 불가
  - Maximum path length : O(n)
- Self attention
  - Query, key, value 들은 n x d matrix
  - 만약 dot attention이라면 , [nxd] matrix가 [dxn]와 곱해져, [nxn] matrix를 만든다.
  - Self-attention layer computation cost : O(n^2d)
  - Sequence operation layer computational cost : O(n^2d)
  - Sequence operation : O(1)모든 토큰이 동시에 작동하기 때문
  - Maximum path length : O(1)
    
<img width="564" alt="스크린샷 2023-08-03 오후 6 25 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b975aa5e-2ab4-409b-839b-b6a8054b6e9f">

<img width="605" alt="스크린샷 2023-08-03 오후 6 25 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/79e74df1-936c-4259-816d-4cb2f7f7a3d6">

## 트랜스포머 (Transformer)
<img width="440" alt="스크린샷 2023-08-03 오후 6 26 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c321e9bc-c247-4089-9841-fbe91aa568f7">

RNN 을 self-attention로 교체한다!  
auto-regressive 한 decoder 도 그대로 RNN에서 self-attention로 교체할 수 있을까?  
-> recap : masked attention로 autoregressive 한 구조를 재현할 수 있다!  

주요 파라미터 :
Attention  
K : key, Q : query, V : value  
$d_k, d_q, d_p$ : key, query, value의 차원크기  
$d_{ff}$=2048 : Transformer내 feedforward net의 은닉층의 차원크기    
$d_{model}$=512 : 인코더 디코더 차원의 크기  
num_layers(N) =6 : transformer layer의 총 층의 개수  
num_headers(h) =8 : Attention을 병렬로 분할할 숫자 (분할후 합치는 방식)  
인코더 self-attention (k,q,v 모두 같은 encoder이 source)  
디코더 masked self-attention (k,q,v 모두 같은 decoder이 source)   
디코더의 인코더-디코더 attention (k,v: encoder, q: decoder)  

## 트랜스포머 (Transformer) – 주요 구현 디테일 - Scaled Dot-product attention / Multi-head attention
앞서서 배운 scaled dot-product attention은 본 논문에서 제시!  
Attention(Q,K,V) = softmax($Q K^T \over \sqrt d_k$)V  

Multi-head Attention : 각각의 head에서 구한 독립적인 attention representation을 종합한다 -> 집중이 필요한 부분을 분산해서 파악할 수 있게 하여 모델 성능을 향상시킨다. 
- $MultiHead(Q,K,V) = Concat(head_1, ... head_h) W^O $
- $head_i =Attention(Q W_i^Q, K W_i^K, V W_i^V)$

<img width="657" alt="스크린샷 2023-08-03 오후 6 41 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/91bbfaff-7411-49c8-9527-9839ef90a667">

## 트랜스포머 (Transformer) – 주요 구현 디테일 - Positional Encoding
RNN과 다르게, attention자체는 시퀀스의 순서를 인지하지 못한다!  
이를 위해 position 정보를 input과 함께 넣어줘야 한다!   
여러가지를 넣어줄 수 있겠지만, 원 논문에서는 삼각 함수(sin, cos)을 이용해서 넣어주었다.  
- pos: token의 위치 index
- i: 벡터의 차원 index

  <img width="264" alt="스크린샷 2023-08-03 오후 6 42 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d1e85b36-8d7d-4eb2-8244-a6485ea151f9">

## 트랜스포머 (Transformer) – 결과
<img width="733" alt="스크린샷 2023-08-03 오후 6 44 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/56706b3b-9c3a-4716-8a28-46b61a736600">

<img width="369" alt="스크린샷 2023-08-03 오후 6 46 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d593a318-3c0a-4937-abbb-08c6fc6d1d6b">
- 모델이 크면 클 수록 전반적으로 성능이 나아지는 방향 🡪 추후: GPT / BERT 등 big transformer로의 가능성… (TBD)

