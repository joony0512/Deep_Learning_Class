# Deep generative models #1 - Autoregressive vs autoencoder vs embedding vs seq2seq - 4. Embedding

## Embedding
ML에서의 embedding이란?
- “An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors”. (https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?hl=ko)
- 다른 말로 embedding 은, discrete 한 고차원 벡터를 continuous 한 저차원 벡터로  매핑(mapping) 할 수 있는 표현(Representation)이라 할 수 있다.
  - 이는 차원 축소(Dimension reduction) 등으로 구할 수 있다.
    
  <img width="606" alt="스크린샷 2023-07-27 오후 10 16 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c66c8008-ecc1-40b4-90cf-3de2a8c188f5">

- 크게 3가지 용도:
1. 가장 가까운 이웃 정보를 찾도록 해준다. 
   - 유저의 관심사나 클러스터 카테고리에 대해서 추천을 하도록 도와준다.
2. 머신 러닝의 피처로서  learning을 위한 입력 값을 임베딩하여 사용할 수 있다.
3. 카테고리 간 개념과 관련 정도를 시각화 해줄 수 있다.

## Embedding의 종류: Categorical value - One-hot encoding  | Label encoding
Label encoding

- 고유번호를 integer로 저장하는 방법 예: [4, 0, 3, …]

- Pros: 간단하고, one-hot에 비해 변환이 빠르다. 멱등성(idempotent)이 보장된다.
- Cons: 각 카테고리의 관계를 파악하기 어렵다.
  - 예) 9-10의 관계는 5, 10의 관계보다 가까울 것이라 기대되지만 본 embedding에서는 그렇지 아니한다.
  - Loss 식 설계 등에서 고려할 점이 늘어난다.

One-hot encoding
- 각 카테고리를 고유한 번호로 인코딩하고, 번호와 동일한 index 에서는 1, 나머지에서는 0을 값으로 하는 벡터로 변환한다.
  <img width="349" alt="스크린샷 2023-07-27 오후 10 27 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/23ed6123-45f8-4716-af0b-3ff507f32bc7">
- Pros: 간단하고, 각 카테고리가 독립적일 때 효과적이다.
- Cons:
	- 매우 sparse하다. (대부분 메모리의 값이 0이다)
	- 메모리 적으로 매우 비효율적이다.
	- 카테고리 간 관계를 고려하지 않는다.

## Embedding의 종류: Categorical value - Lookup Table 기반 embedding (torch.nn.Embedding, tf.keras.layers.Embedding)
<img width="675" alt="스크린샷 2023-07-27 오후 10 28 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3271ff0e-60c2-4fef-8cdd-841301d82a7f">
<img width="375" alt="스크린샷 2023-07-27 오후 10 28 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c8d0452d-c33a-4dfa-b0da-cfdcf0a562ea">

- Integer 카테고리에 1대1로 대응하는 embedding vector을 Table 에 모아둔다.
- Integer을 index로 Table 을 Lookup 하여 embedding vector을 반환한다.
- Lookup table 은 '학습'이 가능하며, 1대1 대응이기에 다른 임베딩에 비해 parameter 가 단순한 편이다.

## Embedding의 종류: 자연어 처리를 위한 embedding - Bag of words 기반 
Bag of words: 문장에 많이 쓰인 단어는?
- 주제가 비슷한 문서는 등장하는 단어도 비슷할 것. 
- 즉, 단어 순서와 상관없이 많이 등장하는 단어는 그 주제와 관련이 있을 것!
	- 이러한 전제 하에 단어의 빈도수 자체를 representation으로 사용하는 방식이 bag of words embedding이다.
- Bag : 순서를 고려하지 않고 중복되는 경우를 허용하는 집합
  - sparse vector, static word embedding 기반 중, sparse vector로 구분된다!
    
## Embedding의 종류: 자연어 처리를 위한 embedding - Bag of words 기반: TF-IDF
TF-IDF(Term Frequency - Inverse Document Frequency)
- TF와 IDF을 곱한값
- 점수가 높은 단어일수록 다른 문서에는 많지 않고 해당문서에서 자주 등장하는 단어를 의미
- 장점 : 어떤 단어가 중요한 단어인지 직관적인 해석이 가능
- 단점 : 문맥에 대한 고려를 해주지 않음 (단어의 의미가 포함되지 않음)

TF(단어빈도, term frequency)
- 특정한 단어가 문서내에 얼마나 자주 등장하는지를 나타내는 값 : 이 값이 높을수록 문서에서 중요하다고 생각할 수 있다.
- 하지만 하나의 문서에서 많이 나오지 않고 다른 문서에서 자주 등장하면 단어의 중요도는 낮아진다.
DF(문서빈도, document frequency)
- 특정단어가 등장하는 문서개수
- DF의 역수는 IDF라고 한다.

## Embedding의 종류: 자연어 처리를 위한 embedding - Bag of words 기반: 잠재 의미 분석 (Latent Semantic Analysis; LSA)
### 잠재 의미 분석 ( Latent Sementic Analysis : LSA )
기존 One-hot encoding이나 TF-IDF의 "단어의 의미를 고려하지 못하는 단점"을 해결한 방법
  - TF-IDF행렬에 Truncated Singular Value Decomposition(svd)를 적용
  - SVD의 Singular Value($\Sigma$)는 $A A^T$의 고유값에 제곱근을 취한 결과
  - Truncated SVD는 $\Sigma$행렬의 대각 원소 가운데 상위 t개만 골라낸 형태
  - $A = U \Sigma V^T$

- 장점 : 단어의 잠재적인 의미를 고려
- 단점 : 새로운 정보에 대한 업데이트가 어렵고 단어-문서간의 similarity를 계산하기 어려움(차원이 축소되어서 단어간의 의미 파악이 힘듦)
  <img width="497" alt="스크린샷 2023-07-27 오후 10 53 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ee83e8df-f62e-427f-a8f9-626c4c9487fd">

## Embedding의 종류: 자연어 처리를 위한 embedding - Bag of words 기반: PMI(pointwise mutual information)
Pointwise mutual information(PMI)
- 정보이론의 PMI를 응용하여, 각 word간 semantic representation을 알아낼 수있다.
- TF-IDF와 같이 sparse vector representation방법중 하나
- $PMI(a, b) = log(P(a,b) / P(a) P(b))$
  - 값이 클수록 correlated(함께 많이 등장)되었다는 의미
 
## Embedding의 종류: 자연어 처리를 위한 embedding - 신경망 이용: Word2Vec
<img width="496" alt="스크린샷 2023-07-27 오후 11 03 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2a4ac45e-0428-4c25-8592-74d6c7de2cf4">

- 단어의 의미를 고려하지 못하는 단점을 해결!
- 중심 단어와 주변 단어 간의 관계를 통해서, ‘학습 가능한 모델로’ embedding 하는 방법
  - 학습 하기 때문에, dynamic하게 그 embedding 값이 변할 수 있다!
    
<img width="474" alt="스크린샷 2023-07-27 오후 11 01 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7cc76577-6d13-4979-86f2-1a722a91a511">

- CBOW(continuous bag of words): 주변에 있는 문맥 단어들을 가지고, 중간에 있는 기준 단어를 예측하는 방법
- Skip-Gram : 중심의 기준 단어를 보고 어떤 문맥 단어가 등장할 지 예측하는 방법

- 장점: 단어들의 similarity을 계산하기에 embedding 된 벡터 자체가 단어의 의미를 포함.
  - 적은 차원으로 대상을 표현할 수 있고 일반화 능력을 갖추고 있다.
  - 온라인 방식으로 모델에 데이터가 공급될 수 있고 전 처리가 거의 필요하지 않음 
    - 메모리가 거의 필요하지 않다!
- 단점: 사용자가 지정한 윈도우(주변 단어 몇 개만 볼 지) 내에서만 학습/분석이 이뤄지기 때문에 말뭉치 전체의 공기 정보(co-occurrence)는 반영되기 어려움.
  - 데이터가 충분히 많아야 학습이 잘 된다.
  - 카테고리 수가 너무 많으면(어휘가 많으면) softmax 함수를 사용하여 모델을 학습하기가 매우 어려움
    - 이를 해결하기 위해 negative sampling 같은 방법이 도입

## Embedding의 종류: 자연어 처리를 위한 embedding - 신경망 이용: GloVe (global vectors for word representation)
LSA, Word2Vec의 문제점을 개선한 모델로 “임베딩 된 단어 벡터 간 유사도 측정을 수월하게 하면서도 전체 도큐먼트의 통계 정보를 좀 더 잘 반영”
- LSA: LSA는 전체 도큐먼트 전체의 통계적인 정보를 모두 활용하지만, 단어/문서 간 유사도를 측정하기는 어려운 단점
- Word2Vec: 저 차원 벡터 공간에 임베딩 된 단어 벡터 사이의 유사도를 측정하는 데는 LSA에 비해 좋은 성능을 가지지만, 사용자가 지정한 윈도우(주변 단어 몇 개만 볼지) 내에서만 학습/분석이 이뤄지기 때문에 도큐먼트 전체의 공기 정보(co-occurrence)는 반영되기 어려운 단점
- GloVe: LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 두 가지를 모두 사용
이해하기 위해서는 먼저
- 윈도우 기반 동시 등장 행렬 (window based co-occurrence matrix)
- 동시 등장 확률 (co-occurrence probability)
을 이해해야한다.

윈도우기반 동시등장행렬 (window based co-occurrence matrix)
- 추후 X로 표시
- $X_{ij}$: 중심단어 i가 등장했을때, 윈도우내 주변단어 j가 등장하는 횟수
- $X_i = \Sigma X_{ij}$ : 동시등장행렬에서 i행의 값 모두 더한 값.
- 예 ) I like deep learning, I enjoy embedding, I like fastcampus
  
<img width="641" alt="스크린샷 2023-07-27 오후 11 08 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/215b839e-1d1c-418e-a7aa-a81c2a0671ea">

동시 등장 확률 (co-occurrence probability)
- $P_{ik} =P(k|i)$ -> 동시등장행렬로 부터 특정단어 i의 전체 등장 횟수를 카운트 하고 , 특정단어 i가 등장했을때 어떤 단어 k가 등장한 횟수를 카운트 하여 계산한 조건부 확률
- $P(k|i)$에서 i을 중심단어(center word), k을 주변단어(context word)라 했을때, co-occurrence matrix에서 중심단어 i행의 모든값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값이다.

  <img width="484" alt="스크린샷 2023-07-27 오후 11 13 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9ecba4ee-c052-4e7d-bdd5-61c4d03bf3df">

- Ice가 등장했을때 solid가 등장할 확률은 , stream이 등장했을때 solid가 등장할 확률보다 8.9배 크다
  - 이를 활용한 손실함수는?
 
- 손실함수
  - Embedding된 중심단어($w_i$)와 주변단어벡터($\tilde w_k$)의 내적(dot product)이 전체 corpus에서의 동시등장확률의 log값이 되도록 만드는것
  - 이를 만족하도록 embedding vector을 만드는 것이 목표
  - $dot product(w_i,\tilde w_k) =w_i^T \tilde w_k \approx log P(k|i) = log P_{ik}$의 꼴
  - $X_{ij}$(window based co-occurrence matrix): 중심단어 i가 등장했을때, 윈도우내 주변단어 j가 등장하는 횟수
    
    <img width="292" alt="스크린샷 2023-07-27 오후 11 20 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4c38ad0a-64ad-4d51-8f9a-8dc156ab6446">
    <img width="367" alt="스크린샷 2023-07-27 오후 11 20 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cab4e874-7355-4cc5-af9d-dc832e1bb3cd">

<img width="443" alt="스크린샷 2023-07-27 오후 11 21 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1069a149-6aee-43ec-ae16-2ed25ece1e1d">
<img width="401" alt="스크린샷 2023-07-27 오후 11 21 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0f96db08-a11e-4df1-a678-0d6412be39ec">

## Embedding의 종류: 자연어 처리를 위한 embedding - 신경망 + subwords tokenizing 이용: FastText
- Fasttext와 Word2vec의 가장 큰 차이점은 부분 단어(Sub words)를 사용한다는 것
- Word2vec의 경우, 어휘가 없는 단어를 볼 경우 아무것도 모르지만, fast Text는 어느정도 모방을 함. 즉, 동시 등장 정보를 보존
- N-grams에서도 유연하다! 

잠깐: N-gram은?
  - n개의 연속적인 단어나 character 나열을 의미!
  - Character 3 gram이라면, start char(<), end char(>) 포함하여,
  - "hello" => "<he", "hel", "ell", "llo", "lo>"
	
- 결과적으로, 이러한 tokenizing 기법으로 word2vec 등 에서 알려진 out of vocabulary(OOV) 문제를 해결한다 !
- 즉, 학습 때 포함되지 않은 단어들에 대해서도, subword의 vector 값을 이용하여 의미적으로 유사한 단어끼리 공간상에 근접한 위치 값을 가지게 한다.
 - 예를 들면, “<youn”은 adole, adoles, doles 과 높은 similarity를 가진다! 
 - 그러나, 여전히 supervised learning으로 많은 양의 학습데이터가 필요하다.

<img width="287" alt="스크린샷 2023-07-27 오후 11 23 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/54476d80-a4f4-489b-a75f-07e9c3ffdc19">
<img width="490" alt="스크린샷 2023-07-27 오후 11 24 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d515743e-80a0-438e-92e7-3f39490a40f3">

## Embedding의 종류: 자연어 처리를 위한 embedding - ELMo (Embedding for Language Model)
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/51660d61-9614-4b3a-83d3-8cfeff7c326f)

- 기존의 word2vec, GloVe 등은 동음이의어 등을 반영하지 못한다는 문제가 있다!
  - 예) bear: 견디다 / 지탱하다 / 곰
  - People seem to bear this unnecessary noise vs My favorite animal is a bear.  

- Language model(언어 모델)을 이용해 문장에서 각 단어의 문맥을 학습하게 하자!
- ELMo는 RNN에 기반한 embedding
  - 가장 큰 특징으로, 사전 훈련된 언어 모델(pre-trained language model)을 사용한다 !

- biLM (bi-directional language model)을 이용하는데, char CNN 방법 등을 이용해 character 단위로 계산하여 최초의 embedding을 먼저 한다.
	- fast text과 같이 subword 정보를 받으므로, OOV에도 견고하다!

- 그 이후 LSTM 기반의 LM을 각각 forward, backward로 pretraining하고, finetune한다.
- 최종 값을 구하기 위해 아래 과정을 거친다.
1. 각 층의 출력 값을 concatenate한다.
2. 각 층의 출력 값 별로 가중치를 주고 더한다 (weighted sum).
3. 벡터의 크기를 결정하는 스칼라 매개변수를 곱하여 최종적인 representation을 구한다.

## Embedding의 종류: 자연어 처리를 위한 embedding - Structured Self-attentive Sentence Embedding
- Bi- directional RNN으로 fixed length hidden representation로 먼저 embedding하고 그 이후 self attention을 취한다.
- H 를 RNN으로 부터 나온 정보라 하면, 2개의 MLP(linear, W_s1, W_s2)와 결합하여, attention은 다음과 같이 계산될 수 있다.
- $A = softmax(W_{s2} tanh(W_{s1}H^T$
- 추가적으로 attention weight가 학습되지 못하고 어떤 문장이든 비슷한 softmax값 출력하는 것을 방지하기 위해 penalty term(regularization)을 제시.
  - $P = ||AA^T - I ||^2_F$
- Attnetion의 해석가능한  attention weight을 이용하여 사후 분석을 용이하게 함

## Embedding의 종류: 자연어 처리를 위한 embedding - Next?  Transformer based… (TBD)
<img width="556" alt="스크린샷 2023-07-27 오후 11 36 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c29f5360-2dd6-45c1-9b29-a95fc7187e97">
<img width="206" alt="스크린샷 2023-07-27 오후 11 36 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/31ce4a99-9707-40d0-9977-d1dee685d64d">


## Embedding의 종류: 오디오 처리를 위한 embedding
From spectrogram : Spectrogram 형태로 바꾼 후, 이미지를 처리하는 방식 등 (1D CNN, 2D CNN, tSNE, auto-encoder…)으로 Embedding을 진행!

<img width="473" alt="스크린샷 2023-07-27 오후 11 37 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c4c4a0ea-df70-421c-8366-78ca959df81c">

From wave: - Wave2vec

<img width="619" alt="스크린샷 2023-07-27 오후 11 38 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5434e34c-9840-4983-8885-20a93c5c2781">
<img width="576" alt="스크린샷 2023-07-27 오후 11 38 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9d3b6a0c-9d3c-40c2-945c-b67eda258619">
