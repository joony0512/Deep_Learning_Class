## CH04_07_Normalization

## ****정규화 (Normalization) VS 정형화 (Regularization)****

- Normalization : 각 특성의 scale을 조정
    
    → Min-Max Scale : ${ x- x_{min}}\over x_{max} -x_{min}$
    
- Regularization : 모델의 설명도를 유지하면서 모델의 복잡도를 줄이는 방식(ch3참고)

## 배치정규화(Batch normalization; BN)

- 한편, mini-batch로 학습하는 ML문제에서 weight나 layer의 결과를 정규화 (normalization)함으로써 학습과 최적화를 안정화 시킬 수 있다.
    - **이를 통해 내부 공변량 변화(internal covariate shift; ICS) 문제를 해결할 수 있다.**
- 실제 DNN 모델을 학습할 때, 다른 레이어는 고정된 상태라 가정하지만, 실제로는 distribution을 지속적으로 변화시키며 학습되고 있다.
- **공변량 변화(covariate shift)문제**란? : 이전 레이어의 파라미터 변화로 인하여 현재 레이어의 입력의 분포가 바뀌는 현상
- **내부 공변량 변화(Internal covariate shift)문제**란? : 레이어를 통과할 때, covariate shift가 일어나면서 입력의 분포가 약간씩 변하는 현상
- **모델을 학습하기 어렵고, learning rate 등 hyper parameter을 잘 튜닝해야하는 요인이 된다!**
    - **배치 정규화로 해결!**

<img width="1031" alt="스크린샷 2023-06-27 오전 2 03 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b8aacfb0-7c18-498b-8c3e-b0424f28f3ac">


1. mini-batch의 평균
2. Mini-batch의 분산과 표준 편차
3. Normalization (epsilon 값을 제외하면 정규 표준화)
4. 스케일 조정 및 분포 조정
- 원 논문(S. Ioffe and C. Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ICML 2015)에서는, 최적화시, Parameter의 scale 영향을 받지 않으므로, learning rate을 크게 설정할 수 있고 weight regularization과 dropout을 제외할 수 있어, 학습 속도를 향상시킬 수 있다고 주장했다.
- 그리고 결과적으로, Internal covariance shift (ICS)를 완화할 수 있다고 했으나,
- 추후 다른 논문(Santurkar, Shibani, et al. "How does batch normalization help optimization?." *Proceedings of the 32nd international conference on neural information processing systems*. 2018.)에서, **잘 되었던 이유가 ICS와 관계없다**고 주장되었다.!
- 오히려 종종 증가시킨다는 보고!(그러나 그 와중에 성능은 좋다)
    
    <img width="735" alt="스크린샷 2023-06-27 오전 2 07 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1c0fb646-83b8-431e-807d-5947cb5195e6">

    

## **배치 정규화는 왜 잘될까?**

- 신경망에서 가중치의 변경은 그와 연결된 layer에 영향을 미친다.
- DNN의 목적함수를 최적화 하는 것은 복잡하기 때문에 gradient의 vanishing 또는 exploding을 막기 위해, 보통 small learning rate을 사용하거나, 활성화 함수 사용을 선택해왔다.
- Batch Normalization는 최적화를 하기 쉽게 만들기 위해 네트워크를 다시 매개 변수(reparametrized)화 시키는 역할을 했고, 이는
    - 안정적이게, Smoothing효과를 만들었다! (loss변화와 gradient 크기를 적게)
    - 모든 레이어의 평균, 크기, 활성화 함수를 독립적으로 조정할 수 있게 만들었다!
    - 학습이 빠르게!
- 즉, Normalization의 조정들이 gradient가 모델의 업데이트에 끼치는 영향을 조절!
    
    <img width="348" alt="스크린샷 2023-06-27 오전 2 10 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7cdbed02-9e0a-464e-8402-7110679e66dd">

    
    - batch normalization이 중요한 게 아닌, 다른 ICS를 오히려 높이는 normalization 역시 성능을 높인다 !
        
        
        <img width="606" alt="스크린샷 2023-06-27 오전 2 12 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d9b19c53-152e-438b-91dc-a6f966359db4">

    - 즉, Smoothing 효과가 더 크다!

## 정규화(Normalization)의 종류

<img width="541" alt="스크린샷 2023-06-27 오전 2 17 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cec5ceee-679c-41f1-814c-b786021e1c9b">


- 한편 정규화는 Batch에만 국한되지 않고, Layer, channel 등으로 확장될 수 있다.


## Layer normalization
<img width="541" alt="스크린샷 2023-06-27 오전 2 18 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bcddae42-63e2-40d1-b071-3dda514db39c">




- Batch 단위로 적용되는 것이 아닌, layer 에서 적용된다.
    - Batch size와 관계없이 효과를 볼 수 있다!
        
        
        <img width="178" alt="스크린샷 2023-06-27 오전 2 21 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9b8652b2-f55a-469b-af92-596cd792def4">

- 학습 속도를 빠르게 해준다!
    - BERT 등 대중적인 모델에서 사용되는 등 대중화!

## Group normalization

<img width="437" alt="스크린샷 2023-06-27 오전 2 24 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0ae32df8-c848-45a1-9b23-72f8a7db2fa6">


<img width="234" alt="스크린샷 2023-06-27 오전 2 24 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7df2c229-b919-4ff7-8659-7b07d33a3562">


- Layer normalization과 instance normalization의 절충.
- Layer norm과 같이 batch size에 관계 없이 적용할 수 있다.
- Layer norm이 뛰어난 성능이 보이지만, visual recognition 등에서는 성능을 보장하지 않는다는 단점이 있고, 이를 보완하기 위함
- 영상처리 분야에서 grouping 하는 것에 영감을 얻어 연구됨!

### ****Group normalization 결과****

<img width="410" alt="스크린샷 2023-06-27 오전 2 26 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f8193080-0a00-4b1e-9a39-a7ace4c7fe61">
<img width="197" alt="스크린샷 2023-06-27 오후 8 27 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/33a0db28-6740-48d6-b303-eb4261ea09de">


