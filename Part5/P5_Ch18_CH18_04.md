# Research Topics for Productions - 4. eXplainalble AI (XAI)
## eXplainable AI (XAI) - 기존 AI vs XAI
<img width="400" alt="스크린샷 2023-09-07 오후 5 53 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/95037414-1ebb-41e3-bf65-0051c94e5e16">

- XAI 이전의 딥러닝(2012-2015)
  - 사람과 비슷하거나 더 나은 성능 (얼굴 인식, 자연 언어 처리 등)
  - 자동 번역, 자율 주행 등 다양한 산업에 응용

- 그러나, "생명"이 관련된 영역에는 사용이 제한 되었음 (의료, 국방, 금융, 보험, 교육 등)
- 왜?
  - 불확실하거나 불명확한 결과 -> "딥러닝은 블랙박스다"
  - AI 연구자 및 유저가 딥러닝 모델의 결과를 예측할 수 없음 -> production 차원에서 사용 시 결과를 신뢰하기가 어려움
  - "설명가능한 AI"로 AI의 설명 불가능한 점을 보완하자!

<img width="400" alt="스크린샷 2023-09-07 오후 5 56 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/abf6d98d-b710-4508-8329-5d88922e1d27">

## eXplainable AI (XAI) - XAI가 왜 필요할까? (HCI 적인 관점)
<img width="221" alt="스크린샷 2023-09-07 오후 5 57 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/67425545-3150-4740-8b02-5f164de9db5b">

- 모델의 정확도가 떨어지더라도, 설명가능한 모델이 더 좋을 수 도 있다.

- 설명은 비교를 가능하게 한다.
  - "A 가 일어난 이유가 무엇인가" 보다 "B가 아니라 A가 일어난 이유가 무엇인가"가 더 많은 관심을 끄는 주제이다.

- 설명은 선택할 수 있다.
  - 사람들은 아주 정확한 설명보다는 적당히 왜곡 되었어도 이해하기 쉬운 설명을 원한다. 인지 편향에 따라 원하는 설명을 취사 선택하기도 한다.

- 설명은 사회 활동이다.
  - 설명은 의사소통과 상호작용의 과정이므로, 듣는 사람이 처한 환경에 따라서 설명은 달라질 수 있다.

- + 확률은 중요한 문제가 아닐 수 있다.
 
## eXplainable AI (XAI) - Explainer의 이상적인 세팅?
- Interpretable
  - 입력과 결과에 대한 질 좋은 설명이 제공되어야 한다.
  - 사용자의 수준에 맞게 설명이 제공되어야 하며, 비전문가 또한 예측에 대해 쉽게 이해할 수 있어야 한다.
- Local fidelity
  - 예측에 대한 설명은 적어도 지역적으로(locally) 신뢰(faithful)되어야 한다. 즉, 모델이 하나의 데이터에 내린 판단에 대해 설명할 수 있어야 한다.
- Model-agnostic
  - 어떠한 모델을 사용하든 설명 가능해야 한다.
- Global-perspective
  - 정확도만으로는 모델을 정확하게 평가할 수 없기 때문에, 어느 정도 전체적인 모델에 대한 설명 또한 제공되어야 한다.

## eXplainable AI (XAI) - XAI의 카테고리
<img width="500" alt="스크린샷 2023-09-07 오후 6 00 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/47ce7a46-ff7b-4c1f-8f4b-418f2986f6ab">

- Support 가능한 모델의 종류에 따라
  1. BB (Black-Box): 입력과 출력 간의 관계만을 설명할 수 있다. 내부 파라미터나 모델의 동작 메커니즘에 대한 정보가 없다. 단순히 모델이 주어진 입력에 대해 어떤 예측을 하는지를 설명하는 것으로 제한된다.
  2. BB* (경사 하강 가능한 Black Box): BB와 비슷하지만 이 모델은 일부 내부 파라미터를 조정하기 위한 그래디언트 정보를 사용할 수 있다. 따라서 모델의 일부 파라미터를 수정하거나 조정하여 예측을 개선할 수 있다. 그러나 모델의 전체 내부 구조나 파라미터를 완벽하게 이해하거나 수정할 수는 없다.
  3. WB (White-Box): 가장 높은 수준의 접근을 제공한다. 모델의 내부 구조, 파라미터, 동작 메커니즘 등을 완전히 이해하고 수정할 수 있다. 이 모델을 사용하면 모델을 완전히 재구성하거나 새로운 작업에 맞게 수정할 수 있으며, 모델 내부의 모든 정보에 액세스할 수 있다. 많은 제약이 따를 수 있는 방법.

- 설명을 나타내는 범위에 따라
  - Local "X를 넣을때 왜 이런 예측이 나왔지?" 같은 특정 데이터에 국한된 설명
  - Global - 특정한 예측에 귀속되지 않는 모델 자체에 대한 전체적인 설명

## eXplainable AI (XAI) - Post-hoc 접근의 예시
<img width="500" alt="스크린샷 2023-09-07 오후 6 06 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/86be14bf-b88f-4145-b334-b1843dd44410">

## eXplainable AI (XAI) - XAI의 카테고리 ‒ 직군에 따라
<img width="500" alt="스크린샷 2023-09-07 오후 6 07 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/93f2255e-9bee-4cf7-a216-c187dd5029c7">

## eXplainable AI (XAI) - 기존의 방법은?
<img width="500" alt="스크린샷 2023-09-07 오후 6 07 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c7385d57-1cb0-4eda-a48c-8a9e22046cb2">
<img width="500" alt="스크린샷 2023-09-07 오후 6 08 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b0f0007b-42b0-4bd9-bc47-e127713f35e0">

## eXplainable AI (XAI) - Modern XAI의 예시
<img width="400" alt="스크린샷 2023-09-07 오후 6 09 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f6a8afd0-8c66-48d7-8cba-cf80b486e10c">
<img width="400" alt="스크린샷 2023-09-07 오후 6 09 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ff34f7b8-27c0-456d-9e80-239fc2a83dd7">
<img width="400" alt="스크린샷 2023-09-07 오후 6 09 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f6408fe8-8022-4177-ba2c-520c48aa6e2d">
<img width="400" alt="스크린샷 2023-09-07 오후 6 09 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/64426877-7e9e-4562-b1dd-83fec7e7f044">

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ LIME (Local Interpretable Model-Agnostic Explanation; 2016)
- LIME의 전반적인 목표는 기존의 classifier에 대해 지역적으로 신뢰(locally faithful)를 보이는 해석 가능한 표현(interpretable representation)들을 통해 해석 가능한 모델을 식별하는 것 -> 원래 머신러닝 모델 f를 해석 가능한 모델 g로 근사화하여 모델의 동작을 설명하는 것.
  - 𝑥 ∈ $𝑅^d$ : 설명되는 데이터,
  - zʼ: 설명되는 모델의 input
    - 근접성 측정: z'는 설명하려는 데이터 포인트 x와의 근접성을 측정하기 위해 사용된다.
      - LIME은 데이터 포인트 z'와 x 사이의 유사성을 측정하여 모델을 해석하는 데 사용한다.
      - 즉, z'와 x가 얼마나 비슷한지를 파악하여 설명 가능한 모델이 x의 동작을 잘 설명할 수 있도록 한다.
    - 모델의 동작 이해: z'를 사용하면 설명 가능한 모델 g를 학습할 때 어떤 특정 방향으로 데이터를 변화시켜야 하는지를 이해할 수 있다.
      - 이를 통해 특정 특징이 모델의 예측에 미치는 영향을 파악하고 해당 특징이 예측에 어떻게 기여하는지를 설명 가능하게 만들 수 있다.
    - 설명 가능한 모델 학습: LIME에서는 z'를 사용하여 해석 가능한 모델 g를 학습한다.
        - z'를 입력으로 사용하여 모델 f의 예측을 설명 가능한 모델 g의 예측과 일치시키려고 시도한다.
        - 이러한 과정을 통해 g가 f를 근사화하고 x에 대한 설명을 생성한다.
    - 즉, z'는 LIME의 핵심 개념 중 하나로, 설명 가능한 모델을 학습하고 모델의 동작을 해석하기 위한 기준점으로 사용된다.
      - z'는 설명 가능한 모델이 x와 얼마나 유사한 데이터를 사용하여 학습되어야 하는지를 결정하고 설명의 정확성을 향상시키는 데 중요한 역할을 한다.
        
  - 𝑥′ ∈ {0, 1} : 해석 가능한 표현을 위한 binary vector
  - 𝑓(𝑥) : 평가하려고 하는 모델 f가 x를 넣었을때 원하는 label에 대한 확률값
  - G: 해석 가능한 모델들의 class (i.e. linear model; decision tree)
  - 𝑔 ∈ 𝐺: 사용자에게 visual of context를 제공하는 모델
  - Ω(𝑔): 복잡도 측정
  - $𝜋_x (𝑧)$ : z와 x사이의 proximity measure

이때, explainability는 다음과 같이 표현될 수 있다 

<img width="350" alt="스크린샷 2023-09-07 오후 6 12 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/90b8bc49-8a11-4df9-9b7e-909240f766f4">

Interpretability와 local fidelity를 얻기 위해, locality-aware loss ( $𝐿(𝑓,𝑔,𝜋_x)$ )를 줄이고, g의 복잡성을 줄여야한다. (f의 가정이 없다면 model-agnostic 해질것이다.)
- 𝑧' ∈ $\\{ 0, 1 \\}^d$ 의 perturbed 샘플이 있을 때 (기존 데이터 포인트 x의 변형된 버전 z'), original representation인 z을 복원해 𝑓(𝑧)을 얻는다.
  - 변형된 데이터 z'로부터 원래 데이터 포인트 x를 복원한다. 이는 설명 가능한 모델 g가 실제로 어떤 데이터에 대한 설명을 만들어야 하는지를 결정하는 데 사용
  - 원래 데이터 포인트 x를 복원한 후, 이를 설명 가능한 모델 g에 입력으로 제공하여 해당 데이터 포인트에 대한 모델 g의 예측 레이블인 f(z)를 얻음.
  - 이 레이블은 설명 가능한 모델 g를 위한 레이블로 사용.
- 𝑓(𝑧)는 설명 모델 g를 위한 label로 쓰이면, LIME은 최종적으로 원래 모델 f(x)와 설명 가능한 모델 g의 예측 f(z) 사이의 손실 L을 최소화하여 근사.
- 논문에서 sparse linear explanation을 아래와 같이 정의 (G를 linear라 가정)!

  <img width="350" alt="스크린샷 2023-09-07 오후 6 36 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5bcf4a5f-2a1c-436a-b18b-7043ab86a07e">

  - 인 width가 𝜎인 exponential kernel을 사용하고, D는 특정한 거리 함수 (distance function)을 사용

    <img width="350" alt="스크린샷 2023-09-07 오후 6 36 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/964ac995-2e52-4171-85de-18d4ed810bec">

- Dataset을 Z라고하면, 우리의목표는 𝜉(𝑥) 를 최적화 하는것.
  - 그러나, Ω(g)는 직접 계산하지 못하므로, K-feature LASSO라는 알고리즘으로 근사하여 계산한다.~ 이미지 당 10분.

    <img width="400" alt="스크린샷 2023-09-07 오후 6 38 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/269d6754-f7e1-4263-acad-5722a2559468">

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ LIME (Local Interpretable Model-Agnostic Explanation; 2016)
어떤 방식으로 해석 가능한 표현과 해석 모델 G를 고르든, 단점은 분명히 존재할 것!
  - 기존의 분류 모델이 블랙박스 모델일지라도 특정한 해석 가능한 표현은 어떤 행동을 설명할 정도로 강하지 못할 수도 있다.
  - G로 선택한 Sparse linear model은, 만약 기존의 분류 모델이 지역적으로도 ʻ매우 비선형ʼ적이라면 지역적인 선형을 가정한 linear model로는 신뢰도 높은 설명을 할 수 없을 것!

하지만, LIME 모델은 예측을 잘 설명하는 최고의 간섭 샘플만 사용할 수 있는 게 아니라, 특정 데이터 x 근처의 다양한 샘플들(z′∈Z)에 대해서도 설명을 제공할 수 있다.  
그렇기 때문에 적절한 설명 집합(Z)을 선택할 수 있고, 주어진 dataset과 classifier에도 맞출 수 있다.

<img width="350" alt="스크린샷 2023-09-07 오후 6 39 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/35b897cd-7978-4240-b74c-0c32b834999e">

<img width="400" alt="스크린샷 2023-09-07 오후 6 39 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/17264afd-1440-4624-acfb-79b259cb3c5c">

<img width="350" alt="스크린샷 2023-09-07 오후 6 40 11" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/977689cc-9024-4884-ac0f-e68a27b32fd5">

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ LIME (Local Interpretable Model-Agnostic Explanation; 2016)
<img width="500" alt="스크린샷 2023-09-07 오후 6 50 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c31de62f-7a14-470e-8a57-e7da6f8c59a1">

<img width="200" alt="스크린샷 2023-09-07 오후 6 52 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d5fbb098-809c-46c4-944c-257d305020bb">

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ LIME (Local Interpretable Model-Agnostic Explanation; 2016) 장단점?
장점
  - black box로 모델 관계없이 작동!
  - Tabular, Text, Image, embedding된 data 등 폭넓게 이용가능 (data의 output을 보기 때문)
단점
  - Data의 neighborhood의 범위의 정의가 어렵다. (tabular)
  - Kernel의 hyperparameter 튜닝이 어렵고, 결과가 unstable하다. (매번 결과가 달라진다)
  - Feature간 correlation이 무시되는 경향이 있음
  - 설명 모델의 복잡성이 미리 정의가 되어야 한다.

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ SHAP (SHapley Addictive explanation; 2017)
<img width="627" alt="스크린샷 2023-09-07 오후 6 54 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/79d794f1-cd11-422f-8270-dc96f4e7d9c3">

- 어떤 집 하나가 유난히 가격이 낮는데, 그 집이 숲 속에 있기 때문인지, 평수가 작기 때문인지, 혹은 평수가 작아 고양이를 기를 수 없어서 그렇기 때문인지 정확한 이유를 알 수 없다.
- 결과만 보고 해석하지 않고, 각 요소들이 결과값에 얼마나 영향을 미치는 지에 파악할 수 있을까?

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ SHAP (SHapley Addictive explanation; 2017)
LIME은?
  - Local이다. -> Single prediction explanation에 보다 적합
  - Visualization로 결과 해석이 다소 어렵다.

<img width="350" alt="스크린샷 2023-09-07 오후 6 55 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4c2b20df-6025-44a1-a6a7-34d5d8dcea78">

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ SHAP (SHapley Addictive explanation; 2017)
하나의 특성에 대한 중요도
  - 여러 특성들의 조합을 구성하고, 해당 특성의 유무에 따른 평균적인 변화를 통해 값을 계산
  - $𝜙_i$ : i에 대한 Shapley 값은 아래와 같다.
  
    <img width="350" alt="스크린샷 2023-09-07 오후 6 56 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8c797836-b985-47a8-bfcd-d4bbcedb31e3">

  - F: 전체 집합
  - S: 전체 집합에서 i번째 데이터가 빠진 나머지의 모든 부분 집합
  - $𝑓_{s ∪ \\{ i \\} } (𝑥_{s ∪ \\{ i \\} } )$ : 전체 기여도
  - $𝑓_s(𝑥_s)$ : i를 제외한 전체 기여도

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ SHAP (SHapley Addictive explanation; 2017)
<img width="350" alt="스크린샷 2023-09-07 오후 7 00 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8c6b935e-2fbf-4af9-9e4a-09d6b3a4abc5">

Additive Feature Attribution 방법
  - Lime과 같이 binary 변수의 linear함수로 되어있는 explanation 모델을 가진다

    <img width="150" alt="스크린샷 2023-09-07 오후 7 00 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/05334f6d-7ae3-4c42-af00-92d0b9b0f777">

  - g : explanation/surrogate model , $𝑔 (𝑧') = 𝑓(h_x (𝑧'))$
  - f : original prediction model 
  - z′ : simplified input , z′ ≈ x′, $h_x$ : mapping function , $x = h_x(𝑥')$
  - $𝜙_i$ : attribution value

- 복잡한 모델 f(x)대신, 해석이 간단한 모델 g(xʼ)로 해석하고자 한다
- z′ : simplified input , z′ ≈ xʼ라는 가정으로 𝑧' ∈ $[0,1]^M$  (0, 1 인풋이 model에 있는지 여부) 이고,
- g(z') ≈ $𝑓(h_x (𝑧'))$ 을 만족하는 g를 만드는 방법이다.

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ SHAP (SHapley Addictive explanation; 2017)
Additive Feature Attribution의 조건을 전부 만족하여야 이상적이다.
- Local accuracy : 𝑓(𝑥) = 𝑔(𝑧') = $𝜙_0 + ∑_i^M  𝜙_i 𝑥_i' $
  - Simplified input x′을 explanation model 에 넣었을 때의 output g(x′) 는, original input x 에 대한 f(x) 와 match 가 잘 되어야 한다.
- Missingness : $𝑥_i'$ = 0 ⇒ $𝜙_i$ = 0
  - Feature 값이 없다면, attribute 값도 0이 되어야 한다.
- Consistency <img width="250" alt="스크린샷 2023-09-07 오후 7 14 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e5f7d4f4-f4fb-4b6f-92de-b877f94b7ace">
일 때, 모든 input 𝑧' ∈ {0,1} 이고, <img width="250" alt="스크린샷 2023-09-07 오후 7 15 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c0bbe3e3-5663-461c-88e5-7ee74ed6d0e5">
이면, $𝜙_i (𝑓', 𝑥) ≥ 𝜙_i (𝑓,𝑥)$ 이다.

- 모델이 변경되어 Feature 의 Marginal Contribution 이(다른 특성에 관계없이) 증가하거나 동일하게 된다면, Attribution 도 증가하거나 동일하게 유지되어야 하며, 감소할 수 없다.
  
Additive Feature Attribution의 3가지 조건과 정의를 만족하는 유일한 explanation모델은?

<img width="500" alt="스크린샷 2023-09-07 오후 7 17 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2b9d5919-7abc-45bf-a9d6-f94c40a4e3ab">

- $𝜙_i$ 는 Shapley value!

## eXplainable AI (XAI) - Model-Agnostic XAI ‒ SHAP (SHapley Addictive explanation; 2017)
SHAP은? - Shapley Value의 conditional expectation이다.    
보다 심플한 input을 위해, f값이 아닌, f의 conditional 한 expectation을 계산한다.

<img width="500" alt="스크린샷 2023-09-07 오후 7 18 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c9028342-e492-4325-ae5f-864321acdb46">

- 𝑓(𝑧) = $𝑓(h_x, (𝑧'))$ = 𝐸[𝑓(𝑧)| $𝑧_s$ ]
- 위 그림: $𝜙_{0 \sim 3}$ : 긍정 요소, $𝜙_4$ : 방해 요소
- SHAP은 Shapley Value (Local Explanation) 기반으로 하여, 데이터 셋의 전체적인 영역을 해석이 가능하다! (Global Surrogate)
- 모델 f에 따라, 계산법을 다르게 하여 빠르게 처리 한다. (Kernel SHAP, Tree SHAP, .. 등)

<img width="400" src ="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8b77bdce-4541-4443-8fc9-79d615880345">

## eXplainable AI (XAI) - Model-specific XAI ‒ CAM (2016)
<img width="400" alt="스크린샷 2023-09-07 오후 7 23 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9cd14070-e821-4e17-84cb-9e6fb04d69e9">

- GAP layer는 각각의 feature map의 값들을 평균을 취한 것.
- 즉, feature map의 크기와 관계없이 channel이 k개라면 k개의 평균 값을 얻을 수 있다.
- 이러한 GAP는 Fully Connected(이하 FC) Layer와 달리 연산이 필요한 파라미터 수를 크게 줄일 수 있으며, 결과적으로 regulariztion과 유사한 동작을 통해 overfitting을 방지할 수 있다.
- 또한 FC layer는 Convolution layer에서 유지하던 위치정보가 손실되는 반면에, GAP layer는 위치정보를 담고 있기 때문에 localization에 유리하다.

<img width="500" alt="스크린샷 2023-09-07 오후 7 24 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ccd084ec-64a9-4d9f-b803-784add7395c5">

- CNN layer를 거쳐 마지막에 k개의 feature map을 얻는다면 -> GAP를 적용하면, 총 k개의 output.
- K번째 GAP의 feature map: $𝐹_k = ∑_{x,y} 𝑓_k (𝑥, 𝑦)$
- Softmax의 input $𝑆_c = ∑_k 𝑤_k^c 𝐹_k , 𝑤_k^c$ : class c에 대한 $𝐹_k$ 의 가중치(중요도)
  - 이 값을 선형 결합한 후 여기에 softmax를 적용하면 최종 output probability( $𝑃_c$ )가 생성!

<img width="450" alt="스크린샷 2023-09-07 오후 7 28 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8cd5739f-f86f-4f68-bb9b-7888330ae86e">

에서, $∑_k 𝑤_k^c f_k$ (𝑥, 𝑦) 을 activation map이라고 하여, $𝑀_c(𝑥,𝑦)$ 라고 하자.

- $𝑀_c(𝑥, 𝑦)$ 을 이미지에 매핑하면, 어디가 중요한지 파악 가능!

<img width="400" alt="스크린샷 2023-09-07 오후 7 29 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/32aa1a77-659e-4d34-b3f4-bf0ec6805ad5">

<img width="250" alt="스크린샷 2023-09-07 오후 7 30 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7b0f6272-677d-4a77-be45-d6ce1e6c3f57">

## eXplainable AI (XAI) - Model-specific XAI ‒ GradCAM (2017)
<img width="500" alt="스크린샷 2023-09-07 오후 7 31 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/532284a7-0c8d-442d-999f-8de64d2ece8e">

Grad-CAM은 CAM에서 CAM의 weight w를 다른 값으로 대체하고 GAP의존성을 없애, 일반화   

<img width="150" alt="스크린샷 2023-09-07 오후 7 31 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b590603f-af7d-4756-9e2e-4c505aacd494">

- $𝐴_{ij}^k$ 가 마지막 CNN의 k번째 채널의 feature map일때, partial linearlization을 의미하며,
- $\partial y^c \over \partial A_{ij}^k$ 는 backpropagation을 통해 얻어질 수 있다!
- $1 \over z$ $∑_i ∑_j$ 부분은 GAP와 결과적으로 연산이 같다
- 최종적으로 activation map은 𝑅𝑒𝐿𝑈( $∑_k 𝛼_k^c 𝐴^k$ ) 을 통해 구해진다.

## eXplainable AI (XAI) - Model-specific XAI ‒ GradCAM (2017)
<img width="600" alt="스크린샷 2023-09-07 오후 7 37 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d050fec8-3ea2-4ffe-bdc3-eb8e3f1ec615">

## eXplainable AI (XAI) - For more trustable AI ‒ Adversarial Example
<img width="500" alt="스크린샷 2023-09-07 오후 7 37 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3ccb4ef9-b42d-40e8-ae0c-bdd5ef646684">

DNN의 신뢰 가능도를 파악하는 다른 방법: Adversarial example을 이용

## eXplainable AI (XAI) - 참고하면 좋은 자료
책 
- https://christophm.github.io/interpretable-ml-book/
  
리뷰 논문
- Arrieta, Alejandro Barredo, et al. "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI." Information Fusion 58 (2020): 82-115.
- Linardatos, Pantelis, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. "Explainable ai: A review of machine learning interpretability methods." Entropy 23.1 (2021): 18.
  
오픈소스
- SHAP: https://github.com/slundberg/shap
- LIME: https://github.com/marcotcr/lime

AWESOME Curation
- https://github.com/wangyongjie-ntu/Awesome-explainable-AI
