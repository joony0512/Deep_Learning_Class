# <Generative model series #5> Distribution Alignment - 2. Marginal Matching & Cycle COnsistency (Dual Learning)

## Supervised pair 모델들의(pix2pix)의 한계.
<img width="400" alt="스크린샷 2023-08-24 오후 3 38 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b7f15bd0-cee9-40a8-9756-78918aea87e3">

- 생성(Generative Model)에서의 application
    - 순수 $𝑝_{data}$ 만 reproduce, $p_{model}$ 만으로는 응용력이 작다. 
    - A의 style을 가진 것을 B의 input으로 만들고 싶다면? 
    - Supervised pair모델들: pair로 넣어야 한다.
    - 하지만, pair데이터가 만약 너무 비싸다면?

## CycleGAN (2017)
도메인 X와 Y를 mapping하는 함수를 학습하는 것.
- 즉, 𝑥 ∈ 𝑋, 𝑦 ∈ 𝑌이고, 𝑥 ~ $p_{data}$(𝑥), 𝑦 ~ $p_{data}$(𝑦)일때,
- 𝐺:𝑋→𝑌와 𝐹:𝑌→𝑋를 학습하는것이목표!

2개의 constraint를 만족하며 학습을 진행한다.
1. Marginal matching
- 각 매핑의 output은 source domain margin이라면, 대상 도메인의 empirical distribution과 매칭되어야 한다! 
2. Cycle consistency
- 2번의 매핑으로 한 도메인으로 매핑 후 다시 돌아왔을 때, sample은 원본과 비슷하여야 한다.

## CycleGAN (2017) - Marginal matching
<img width="400" alt="스크린샷 2023-08-24 오후 3 58 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c1e5c3d3-7aec-4366-ba6f-de7c78215aa8">


Marginal matching loss은 GAN과 같은 형태로 정의한다.   
$G_{XY}, D_Y$에 대해서,   <img width="300" alt="스크린샷 2023-08-24 오후 4 00 25" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8204cefb-f7ea-4709-a3a1-78bf044ed274">  
$F_{XY}, D_X$에 대해서도 동일하게 loss를 구한다.


## CycleGAN (2017) - Cycle consistency
- Dual learning, back translation이라고도 한다.

    <img width="300" alt="스크린샷 2023-08-24 오후 4 06 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8f5a2332-8a13-4718-9811-0f208398a33b">

- 이상적으로 아래를 만족하게 mapping하는 것이 목표! 
    - $F_{YX} (G_{XY} (x)) =x$
    - $G_{XY} (F_{YX} (x)) =y$

- L1 loss로 차이를 구할 때,목적함수는,  
<img width="300" alt="스크린샷 2023-08-24 오후 4 04 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a8794197-bae6-476f-aa96-57af6dc36abe">

- $𝐿^Y_{cyc}(F_{YX} ,G_{XY})$ 는 위의식의 역을 취해주면된다!

## CycleGAN (2017) - 최종목적함수
최종 목적 함수는  
<img width="400" alt="스크린샷 2023-08-24 오후 4 07 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b4161dad-ee3d-4430-8be7-3a506fce40f1">

가 된다.   
- 이때 𝛾는 marginal matching과 cycle consistency를 조절하는 hyperparameter이다.

## CycleGAN (2017) - 결과
<img width="350" alt="스크린샷 2023-08-24 오후 4 08 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/44fa6a48-f72a-402c-806e-eef9e267723d">
<img width="500" alt="스크린샷 2023-08-24 오후 4 09 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/992e0cf6-8ffd-47b2-835b-f12cdd143db2">


## 참고: DiscoGAN (2017)
- DiscoGAN은 CycleGAN과 2017년도에 동일한 시기에 비슷한 아이디어로 나온 논문! (다만 동기간에 submit되어 둘다 accept되었다.)  
- Reconstruction + Cycle-consistency 을 손실 함수로 같이 이용하고, 거의 같은 모델로 인지되고 있다.
    (참고: https://github.com/nashory/gans-collection.torch/issues/1)
- 손실 함수 구성에서 차이
    - CycleGAN: L1 cycle consistency loss($G_{ab}, G_{ba}$ 두 경우를 더함.)
    - DiscoGAN: L2 cycle consistency loss (각 도메인 별로 따로따로 구한다)
- Generator Model 구조가 조금 다르다.

## DualGAN; Stochastic GAN (2017) - Stochastic Mapping
1:1 deterministic한 매핑은 너무 제약이 많다.   
이를 좀더 flexible하게 바꿀 수 있을까?
    - 𝐺$% 𝑥 : 𝑋 → 𝑌
대신, 아래식으로 확장해보면 어떨까?
    - 𝐺$% 𝑥,𝑧 :𝑋×𝑍 →𝑌
그러나 이경우, cycle-consistency loss역시 같이 개선하여야 한다.
    - è𝐺%$ 𝐺$% 𝑥,𝑧 ,𝑧4 =𝑥에서,z와 zʼ는 관련이 없을수 도 있기에,z의 값이 쉽게무시될 수 있다!
