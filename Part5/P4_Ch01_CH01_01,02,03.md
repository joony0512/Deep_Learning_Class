# CH01_01. AI vs 머신러닝 vs 딥러닝

## 인공지능/기계학습/표현학습/깊은학습

인공지능이란? : 지능적인 기계나 프로그램을 엔지니어링 하는것

- 지식기반, 룰기반

기계학습이란? : 정확하게 프로그래밍되지 않아도 학습할 수 있는 능력

- 데이터의 ‘feature’ 사이 ‘패턴’이나 ‘상관관계’를 뽑아냄

표현학습(Representation Learning)이란?

- ‘Feature’대신, ‘representation’을 학습함.

깊은학습이란? 깊은신경망으로 구성된 알고리즘

- 복잡한 표현을 단순한 표현으로 변환하여 문제를 품

인공지능 (i.e. automated planning)

>머신러닝 (i.e. logistic regression)

>표현학습 (i.e. shallow neural network)

>딥러닝 (Deep learning)

## 인공신경망

인공신경망(Artificial Neural Network)

- 생물학적 뉴런(신경세포)를 모방한 인공뉴런
- 단순 단층 신경망은 , 선형문제밖에 풀 수 없어, 한계가 있었음.

선형모델의 한계를 극복 → 표현학습(Representation Learning)

- 은닉층과 활성함수를 넣어 비선형으로 만든다.
    - 분류전 피처의 공간을 변환시킨다. → 좌표계를 변환 시킨다.
    

## 전통적인 머신러닝 vs 딥러닝

전통적인 머신러닝 : Hand - craft 피처 엔지니어링

- feature engineering을 어느정도 직접 한 후,  모델에 넣음

딥러닝 : 피처 엔지니어링을 DNN이 어느정도 알아서 → 표현학습과 분류를 동시에!

- feature engineering을 알아서 해줌—> 신경망에 맡김

# CH01_02.기계학습의 종류

## ML이란?

- 기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습할 수 있게 하는 방법

### ML의 종류

- 지도학습 Supervised → p(y|x)
    - 바람직한 결과(label)을 줌으로써 학습하는 방법
        - 레이블(label) o
        - 직접적인 피드백 o
        - 아웃풋과 피처를 예측
    - 분류 Classification
    - 회귀 Regression
- 비지도학습 Unsupervised→ p(x)
    - 정답정보(label)없이 학습하는 방법
        - 레이블(label) x
        - 피드백 x (혹은 간접적 피드백)
        - 데이터에서 숨겨진 구조찾기
    - 군집화 Clustering
    - 차원축소 Dimension Reduction
    - 연관 Association
- 지도학습과 비지도학습은?
    - 엄밀히 구분되어있지는 않다.
        - ex) 자가회귀(autoregressive)
        - ex) 결합분포(joint distribution)
- 준지도학습(Semi-supervised learning)
    - 레이블 된/ 안된 데이터 모두 학습
    - 일반적으로, 다수의 레이블되지 않은 데이터와, 레이블된 골드레이블(Gold label) 데이터로 구성
- 자기지도학습(Self -supervised learning)
    - 레이블을 스스로 만들어서 학습한다.
    - pretext task를 설정(데이터의 일부분을 은닉하여 모델이 그 부분을 예측하도록 학습)하여 이를 학습한 후, downstream task로 전이학습하여 다른 테스크에 적용.
        - 사진에 구멍뚫고 맞추게 하기
- 강화학습(Reinforcement Learning)
    - 학습하는 시스템인 에이전트(Agent)는 환경(Environment)을 관찰해서 나온 상태(State)가 주어졌을때, 정책(Policy)에 기반하여 행동(Action)을 실행하고 보상(Reward)을 받는다.
    - 가장 큰 누적 보상을 얻을 수 있는 최적화된 정책을 스스로 학습한다.
    
    ![Untitled](https://user-images.githubusercontent.com/109457820/229101248-152fd5bc-f5cc-4c73-96d4-3b79fde21c72.png)
    
# CH01_03.선형회귀, 로지스틱회귀, log-likelihood

## 지도학습

- Classification(분류)
    - 이산적(discrete)으로 구성된 레이블을 각각의 학습 데이터로부터 학습시키는 경우
- Regression(회귀)
    - 연속적인 레이블을 각각의 학습 데이터로부터 학습시키는 경우

## 선형회귀

- 선형회귀로 이항분류하기
    - Linear Regression : f(x) = θ1x + θ0라고 하자.
    - f(x)의 에러의 제곱의 합(sum of squared residuals; RSS)을 목적함수로 최소화하여, 최적의 함수를 찾는다.
        
        ![Untitled (1)](https://user-images.githubusercontent.com/109457820/229101415-679c5cad-c195-46c6-ad7a-f839c192578f.png)
        
    - 즉, Rss를 최소화하는데 , 이 최소값은 미분한 값이 0이 될때이다.
        - θ0 = E[y] -θ1E[x]
        - θ1 = Σ(yi -E[y])(xi-E[x]) / Σ(xi-E[x])**2
    
    ![Untitled (2)](https://user-images.githubusercontent.com/109457820/229101483-f10fa4b8-0fb6-4e70-a869-297ae4d58a09.png)
    
    - 선을 기준으로 왼쪽 오른쪽으로 분류 가능 → 이항분류

## 로지스틱 회귀

- 로지스틱회귀로 이항분류를 해보자.
- Linear Regression : f(x) = θ1x + θ0
- Logistic Regression :f(x) =1/(1+e**(-x))
    - Sigmoid함수, Logistic함수라고도 불림
    - **Sigmoid함수**
        - 연속함수이며 대칭함수(symmetric)이다.
        - 미분이 매우 쉬워, gradient descent에 적합하다!
        - f’(x) = f(x) - f(x)**2
    - 로그 우도법(Log Likelihood)
        - 우리가 이항 분류를 하고 있기 때문에, 각 값은 0혹은 1로 예측될 것이다.
        - 즉, 이는 Bernoulli분포를 따른다고 할 수 있다.
            
            ![Untitled (3)](https://user-images.githubusercontent.com/109457820/229101619-757ef6c3-b631-4796-b561-158e43cc121f.png)
            
        - 모든 샘플이 독립일경우 모든 i에 대해 곱할 수 있다.  :  Likelihood
            
            ![Untitled (4)](https://user-images.githubusercontent.com/109457820/229101699-63d30537-8a92-49f1-9c01-e19eb58f704c.png)
            
            - 이는 계산하기 힘드므로 log를 씌우면 :  Log likelihood
                
                ![Untitled (5)](https://user-images.githubusercontent.com/109457820/229101777-2f082ede-628e-47f3-837d-2fca38c09280.png)
                
        - 이를 음수로 취해 줄이는 방향의 손실함수(loss function)으로 사용
            - negative log-likelihood
            
            !![Untitled (6)](https://user-images.githubusercontent.com/109457820/229101844-b61b8b1c-a0db-41df-bc5c-d61dcd96933a.png)
            
            - 이는 Cross Entropy와 같다
