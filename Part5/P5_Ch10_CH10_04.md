# <Generative model series #2> Flow Models - 4. Dequantization
## Continuous flow에서 이산적인 데이터(discrete data)을 어떻게 처리할까?
- 디지털 데이터는 일반적으로, 이산적으로 정의되어 있다. (i.e. 이미지 – 각 픽셀에 3 bit(3 channel, 8bit (0~255))을 할당)
- 연속확률분포 형태의 모델로는 중첩 문제 등으로 이산 데이터에 대한 제대로 된 성능 평가가 어려움 - degeneracy 문제!
- Data에 실수의 노이즈를 섞어 dequantization 시킨다! 
 
## Continuous flow에서 이산적인 데이터(discrete data)을 어떻게 처리할까? - Uniform Dequantization
<img width="330" alt="스크린샷 2023-08-10 오후 5 15 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2527f95b-323a-4620-a2f4-86a6a316aa48">

데이터가 x, 노이즈가 u일때, 우리는 noisy data(y = x + u )로 evaluation할것이다. 이때 noise가 uniform distribution을 따른다면 uniform dequantization이라고 부른다.   
이산형 구간 내 확률 밀도의 적분으로 이산형 확률질량(discrete probability mass)를 근사한다.  
<img width="453" alt="스크린샷 2023-08-10 오후 5 19 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ba2121e3-43e0-4c46-b907-7ea84701b845">  
- uniformly dequantized data y를 가지고 continuous density model을 학습시키는 건 어떤 original discrete data 에 대한 모델인 Pmodel 의 log likelihood의 lower bound를 최대화 하는 것이다.
  
<img width="865" alt="스크린샷 2023-08-10 오후 5 20 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d81a3dcc-d092-4ea3-a2d1-bda104266e68">

- 결과적으로, uniformly dequantized data로 continuous model의 log-likelihood를 최대화 하는 건 continous model degenerately collapsing을 일으키지 않는다!   
- 왜냐하면 objective가 discrete model의 log-likelihood 보다 작다는 것이 보장 (bounded)되어있기 때문이다.   
