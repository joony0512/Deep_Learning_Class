# CH01_07.[TheorySession3]정보이론

### 정보이론

- 정보량 : 특정한 관찰에서 얼마만큼 정보를 획득했는지 수치화 한 값.
    - 사건 A가 발생할 확률 $P(X)$라 할때
    - 정보량 : $h(X) = -logP(X)$
    - 정보가 자주 일어나면 작아진다.
- 엔트로피(Entropy): 이산확률 변수의 평균정보량을 의미 (클수록 정보량이 많다)
    - $H[X] = \Sigma p_i log_2 1/p_i = -\Sigma p_i log_2p_i (p_i = P(X =x_i)$
- 쿨백-라이블러 발산( KL-Divergence : KLD)
    - 두 확률 분포의 다른 정도를 나타내는 척도
    - Relative entropy라고도 한다.
    - $D_{KL}(P||Q) = -\Sigma p_ilog_2q_i -(-\Sigma p_ilog_2 p_i) = -\Sigma p_i log_2 q_i/p_i = H(P,Q) -H(P)$
        - KL-Divergence 의 성질
        - $D_{KL}(P||Q) =! D_{KL}(Q||L)$
        - $D_{KL}(P||Q) =0, if$  and only if p=q
        - $D_{KL}(P||Q)$ ≥ 0 ( Jenson Inequality)
- 크로스 엔트로피( cross-entropy)
    - $H(P,Q) = \Sigma p_i log_2 1/q_i = -\Sigma p_i log_2 q_i$\
    - 실제 머신러닝 학습상황 :
    $D_{KL}(P_{data}||q_{model}) = E_{x~p_{data}}[log_2 p_{data}(x) - log_2 q_{model}(x)]$ 최소화하기 
    → 데이터는 고정, 모델을 최적화해야한다.
    - 왼쪽의 data관련 텀은 모델과 관련없는 데이터 생성부분이기 때문에, 단순분류 문제에서는 생략가능
    즉, $-E_{x~p_{data}}[ log_2 q_{model}(x)]=-\Sigma_i p_{data} log_2 q_{model} = H(P_{data}, Q_{model})$

---
