# Deep Reinforcement Learning - 6. Deep RL의 발전 ‒ Model-Based RL
## Model-based RL - Controller
Control theory가 Model-based RL에 영향을 많이 끼쳤다.
- RL의 목표: policy( $𝜋_\theta(𝑎_t|𝑠_t)$ )을 최적화
- Control theory에서의 목표: controller( $𝑝(𝑢_t|𝑥_t)$ )을 최적화
  - 만약 state, action에 대한 model term로 바꾸면? -> $𝑝 (𝑠_{t+1} | 𝑠_t, 𝑎_t)$
  - Model-based RL은 policy보다 model(controller) $𝑝 (𝑠_{t+1} | 𝑠_t, 𝑎_t)$ 를 최적화 한다!
  - 참고: 만약 stochastic이 아닌, deterministic하다면 $𝑓(𝑠_t, 𝑎_t) = 𝑠_{t+1}$ 로 많이 표기한다.
- 모델의 cost function을 정의할 수 있다면, data로부터 model(𝑓: 𝑋 × 𝑈 → 𝑋)을 학습하는 식으로 사용하여 최적의 action을 계산하자!

  <img width="179" alt="스크린샷 2023-08-30 오후 6 58 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cbc6e019-6f99-454e-bf56-511a7a0f8400">

## Model-based RL - Why Model-based?
장점
- Model-based RL은 샘플 효율성을 가진다는 강한 장점!
- 모델과 cost function을 안다고 할 때, 우리는 sampling 없이 최적의 controls을 구상할 수 있다!
- On-policy Policy Gradient 방법들은 10M training iteration을 수반한다면, Model Based RL은 100번의 training iteration안에서 가능! 10만 배가 빠르다!
단점
- Model based 방법론들은 더 많은 가정들과 근사치를 가지고 있다.
- 훈련된 모델을 더 작은 작업으로 제한!
- 실제 물리적인 모델이 존재하는 환경에는 Model Based RL이 적합하지만, 시뮬레이션 환경 등에서는 모델에 많은 가정과 근사치가 포함되어 있기에 성능이 저하될 수 있으며, 훈련 시에도 일반적인 경우보다 작은 task 범위에서 밖에 사용할 수 없다. 

## Model-based RL - Model을 배운다면?
$𝑓(𝑠_t, 𝑎_t)$ 을 데이터로 부터 배우고, $𝑓(𝑠_t, 𝑎_t)$ 을 통해 plan을 하자!
1. 초기세팅 데이터 𝐷={ $(𝑠,𝑎,𝑠')_i$ }을 모으기위해 base policy $𝜋_0(𝑎_t|𝑠_t)$ 을 run한다.
2. $∑_i || 𝑓(𝑠_i,𝑎_i) − 𝑠'_i||^2$ 을 줄이기위해 dynamic model 𝑓(𝑠,𝑎)을 배운다.
3. 𝑓(𝑠, 𝑎)을 통해 planning을 하고 action을 선택한다.
- 여기까지 한다면?

<img width="150" alt="스크린샷 2023-08-30 오후 7 12 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/24bc0265-8f68-492d-9893-4470c3541039">

이므로, distribution mismatch 문제가 점점 더 악화 될 것이다 
- 데이터를 추가하자 -> 그리고 모델의 실수를 개선하기 위해 replanning을 하자!

## Model-based RL - MPC (Model predictive control)
$𝑓(𝑠_t, 𝑎_t)$ 을 데이터로 부터 배우고, $𝑓(𝑠_t, 𝑎_t)$ 을 통해 plan을 하자!
1. 초기세팅 데이터 𝐷={ $(𝑠,𝑎,𝑠')_i$ }을 모으기위해 base policy $𝜋_0(𝑎_t|𝑠_t)$ 을 run한다.
2. $∑_i || 𝑓(𝑠_i,𝑎_i) − 𝑠'_i||^2$ 을 줄이기위해 dynamic model 𝑓(𝑠,𝑎)을 배운다.
3. 𝑓(𝑠, 𝑎)을 통해 planning을 하고 action을 선택한다.
4. first planned action을 실행하고, resulting state 𝑠′ 을 관찰한다.
5. (𝑠, 𝑎, 𝑠') 를 dataset 𝐷 에 추가한다. (2-3 반복 n step 마다)

MPC에서는 전체 trajectory을 최적화하지만 첫 번째 action만 취한다! 관찰하고 다시 실험!
- Replanning을 함으로써 현재 상태를 다시 관찰한 후 시정 조치를 취할 수 있는 기회를 제공하자
- Stochastic model의 경우, 더 도움
  - 그렇다면, 3번 과정의 replanning은 어떻게 할까?
 
## Model-based RL - Backpropagation(역전파)를 policy에 적용한다면?
$𝑓(𝑠_t, 𝑎_t)$ 을 데이터로 부터 배우고, $𝑓(𝑠_t, 𝑎_t)$ 을 통해 plan을 하자!
1. 초기세팅 데이터 𝐷={ $(𝑠,𝑎,𝑠')_i$ }을 모으기위해 base policy $𝜋_0(𝑎_t|𝑠_t)$ 을 run한다.
2. $∑_i || 𝑓(𝑠_i,𝑎_i) − 𝑠'_i||^2$ 을 줄이기위해 dynamic model 𝑓(𝑠,𝑎)을 배운다.
3. 𝒇(𝒔, 𝒂)로 역전파(backpropagation)하여 action을 고른다.
4. first planned action을 실행하고, resulting state 𝑠′ 을 관찰한다.
5. (𝑠, 𝑎, 𝑠') 를 dataset 𝐷 에 추가한다. (2-3 반복 n step 마다)

   
Policy만으로 만든 것보다 더 나은 행동을 planning으로 하게 만든다! 
- 정책에 비해 상대적으로 expert 하다!
- 대표적인 모델 ? -> Alpha(GO) Zero

<img width="200" alt="스크린샷 2023-08-30 오후 7 17 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1ddd0bb2-df6b-40d0-b322-e2fc7b3cd766">

<img width="400" alt="스크린샷 2023-08-30 오후 7 18 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/dfb1e960-720d-4822-8f3e-3762edfc473e">

## Model-based RL - Alpha(GO) Zero
<img width="300" alt="스크린샷 2023-08-30 오후 7 20 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/485eec36-bc36-4919-a500-85ea976e3043">

<img width="500" alt="스크린샷 2023-08-30 오후 7 20 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d41204e5-48be-40d2-8680-ded81c357813">

신경망 $𝑓_\theta$ 이 input state s를 받아 2개의 output과 continuous value $𝑣_\theta$ ∈ [−1, 1] 과 모든 가능한 action space에 대한 확률 벡터인 policy 𝑝!(𝑠)를 return한다   
즉, 𝑝,𝑣 = $𝑓_theta$ (𝑠)  
Monte-Carlo Tree Search (MCTS) 모델로 이 과정을 improve!  

## Model-based RL - Alpha(GO) Zero ‒ DNN
신경망 $𝑓_\theta$ 의 objective function?  
- $𝜋_t$ 을 $𝑠_t$ 로부터 policy의 측정값 (DNN의 t step후의 output)이라고 하고,  
- $𝑧_t$ ∈ {−1,1}는 $𝑠_t$ 에서의 player의 perspective로 부터의 게임의 최종 결과라면, DNN은 ($𝑠_t, 𝜋_tt, 𝑧_t$) 형태의 training data를 사용하여 아래 목적함수를 최소화 한다.
- <img width="300" alt="스크린샷 2023-08-31 오후 4 56 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e98ec606-fb99-4792-a00d-f1614557b1a2">

기본적인 생각?  
- 시간이 지남에 따라 DNN이 “어떤 상태가 결국 승(패)로 이어지는지”를 알게 된다는 것!
- 정책을 배우는 것은, 주어진 state에서 가장 좋은 action이 무엇인지에 좋은 추정을 하는 것.
- 일반적으로 DNN은 게임에 따라 아키텍처가 달라진다.
- 논문(바둑)에서는 CNN 아키텍처를 사용!

## Model-based RL - Alpha(GO) Zero - MCTS
- $𝑠_t$ 로부터 DNN은 policy의 측정값 $𝑝_\theta (𝑠_t)$ 을 추론!
- 학습 단계에서 이 측정을 증가시키고 싶다면? -> Monte Carlo Tree Search(MCTS)로 개선해보자.
- Minimax에서의 행동 트리를 탐색하는 과정에서 Monte Carlo 기법을 적용 -> 모든 상태에 대해 찾아보지 않고서도 좋은 행동을 선택할 수 있도록 설계된 알고리즘

<img width="500" alt="스크린샷 2023-08-31 오후 5 00 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e118aef0-6da1-496d-9208-002eced71f2b">

MCTS에서 수렴을 통해 얻고자 하는 최종적인 결과는?
- 트리에 있는 각 노드 (게임의 상태) 에서 ʻ내가 이길 확률이 얼마나 되는지ʼ 알아내는 것!
- 즉. 현재 게임의 상태 s로 부터 내가 이길 수 있는 정도를 나타내는 가치 v(s)를 만들어 내는 것

## Model-based RL - Alpha(GO) Zero - MCTS
Tree search에서 다음을 계산한다!
- 𝑄 (𝑠, 𝑎) : state로부터 action을 취할 때 expected reward 값 (i.e. Q-value)
- 𝑁 (𝑠, 𝑎) : simulation을 중 state s로부터 action을 취한 수
- 𝑃 (𝑠, ⋅) = $𝑝_\theta$ (𝑠) : 현재 신경망에 의해 return된 정책에 따라, 상태 s에서 action을 취하는 초기추정치.  
여기에서, Q 값의 upper confidence bound인 𝑈(𝑠, 𝑎)를 계산!

<img width="300" alt="스크린샷 2023-08-31 오후 5 07 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/603ec2fa-27ca-450e-8962-669ad66f6d06">

- 여기서, $𝑐_{pcut}$ 는 exploration 을 조절하는 hyperparameter이다
   
- 현재 DNN에 의해 반환된 초기 정책(initial policy)을 개선하기 위해 MCTS를 사용하기 위해서는?
1. S을 루트로 하여 빈 search tree를 초기화한다.
2. Upper confidence bound U(s, a)를 최대화하는 action a를 계산한다.
  - 만약 tree에 다음 상태 sʼ (state s에서 동작 a를 수행하여 얻어지는)가 존재한다면, s′에서 검색을 재귀적으로 호출
  - 존재하지 않을 경우, 새로운 상태를 DNN로부터 얻어, $𝑃 𝑠(', ⋅) = 𝑝_\theta(𝑠')$ 와 value 𝑣 (𝑠', ⋅) = $𝑣_\theta(𝑠')$ 로, Q(sʼ,a),N(sʼ,a)역시 모든 a에 대해 0로 초기화한다..
3. 전체 rollout을 수행하는 대신, 현재 시뮬레이션에 나타난 경로를 따라 v(sʼ)를 전파하고 모든 Q(s, a)를 업데이트 한다.
4. 만약, terminal state에 이르면, 이겼다면 +1, 아니라면 -1로 실제 reward를 수행한다.
  - 시뮬레이션을 반복하면, root의 N(s, a) 값은 policy에 대한 더 나은 근사가 된다.

## Model-based RL - Alpha(GO) Zero ‒ MCTS
<img width="450" alt="스크린샷 2023-08-31 오후 5 11 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3c1f6f45-e5cb-410b-86eb-b9099e7a8ca3">

## Model-based RL - Alpha(GO) Zero ‒ Self-play
셀프 플레이 중에 MCTS를 수행하고 개선된 정책(𝜋(𝑠))에서 어떤 move를 할 것인지를 sampling하여 결정한다.

<img width="500" alt="스크린샷 2023-08-31 오후 5 12 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c7606f05-9baa-4e89-925d-dfb62374b027">

<img width="500" alt="스크린샷 2023-08-31 오후 5 12 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5035dd4b-4806-4991-82bb-f04022642f22">
