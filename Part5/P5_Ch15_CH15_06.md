# Deep Reinforcement Learning - 6. Deep RL의 발전 ‒ Model-Based RL
## Model-based RL - Controller
Control theory가 Model-based RL에 영향을 많이 끼쳤다.
- RL의 목표: policy( $𝜋_\theta(𝑎_t|𝑠_t)$ )을 최적화
- Control theory에서의 목표: controller( $𝑝(𝑢_t|𝑥_t)$ )을 최적화
  - 만약 state, action에 대한 model term로 바꾸면? -> $𝑝 (𝑠_{t+1} | 𝑠_t, 𝑎_t)$
  - Model-based RL은 policy보다 model(controller) $𝑝 (𝑠_{t+1} | 𝑠_t, 𝑎_t)$ 를 최적화 한다!
  - 참고: 만약 stochastic이 아닌, deterministic하다면 $𝑓(𝑠_t, 𝑎_t) = 𝑠_{t+1}$ 로 많이 표기한다.
- 모델의 cost function을 정의할 수 있다면, data로부터 model(𝑓: 𝑋 × 𝑈 → 𝑋)을 학습하는 식으로 사용하여 최적의 action을 계산하자!

  <img width="179" alt="스크린샷 2023-08-30 오후 6 58 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cbc6e019-6f99-454e-bf56-511a7a0f8400">

## Model-based RL - Why Model-based?
장점
- Model-based RL은 샘플 효율성을 가진다는 강한 장점!
- 모델과 cost function을 안다고 할 때, 우리는 sampling 없이 최적의 controls을 구상할 수 있다!
- On-policy Policy Gradient 방법들은 10M training iteration을 수반한다면, Model Based RL은 100번의 training iteration안에서 가능! 10만 배가 빠르다!
단점
- Model based 방법론들은 더 많은 가정들과 근사치를 가지고 있다.
- 훈련된 모델을 더 작은 작업으로 제한!
- 실제 물리적인 모델이 존재하는 환경에는 Model Based RL이 적합하지만, 시뮬레이션 환경 등에서는 모델에 많은 가정과 근사치가 포함되어 있기에 성능이 저하될 수 있으며, 훈련 시에도 일반적인 경우보다 작은 task 범위에서 밖에 사용할 수 없다. 

## Model-based RL - Model을 배운다면?
$𝑓(𝑠_t, 𝑎_t)$ 을 데이터로 부터 배우고, $𝑓(𝑠_t, 𝑎_t)$ 을 통해 plan을 하자!
1. 초기세팅 데이터 𝐷={ $(𝑠,𝑎,𝑠')_i$ }을 모으기위해 base policy $𝜋_0(𝑎_t|𝑠_t)$ 을 run한다.
2. $∑_i || 𝑓(𝑠_i,𝑎_i) − 𝑠'_i||^2$ 을 줄이기위해 dynamic model 𝑓(𝑠,𝑎)을 배운다.
3. 𝑓(𝑠, 𝑎)을 통해 planning을 하고 action을 선택한다.
- 여기까지 한다면?

<img width="150" alt="스크린샷 2023-08-30 오후 7 12 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/24bc0265-8f68-492d-9893-4470c3541039">

이므로, distribution mismatch 문제가 점점 더 악화 될 것이다 
- 데이터를 추가하자 -> 그리고 모델의 실수를 개선하기 위해 replanning을 하자!

## Model-based RL - MPC (Model predictive control)
$𝑓(𝑠_t, 𝑎_t)$ 을 데이터로 부터 배우고, $𝑓(𝑠_t, 𝑎_t)$ 을 통해 plan을 하자!
1. 초기세팅 데이터 𝐷={ $(𝑠,𝑎,𝑠')_i$ }을 모으기위해 base policy $𝜋_0(𝑎_t|𝑠_t)$ 을 run한다.
2. $∑_i || 𝑓(𝑠_i,𝑎_i) − 𝑠'_i||^2$ 을 줄이기위해 dynamic model 𝑓(𝑠,𝑎)을 배운다.
3. 𝑓(𝑠, 𝑎)을 통해 planning을 하고 action을 선택한다.
4. first planned action을 실행하고, resulting state 𝑠′ 을 관찰한다.
5. (𝑠, 𝑎, 𝑠') 를 dataset 𝐷 에 추가한다. (2-3 반복 n step 마다)

MPC에서는 전체 trajectory을 최적화하지만 첫 번째 action만 취한다! 관찰하고 다시 실험!
- Replanning을 함으로써 현재 상태를 다시 관찰한 후 시정 조치를 취할 수 있는 기회를 제공하자
- Stochastic model의 경우, 더 도움
  - 그렇다면, 3번 과정의 replanning은 어떻게 할까?
 
## Model-based RL - Backpropagation(역전파)를 policy에 적용한다면?
$𝑓(𝑠_t, 𝑎_t)$ 을 데이터로 부터 배우고, $𝑓(𝑠_t, 𝑎_t)$ 을 통해 plan을 하자!
1. 초기세팅 데이터 𝐷={ $(𝑠,𝑎,𝑠')_i$ }을 모으기위해 base policy $𝜋_0(𝑎_t|𝑠_t)$ 을 run한다.
2. $∑_i || 𝑓(𝑠_i,𝑎_i) − 𝑠'_i||^2$ 을 줄이기위해 dynamic model 𝑓(𝑠,𝑎)을 배운다.
3. 𝒇(𝒔, 𝒂)로 역전파(backpropagation)하여 action을 고른다.
4. first planned action을 실행하고, resulting state 𝑠′ 을 관찰한다.
5. (𝑠, 𝑎, 𝑠') 를 dataset 𝐷 에 추가한다. (2-3 반복 n step 마다)

   
Policy만으로 만든 것보다 더 나은 행동을 planning으로 하게 만든다! 
- 정책에 비해 상대적으로 expert 하다!
- 대표적인 모델 ? -> Alpha(GO) Zero

<img width="200" alt="스크린샷 2023-08-30 오후 7 17 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1ddd0bb2-df6b-40d0-b322-e2fc7b3cd766">

<img width="400" alt="스크린샷 2023-08-30 오후 7 18 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/dfb1e960-720d-4822-8f3e-3762edfc473e">

## Model-based RL - Alpha(GO) Zero
<img width="300" alt="스크린샷 2023-08-30 오후 7 20 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/485eec36-bc36-4919-a500-85ea976e3043">

<img width="500" alt="스크린샷 2023-08-30 오후 7 20 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d41204e5-48be-40d2-8680-ded81c357813">

신경망 $𝑓_\theta$ 이 input state s를 받아 2개의 output과 continuous value 𝑣! ∈ [−1, 1] 과 모든 가능한 action space에 대한 확률 벡터인 policy 𝑝!(𝑠)를 return한다   
즉, 𝑝,𝑣=𝑓!(𝑠)
Monte-Carlo Tree Search (MCTS) 모델로 이 과정을 improve!
