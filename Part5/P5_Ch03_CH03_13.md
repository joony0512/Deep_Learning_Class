## CH03_13-Regularization-DeepDoubleDescent현상과Regularization[Theory
Session2]

![Untitled (44)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cf5c125e-a729-455e-a9c3-6786e0f4aad4)

- OpenAI의 Nakkiran, Preetum 등은 2019년, 기존의 Bias-variance trade-off의 개념과 상반되게, “모델이 충분히 크다면” (좌우측 그림) 그리고 “데이터가 충분히 많다면” 그리고 “epoch가 충분히 크다면”(우측 그림) , 오버 피팅 구간 이후에도 테스트 에러가 다시 감소하기 시작하는 **“double descent” 구간이 발견**된다고 실험적으로 발견.

**Model-wise 실험 : CNN**
![Untitled (45)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12450da8-b4ca-40b8-b577-8e5bf30315d9)


- **ResNet18, standard CNN(4conv-layers)**
    - layers 의 너비[k; 2k; 4k; 8k]의 k를 변화시키며 실험
    - standard ResNet18, k = 64 를 기반으로 사용
    - Adam optimization (learning rate 0.0001, 4K epoch) 을 이용해 optimization
- Cifar100데이터: 라벨에 noise가 있든 없든 double descent현상이 발생, label noise의 비율이 커지면 커질수록 현상은 더 심화됨.
- Cifar10의 경우에는 noise label이 없는 경우에는 거의 일어나지 않지만, label noise가 추가되면 double descent현상 발생

**Model-wise 실험: Sequence계열은 -Transformer**

![Untitled (46)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4027a70d-1062-43e8-bfe0-c4b0f63c098f)

- Transformer의 embedding dimension에 따른 model wise double descent 현상을 보면
    - embedding size가 크면 클수록 모델의 train loss성능이 좋아지지만, test loss을 확인해보면 embedding dimension이 적을 때 더 성능이 좋은, 일반적인 model complexity에 따른 모델 에러의 그래프가 어느정도 확인된다.
    - 한편, deep double descent가 일어난다!

**Optimizer에 따른 실험**

![Untitled (47)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7c484395-b63d-4957-8a97-21e0f5b26c9a)


- 이 현상은 Optimizer를 어떤 것을 쓰더라도 어느정도 발생하는 것이 관측.

**Epoch-wise 실험**

![Untitled (48)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d47d81c2-067e-466f-94ae-dd5de29d53c4)


- epoch가 증가함에 따라 모델이 충분히 크면, double descent가 일어나는 것을 확인 !
- 모델이 작으면 우리가 아는 방식대로, test error가 epoch가 지남에 따라 올라간다.
![Untitled (49)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/342d57ef-b69f-4e51-a707-6901651e87a8)


- ****또한 데이터에 노이즈가 많을 수록 그 현상이 심화된다!****

**Sample-wise 실험**

![Untitled (50)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/033d58d6-57ab-4652-8917-9842a7c62820)


- Sample이 증가하면 deep double descent가 일반적으로 완화되는 현상을 보인다 !
- 그러나, 데이터가 적을때 (이 경우에서는 <10k)는 오히려 데이터가 많아질수록 성능이 적어질 수도 있다.

**Summary**

- 데이터 양이 어느정도 충분하다는 가정하에,그리고 모델사이즈가 충분히 크면 deep double descent가 일어난다.
- 이 때, 일어나는 정도는
    
    1. 모델 사이즈가 커짐에 따라,
    
    2. 데이터에 노이즈가 많아짐에 따라,
    
    3. epoch가 커짐에 따라
    
    4. 샘플에 따라 조금씩 다 달라진다 !
