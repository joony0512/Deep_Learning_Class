# Pretrained Large-Scale Transformer - 3. Pre-trained Transformer Encoder - BERT을 중심으로
## BERT: Pre-training of deep bidirectional transformers for language understanding

<img width="604" alt="스크린샷 2023-08-08 오후 2 56 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e3c7354d-df50-41fa-b50d-117f70f292d7">

Transformer decoder을 사용하여 autoregressive한 language modeling을 하는 GPT와 다르게,  
BERT는 Transformer encoder (bidirectional transformer)을 이용하고 ELMo 와 같이 Embedding 을 수행 것을 주 목적으로 한다.  
GPT와 ELMo처럼 BERT도 pre-training을 이용하여 모델 성능을 높인다!

## BERT: Pre-training of deep bidirectional transformers for language understanding - 모델의 구조
BERT는 Transformer encoder (bidirectional transformer)을 이용!

<img width="198" alt="스크린샷 2023-08-08 오후 3 00 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/acf6c2ab-1131-49c7-8988-dc4d44e843bc">

초록색 박스: encoder
	- input의 representation을 학습.
주황색 박스: decoder
	- generative model로 encoder의 결과 혹은 이전에 생성한 결과로 output을 생성
파란색 박스: encoder-decoder attention
	- encoder와 decoder을 연결

## BERT: Pre-training of deep bidirectional transformers for language understanding - 모델의 구조 (Embedding)
Input을 위한 Embedding은 아래와 같이 나타난다. 

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5c02c0c3-1055-4d85-9b7a-2f5f97be316a)

Token : Word piece embedding 방식 사용, 각 Char(문자) 단위로 embedding을 하고, 자주 등장하면서 가장 긴 길이의 sub-word을 하나의 단위로 만든다. 자주 등장하지 않는 단어는 다시 sub-word로 만든다. 이는 이전에 자주 등장하지 않았던 단어를 모조리 'OOV'처리하여 모델링의 성능을 저하했던 'OOV'문제도 해결.  
Segment : Sentence Embedding, 토큰 시킨 단어들을 다시 하나의 문장으로 만드는 작업. BERT에서는 두개의 문장을 구분자([SEP])을 넣어 구분하고 그 두 문장을 하나의 Segment로 지정하여 입력하여 2개 이상의 입력이 필요한 경우를 대응.  
Position : Transformer는 기본적으로 위치정보를 position encoding을 같이 넣어줘야 작동한다.

<img width="451" alt="스크린샷 2023-08-08 오후 3 08 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4769e1ac-20ca-4bb7-91ac-ca7deb222f1d">

Pre-training. -> Fine-tuning 단계로 나뉜다.  
semi-supervised pre-training 단계에서는 2가지 task를 진행한다. 
- Masked language modeling (MLM)
- Next sentence prediction (NSP)

Pretraining 단계에서는 아래의 데이터를 sentence가 아닌 document 레벨의 corpus로 사용하였다.
- BooksCorpus (800M 단어)
- English Wikipedia (2500M 단어) – 리스트, 테이블, 헤더 등 제외한 텍스트들만 사용

## BERT: Pre-training of deep bidirectional transformers for language understanding -	Step 1. Pretraining: masked language modeling (MLM)
- Bidirectional representation을 학습시키기 위해 입력 토큰들 중 일부를 masking하여 가리고, 그 가려진 토큰이 어떤 것인지 예측!
- 기존 input을 복구하는 일종의 denoising auto-encoding이다.
- 모든 실험에서 입력 토큰들을 각 sequence의 일부(15%)를 아래와 같이 변경하고, 다시 Sequence 를 예측하도록 학습한다.

  - 80%의 확률로 [MASK] 토큰을 적용한다.
  - 10%의 확률로 랜덤한 토큰을,
  - 10%의 확률로 원래의 토큰을 사용한다.

- 단순히 [MASK] 토큰/랜덤 토큰을 예측하는 과제를 수행하는 경우, [MASK] 토큰을 사용하지 않는 Fine-tuning 단계에서 방해가 될 수 있음
- 대상이 어떤 토큰인지 구분할 수 없으므로 정답을 찾을 수 없는 문제지만, 문맥을 파악하는 능력을 학습하게 됨 -> 실험적으로 좋은 성능을 보임

- Pre-training 상황에서 overfitting 을 방지하고 일반화된 정보를 학습함.
  - fine-tuning 단계에서는 [MASK]토큰이 활용되지 않고, 정상적인 문장이 들어오기 때문.
  - 따라서 [MASK]토큰이나, 랜덤 토큰 만에 대해서 최적화가 되어버리면, 이후 단계에서는 오히려 정상적인 입력 문장을 처리하는데 방해가 될 수 있는 것.

## BERT: Pre-training of deep bidirectional transformers for language understanding - Step 1. Pretraining: next sentence prediction (NSP)
QA(Question Answering), NLI(Natural Language Inference) 등 많은 downstream task들이 두 문장 사이의 관계를 학습시켜야 함.
- 하지만 기존의 일반적인 언어 모델을 직접 적용할 수 없고, 해당 Task마다 이러한 모델을 구현해야 했었다.
- BERT에서는 NSP 과정을 포함시키면서, 문장 사이의 관계를 파악하는 모델도 같이 일반화!
Binarized NSP 태스크를 수행 Binarized NSP란, 학습에서 입력할 두 문장을 선택할 때, 50%는 바로 다음에 오는 문장을, 50%는 관련 없는 문장을 선택해서 다음문장인지 아닌지를 ‘분류’.
- 이전 bidirectional을 적용시킨 MLM은 기존의 transformer보다 그냥 성능적인 향상 정도였다면, NSP는 단순히 성능뿐만 아니라, 모델 자체의 역할을 확장!
- NSP는 매우 간단한 작업이지만, QA와 NLI에 매우 효과적으로 성능 향상

  ![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/865c877c-7959-47de-966c-b97b4a36c29b)

Ablation 실험
- No NSP: MLM만 적용
- LTR & No NSP: MLM 및 NSP 둘 다 적용하지 않은 기존 좌우 모델을 사용한 경우
  <img width="420" alt="스크린샷 2023-08-08 오후 3 42 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/888675ba-4bbf-4a2d-96e8-e1ea0b5bf6bf">

## BERT: Pre-training of deep bidirectional transformers for language understanding -	Step 2. Fine-tuning
- 입력에서 pre-training 단계의 문장 A와 문장 B는 Paraphrasing (해설), Entailment (의미 함축) 에서의 전제-가설, Question answering 에서의 질문-구절, Text classification 이나 sequence tagging에서의 degenerate 등의 분야에 대한 text pair와 유사하게 구성된다.
- 출력에서는 token representation 들은 토큰레벨의 태스크에 대한 output layer에 보내고, [CLS] representation은 entailment 나 sentiment analysis 와 같이 classification에 대한 output layer로 보낸다.
- Fine-tuning 단계는 pre-training 단계에 비해서 계산 비용이 적게 발생한다. 

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/01d828a2-fe72-4289-a696-484e1c359fb2)

## BERT: Pre-training of deep bidirectional transformers for language understanding - 결과
<img width="300" alt="스크린샷 2023-08-08 오후 3 57 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/38f6928d-7111-4566-bab0-5a0b5ca25f07">
<img width="300" alt="스크린샷 2023-08-08 오후 3 57 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6e3a7393-fdd2-499e-9268-97544d98dc89">
<img width="300" alt="스크린샷 2023-08-08 오후 3 56 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/30fce905-807b-479e-aac5-9e1d76225d10">
<img width="500" alt="스크린샷 2023-08-08 오후 3 56 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/457fb990-2404-48d6-a809-26d224a5357f">

## ALBERT: LITE BERT 
BERT와 같은 Pre-trained language representation 모델은 일반적으로 모델의 크기가 커지면 성능이 향상. 
- 하지만, 모델이 커짐에 따라 아래의 문제들이 생길 수 있다!
  - Memory limitation - 모델의 크기가 메모리 량에 비해 큰 경우 학습 시, 메모리 부족 문제
  - Training time - 학습하는데 오랜 시간이 소요됨
  - Memory degradation - Layer의 수 혹은 hidden size가 너무 커지면 모델 성능 감소

<img width="229" alt="스크린샷 2023-08-08 오후 5 38 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6891dc0b-ebc3-41e1-ab8b-3b3bbd4df0ba">

Factorized embedding parameterization
- Input Layer의 Parameter 수를 줄인다. 즉, 모델 크기를 줄인다

  <img width="108" alt="스크린샷 2023-08-08 오후 5 46 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6f3bb1a0-a240-4a9e-a664-4f2c6110ff1c">

  - Input token embedding size (E) <  transformer의 각 layer input output hidden size(H)로 두면, 모델 사이즈가 커져도 성능이 떨어지지 않는다

<img width="441" alt="스크린샷 2023-08-08 오후 5 46 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/716f086f-fa96-45d5-8841-4f0643f63331">

Cross-layer parameter sharing
- Transformer의 각 Layer 간 같은 Parameter을 공유하여 사용  
-> 모델 크기 줄임 (universal transformer와 비슷한 아이디어)
	- factorized embedding parameterization 보다 cross-layer parameter sharing 적용 시 모델의 크기가 훨씬 많이 줄어든다!  
	- cross-layer parameter sharing은 말 그대로 transformer layer 간 같은 parameter을 공유하며 사용하는 것이다.
<img width="600" alt="스크린샷 2023-08-08 오후 5 53 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cebc18e8-387b-4167-a7a0-c72a54228678">

<img width="300" alt="스크린샷 2023-08-08 오후 5 53 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/dafc168f-f8ca-4556-a190-b4ae09e788f7">

## ALBERT: LITE BERT - 결과 
<img width="426" alt="스크린샷 2023-08-08 오후 6 23 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d5d0d144-9c2c-4e03-add1-cd3965d958bb">

## RoBERTa: A robustly optimized BERT pretraining approach

<img width="201" alt="스크린샷 2023-08-08 오후 6 26 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9d1dfc87-4d19-4235-8bb2-ec3c44416f93">

Bert에서 아래 내용을 개선!  
- 보다 큰 데이터로 Pretraining (160GB)
	- BookCorpus + Wikipedia (16GB)
	- CC-News (76GB) : CommonCrawl News Dataset의 영어부분으로 2016년 9월 ~ 2019년 2월 사이에 크롤링 된 6300만개의 영문 뉴스 기사  등.
	- OpenWebText (38GB) : OpenGPT2에서 사용된 WebText Corpus의 open-source 버전, Reddit에서 공유되는 URL에서 추출한 Web Contents
	- Stories (31GB) : CommonCrawl Data의 일부분으로, 이야기와 같은 스타일의 Winograd schema와 일치하도록 filtering
	- Dynamic Masking : 기존의 BERT는 data preprocessing과정에서 masking을 한번만 수행함.
		- 본 논문에서는 data을 10개 복제하여 각 sequence 40 epoch에 걸쳐 10가지 방법으로 masking되도록 처리.
		- 즉, 훈련 중 동일한 mask는 4번만 보게 되고, 더 많은 종류의 masking을 볼 수 있게 된다.
		
  	
- Model Input Format and Next Sentence Prediction
  
<img width="319" alt="스크린샷 2023-08-08 오후 6 31 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/929ac2cf-884d-4611-ba88-84b875d55f5f">
	
	- BERT의 NSP을 여러 과거연구의 의문들을 바탕으로 개선
	1. Segment-pair + NSP: 기존 NSP로, 각 입력에 여러 개 문장 포함되며 한 쌍의 segment가 있지만 512 미만의 token
	2. Sentence-pair + NSP: 인접 및 다른 document의 sentence 쌍: 1보다 길이가 작기때문에 문제 없음
	3. Full-sentence + NSP: 하나이상의 document에서 연속적으로 샘플링 (document가 나눠지면 분리하지 않음): doc 사이에 별도의 separation token추가.
	4. Doc-sentence + NSP: 입력이 document 경계 넘을 수 없다는 점 제외하고, full-sentences와 유사하게 구성. 512개보다 작을 경우, 비슷한 수의 전체 문장을 얻기 위해 동적으로 batch size조절

- 더 큰 batch size! (기존 BERT는 1M step에 대해 256 batch size)
	<img width="325" alt="스크린샷 2023-08-08 오후 6 37 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cc8d2e48-a74d-41a3-9a9b-073eaa7ef563">

## RoBERTa: A robustly optimized BERT pretraining approach - 최종 결과 요약
<img width="469" alt="스크린샷 2023-08-08 오후 6 48 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/aac3b917-95bd-4646-915a-7e18a0d463a5">

## ELECTRA: Pre-training text encoders as discriminators rather than generators
<img width="549" alt="스크린샷 2023-08-08 오후 6 54 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6734816d-87a6-4c86-bdc6-b8ca1a4af028">

- Bert의 MLM Pretrained 방법에 어떤 부분이 original이 아닌 replaced된 값인지 파악하는 추가적인 discriminator를 붙여 파악.
- Generative Adversarial Network(GAN)의 아이디어와 비슷 (TBD)
  <img width="291" alt="스크린샷 2023-08-08 오후 6 55 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5a7dc147-7e32-4621-ad51-2998267fd9cc">

## ELECTRA: Pre-training text encoders as discriminators rather than generators - 결과
<img width="467" alt="스크린샷 2023-08-08 오후 6 56 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d7a64e52-c5fd-4d2d-acbd-b847d2d2af5c">
<img width="372" alt="스크린샷 2023-08-08 오후 6 57 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/387ed87c-d479-49c7-afb1-d727b122cab8">
