# Pretrained Large-Scale Transformer - 3. Pre-trained Transformer Encoder - BERT을 중심으로
## BERT: Pre-training of deep bidirectional transformers for language understanding

<img width="604" alt="스크린샷 2023-08-08 오후 2 56 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e3c7354d-df50-41fa-b50d-117f70f292d7">

Transformer decoder을 사용하여 autoregressive한 language modeling을 하는 GPT와 다르게,  
BERT는 Transformer encoder (bidirectional transformer)을 이용하고 ELMo 와 같이 Embedding 을 수행 것을 주 목적으로 한다.  
GPT와 ELMo처럼 BERT도 pre-training을 이용하여 모델 성능을 높인다!

## BERT: Pre-training of deep bidirectional transformers for language understanding - 모델의 구조
BERT는 Transformer encoder (bidirectional transformer)을 이용!

<img width="198" alt="스크린샷 2023-08-08 오후 3 00 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/acf6c2ab-1131-49c7-8988-dc4d44e843bc">

초록색 박스: encoder
	- input의 representation을 학습.
주황색 박스: decoder
	- generative model로 encoder의 결과 혹은 이전에 생성한 결과로 output을 생성
파란색 박스: encoder-decoder attention
	- encoder와 decoder을 연결

## BERT: Pre-training of deep bidirectional transformers for language understanding - 모델의 구조 (Embedding)
Input을 위한 Embedding은 아래와 같이 나타난다. 

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5c02c0c3-1055-4d85-9b7a-2f5f97be316a)

Token : Word piece embedding 방식 사용, 각 Char(문자) 단위로 embedding을 하고, 자주 등장하면서 가장 긴 길이의 sub-word을 하나의 단위로 만든다. 자주 등장하지 않는 단어는 다시 sub-word로 만든다. 이는 이전에 자주 등장하지 않았던 단어를 모조리 'OOV'처리하여 모델링의 성능을 저하했던 'OOV'문제도 해결.  
Segment : Sentence Embedding, 토큰 시킨 단어들을 다시 하나의 문장으로 만드는 작업. BERT에서는 두개의 문장을 구분자([SEP])을 넣어 구분하고 그 두 문장을 하나의 Segment로 지정하여 입력하여 2개 이상의 입력이 필요한 경우를 대응.  
Position : Transformer는 기본적으로 위치정보를 position encoding을 같이 넣어줘야 작동한다.

<img width="451" alt="스크린샷 2023-08-08 오후 3 08 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4769e1ac-20ca-4bb7-91ac-ca7deb222f1d">

Pre-training. -> Fine-tuning 단계로 나뉜다.  
semi-supervised pre-training 단계에서는 2가지 task를 진행한다. 
- Masked language modeling (MLM)
- Next sentence prediction (NSP)

Pretraining 단계에서는 아래의 데이터를 sentence가 아닌 document 레벨의 corpus로 사용하였다.
- BooksCorpus (800M 단어)
- English Wikipedia (2500M 단어) – 리스트, 테이블, 헤더 등 제외한 텍스트들만 사용

## BERT: Pre-training of deep bidirectional transformers for language understanding -	Step 1. Pretraining: masked language modeling (MLM)
- Bidirectional representation을 학습시키기 위해 입력 토큰들 중 일부를 masking하여 가리고, 그 가려진 토큰이 어떤 것인지 예측!
- 기존 input을 복구하는 일종의 denoising auto-encoding이다.
- 모든 실험에서 입력 토큰들을 각 sequence의 일부(15%)를 아래와 같이 변경하고, 다시 Sequence 를 예측하도록 학습한다.

  - 80%의 확률로 [MASK] 토큰을 적용한다.
  - 10%의 확률로 랜덤한 토큰을,
  - 10%의 확률로 원래의 토큰을 사용한다.

- 단순히 [MASK] 토큰/랜덤 토큰을 예측하는 과제를 수행하는 경우, [MASK] 토큰을 사용하지 않는 Fine-tuning 단계에서 방해가 될 수 있음
- 대상이 어떤 토큰인지 구분할 수 없으므로 정답을 찾을 수 없는 문제지만, 문맥을 파악하는 능력을 학습하게 됨 -> 실험적으로 좋은 성능을 보임

- Pre-training 상황에서 overfitting 을 방지하고 일반화된 정보를 학습함.
  - fine-tuning 단계에서는 [MASK]토큰이 활용되지 않고, 정상적인 문장이 들어오기 때문.
  - 따라서 [MASK]토큰이나, 랜덤 토큰 만에 대해서 최적화가 되어버리면, 이후 단계에서는 오히려 정상적인 입력 문장을 처리하는데 방해가 될 수 있는 것.

## BERT: Pre-training of deep bidirectional transformers for language understanding - Step 1. Pretraining: next sentence prediction (NSP)
QA(Question Answering), NLI(Natural Language Inference) 등 많은 downstream task들이 두 문장 사이의 관계를 학습시켜야 함.
- 하지만 기존의 일반적인 언어 모델을 직접 적용할 수 없고, 해당 Task마다 이러한 모델을 구현해야 했었다.
- BERT에서는 NSP 과정을 포함시키면서, 문장 사이의 관계를 파악하는 모델도 같이 일반화!
Binarized NSP 태스크를 수행 Binarized NSP란, 학습에서 입력할 두 문장을 선택할 때, 50%는 바로 다음에 오는 문장을, 50%는 관련 없는 문장을 선택해서 다음문장인지 아닌지를 ‘분류’.
- 이전 bidirectional을 적용시킨 MLM은 기존의 transformer보다 그냥 성능적인 향상 정도였다면, NSP는 단순히 성능뿐만 아니라, 모델 자체의 역할을 확장!
- NSP는 매우 간단한 작업이지만, QA와 NLI에 매우 효과적으로 성능 향상

  ![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/865c877c-7959-47de-966c-b97b4a36c29b)

Ablation 실험
- No NSP: MLM만 적용
- LTR & No NSP: MLM 및 NSP 둘 다 적용하지 않은 기존 좌우 모델을 사용한 경우
  <img width="420" alt="스크린샷 2023-08-08 오후 3 42 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/888675ba-4bbf-4a2d-96e8-e1ea0b5bf6bf">

## BERT: Pre-training of deep bidirectional transformers for language understanding -	Step 2. Fine-tuning
- 입력에서 pre-training 단계의 문장 A와 문장 B는 Paraphrasing (해설), Entailment (의미 함축) 에서의 전제-가설, Question answering 에서의 질문-구절, Text classification 이나 sequence tagging에서의 degenerate 등의 분야에 대한 text pair와 유사하게 구성된다.
- 출력에서는 token representation 들은 토큰레벨의 태스크에 대한 output layer에 보내고, [CLS] representation은 entailment 나 sentiment analysis 와 같이 classification에 대한 output layer로 보낸다.
- Fine-tuning 단계는 pre-training 단계에 비해서 계산 비용이 적게 발생한다. 

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/01d828a2-fe72-4289-a696-484e1c359fb2)

## BERT: Pre-training of deep bidirectional transformers for language understanding - 결과
<img width="300" alt="스크린샷 2023-08-08 오후 3 57 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/38f6928d-7111-4566-bab0-5a0b5ca25f07">
<img width="300" alt="스크린샷 2023-08-08 오후 3 57 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6e3a7393-fdd2-499e-9268-97544d98dc89">
<img width="300" alt="스크린샷 2023-08-08 오후 3 56 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/30fce905-807b-479e-aac5-9e1d76225d10">
<img width="935" alt="스크린샷 2023-08-08 오후 3 56 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/457fb990-2404-48d6-a809-26d224a5357f">

## ALBERT: LITE BERT 
BERT와 같은 Pre-trained language representation 모델은 일반적으로 모델의 크기가 커지면 성능이 향상. 
- 하지만, 모델이 커짐에 따라 아래의 문제들이 생길 수 있다!
  - Memory limitation - 모델의 크기가 메모리 량에 비해 큰 경우 학습 시, 메모리 부족 문제
  - Training time - 학습하는데 오랜 시간이 소요됨
  - Memory degradation - Layer의 수 혹은 hidden size가 너무 커지면 모델 성능 감소

