# 합성곱 신경망(CNN) - 4 Recent trends of CNN models
## CNN의 발전 방향 - SeNet(2017 -2018)
- SE Block을 Convolutional layer뒤에 병렬적으로(skip connection과 함께) 붙여 성능을 개선 !

  <img width="249" alt="스크린샷 2023-06-30 오후 7 00 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/14555a04-6517-4830-9e65-5d803567cce9">

  <img width="451" alt="스크린샷 2023-06-30 오후 7 00 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e69bfb49-943b-485c-9553-acd1106ca2d2">

- 1. $F_{tr}$ : conv transformation을 수행 : 사이즈 변환된다  
  -> conv연산 ($v_c$가 필터일때):<img width="147" alt="스크린샷 2023-06-30 오후 7 09 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf3e6cd4-7c36-4ae0-84f6-f85147d88e60">
- 2. $F_{sq}$ : squeeze : U의 output의 각 채널별 정보를 global average pooling등으로 squeeze(추출)한다.  
  -> <img width="187" alt="스크린샷 2023-06-30 오후 7 11 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9b26e657-2f67-44c2-809e-af98e50a8018">
- 3. $F_{ex}$ : excitation : 스칼라의 가중치를 계산한다. $\sigma$b : sigmoid, $W_1$,$W_2$ : fully-connected, $\delta$ =ReLU
  -> $s = F_{ex}(z, W) = \sigma(g(z, W)) = \sigma (W_2 \delta (W_1 z))$
- 4. $F_{scale}$ : 스칼라 가중치를 U의 출력값에 더한다.
 
## SE block의 장점
1. 유연하다 -> convolutional net에도 붙일 수 있다.
2. 추가적인 계산량이 적다.
   - 파라미터 증가량이 조금 있으나, 성능 향상이 확실하다.
   - 파라미터가 $W_1$,$W_2$ 가 추가되지만, ratio를 작게 설정하면 파라미터 수가 작아진다.
   - convolutional layer 하나당 $2C^2 /r$개의 파라미터만 늘어난다.
       - 논문에서는 r=16일때가 보통 적절하다.
  - 2017 imagenet challenge (ILSVRC)기준 SOTA
    <img width="692" alt="스크린샷 2023-06-30 오후 7 23 14" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/de1a984c-f049-488c-b74b-9fb1b2e2af0b">
## CNN의 발전 방향 - SeNet (2017-2018)의 성능
<img width="666" alt="스크린샷 2023-06-30 오후 7 23 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c2ebc0f5-2f85-4740-a51a-24ec11d09401">

- 모든 블록 모듈이 각 모델들에 붙을 경우, 성능이 조금씩 상승!

## CNN의 발전 방향 - EfficientNet (2018-2019)
<img width="248" alt="스크린샷 2023-06-30 오후 7 26 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9d05f39c-d335-44a4-9d2c-3fdeb3d8ef73">
- 모델이 항상 깊은 게 좋은 것이 아니다 ! 
- 적절한 깊이의 튜닝을 잘하면 모델 성능이 충분히 나오고 SOTA에 근접하다!
- 발견 및 실험: AutoML(Hyper Parameter Optimization; HPO)를 도입하여 search space내에서 최적의 값을 찾음

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fdca6bfb-2e6b-41b2-bd88-a6dabb789337)

<img width="618" alt="스크린샷 2023-06-30 오후 7 28 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6c387ba8-0399-40a2-8d64-c0b3dd43c745">

- 모델의 Scaling에 대해 여러가지 관점으로 실험 & 분석
- 80% 까지는 Top1 Acc. 가 상승하지만, 
- 특히 depth의 경우 그 이후에는 saturation된다. 

<img width="280" alt="스크린샷 2023-06-30 오후 7 29 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bba79577-ecc1-439a-a43a-95f3d11d627c">

- Depth와 Resolution을 고정하고, width만 조절하여 성능 그래프를 보면…
- “Resolution”을 scaling하는 것이 ”Depth”보다 영향을 크게 준다는 것을 파악할 수 있다!

-  ResNet, DenseNet보다 빠르고, 가볍고 성능이 좋다
    - 참고: FLOPS: Floating point operations per seconds로 속도의 지표.
      
<img width="362" alt="스크린샷 2023-06-30 오후 7 30 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/49e2f836-1cd0-4850-b7fb-34a08d03e2e5">

<img width="356" alt="스크린샷 2023-06-30 오후 7 32 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf14fab8-3db2-4f09-b5d4-4d7af1c2317d">

## CNN의 발전 방향 - self-training(자가 학습 기법)과 Data augmentation (2019)
- EfficientNet이 등장한 이후, 발전 방향은 모델 보다, 데이터나 학습 과정을 개선하는 것으로 트랜드가 바뀌었다.
- 2019년 SOTA였던, Touvron, Hugo, et al. "Fixing the train-test resolution discrepancy." arXiv preprint arXiv:1906.06423 (2019)은, 
- EfficientNet의 data augmentation기법에 맞춰 train/test에 사용하는 augmentation을 할때에 맞춰 모델의 scaling을 개선한 방법이었다!

<img width="364" alt="스크린샷 2023-06-30 오후 7 33 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e6950112-72d8-42bf-980f-1e9540a216d3">
