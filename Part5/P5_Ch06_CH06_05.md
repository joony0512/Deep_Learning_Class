# 순환 신경망 (RNN) - 5. 주의 집중 기반 순환 신경망 (Attention-based RNN)

## 인코더-디코더 기반 기계 번역 (Machine Translation)의 문제
<img width="235" alt="스크린샷 2023-07-13 오후 10 21 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/64150280-7e68-4ad9-8868-b99fe5e538a8">
<img width="592" alt="스크린샷 2023-07-13 오후 10 21 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5c7c4155-997d-4520-8c01-1f344b572e22">


- 인코더-디코더 기반의 기계 번역(seq2seq)은, 입력 문장에서 필요한 모든 정보를 고정된 길이의 vector로 변환하였다.
- 그러나, 이는 입력 문장이 길어짐에 따라 퍼포먼스가 심각하게 감소하는 문제를 발생시켰다.
- LSTM, GRU 등도 완전히 long-term dependency를 해결하지 못하였다

## 기계번역을 위한 주의 집중(attention) 기반 RNN
<img width="186" alt="스크린샷 2023-07-13 오후 10 22 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d5f67898-4759-42dc-ab33-ce740aaca51b">

- 주의 집중(attention) 모델 : 2014~ 2015년
- encoder-decoder모델에서, 인코더의 input상태들에 대한 디코더 output의 가중치 조합(weighted combination)을 생성한다.
- 왜 주의 집중 신경망이라고 부를까?
  - 이 combination weight $\alpha_t$는 네트워크의 아웃풋이 input signal의 관련된 부분에 집중할 수 있게 도와주기 때문에, '주의 집중'(attention)매커니즘이라고 부른다.
    
<img width="395" alt="스크린샷 2023-07-13 오후 10 28 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f4734c53-cc56-4d0b-924b-9540246b2e9e">

- 주의 집중 기반의 모델은 long-term dependency를 좀더 완화시킬 수 있는 robust한  실험 결과를 나타내었다.

## 기계번역을 위한 주의 집중(attention) 기반 RNN
- Attention모델은 attention weight을 visualization하여 검증할 수 있다.

<img width="839" alt="스크린샷 2023-07-13 오후 10 30 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5e530b76-0f45-45ae-8d8b-58542cf2b17c">


## 기계번역을 위한 주의 집중(attention) 기반 RNN - Attention의 기본 메커니즘
- 일반적인 attention 은 오른쪽과 같다.
- Key, Query 값으로 “attention score”을 계산하고, 이를 종합하여  attention weights을 계산한다.
- 이 attention weights을 실제 Value 값에 곱해 어떤 값에 “집중”할지를 결정한다.
- RNN 기반의 sequence-to-sequence의 경우,
- Query는 t 시점의 ‘디코더(decoder)’ 셀에서의 은닉 상태(hidden state), Key와 Value는 모든 시점의 ‘인코더 셀의 은닉 상태’ 이다!
  
<img width="423" alt="스크린샷 2023-07-13 오후 10 31 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/09343c0f-fe65-4558-99ba-23d49da3e4a7">

## 기계번역을 위한 주의 집중(attention) 기반 RNN - Attention의 기본 메커니즘
<img width="205" alt="스크린샷 2023-07-13 오후 10 35 25" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/60951764-d1c4-463f-8456-3f19a8d523e0">
- x 시퀀스의 길이를 n, y의 시퀀스의 길이를 m이라고 하자.
- 왼쪽의 그림에서 encoder의 hidden state는 <img width="221" alt="스크린샷 2023-07-13 오후 10 50 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7ee67653-f611-430a-9647-3fa73b21c253">
- decoder 네트워크의 t = 1,...,m에서의 hidden state는 t에서의 context vector c_t가 주어졌을때, <img width="143" alt="스크린샷 2023-07-13 오후 10 52 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/476bff9f-e10e-4490-acb1-d02a1dabf8cc"> 이다.
- 이때 c_t는 각 인코더의 input으로 부터 받아온 값이다. 즉 <img width="105" alt="스크린샷 2023-07-13 오후 10 53 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fe426ef8-60bc-4a49-bde8-45d2fc9b9cd9">
- y_t셀과 x_i셀이 관련된 정도를 나타내는 attentional weight $\alpha_{t,i}$는 <img width="290" alt="스크린샷 2023-07-13 오후 10 54 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6d96983f-42a6-40c7-8e17-96d1e66f01fb"> 와 같이 계산하며, 이는 미리 정의 하였던 score함수의 softmax값과 동일하다.
- score 함수는 논문마다 다르며 여러종류가 있다.

## 기계번역을 위한 주의 집중(attention) 기반 RNN - Attention 의 종류 (score 함수에 따라)

<img width="774" alt="image" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/aa2cde0a-9a90-44df-abf1-966eec808990">

<img width="640" alt="스크린샷 2023-07-13 오후 10 57 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e0bb515d-7742-458e-97ec-8343c4c5b771">

## Attention의 종류 - Global vs Local 
- Global attention: 모든 encoder의 은닉 상태(hidden state)를 사용하는 attention
- Local attention: encoder의 은닉 상태의 하위 집합(subset)만을 사용하는 attention 

<img width="473" alt="스크린샷 2023-07-13 오후 10 58 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/110058ff-a780-4c3a-b329-ef93ebb360fe">

## Attention 기반 seq2seq의 응용 - 음성 인식 (speech recognition; speech to text)
<img width="350" alt="스크린샷 2023-07-13 오후 10 59 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c93c4be0-c44f-48b9-9325-ea806dd6f945">
<img width="413" alt="스크린샷 2023-07-13 오후 10 59 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a304c16b-1289-429b-af7b-8923535df9f2">

## Attention 기반 seq2seq의 응용 - 음성 합성 (speech synthesis; text to speech)
<img width="198" alt="스크린샷 2023-07-13 오후 10 59 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e4b07919-2942-4647-99a7-e24d6b9285c0">
<img width="420" alt="스크린샷 2023-07-13 오후 11 00 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d10a1c01-5ad1-4a4d-808f-e24b4517c0c2">

## Attention 기반 seq2seq의 응용 - Image annotation (captioning) 
- 각각의 Word를 생성할 때 해당하는 이미지와 관련된 부분에 집중하도록 한다. 

<img width="423" alt="스크린샷 2023-07-13 오후 11 00 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/99fdccbe-eac3-4af9-b20a-518319c33693">

<img width="477" alt="스크린샷 2023-07-13 오후 11 01 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9ce026fe-3322-476a-a1a4-a07cb5381dc8">
