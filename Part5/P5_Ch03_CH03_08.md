## CH03_08-Regularization-AdversarialLearning

### Adversarial Training(적대적학습)
![Untitled (23)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/65645bcc-df96-42dd-962a-0f019dfcf00b)


- 사람이 관측할수 없을정도의 작은 노이즈를 넣으면, 완전 다른 클래스가 나올 수 있다.
- 입력은 아주 조금 바뀌었으나, 출력이 매우 달라지며, 그때의 기울기가 매우 가파르다.
    - 일종의 오버피팅상태. 이런현상을 생각해 노이즈를 섞어 학습을 진행한다면? → Regularization
- 우리가 각 인풋을 노이즈를 섞어 $\epsilon$만큼 바꿀때, $\epsilon||w||_1$변화한다고 할 수 있다.
- 이때, w가 만약 높은 차원이라면 그 값은 매우 커질 수 있따.
- 적대적학습은 로컬 상수값(노이즈)을 추가한 데이터를 학습하여, 이러한 locally linear behavior 로 인해 생기는 네트워크의 민감성(sensitivity)을 줄일 수 있다.
- 적대적학습을 위해 예를들어 fast gradient방법은 아래와 같이 만들어진 데이터를 생성할 수 있다.
    - $w^T\tilde{x} = w^Tx + w^T\eta$
    - $\eta = \epsilon sign(\nabla_xJ(w))$
- 즉, w근처에 gradient에 선형값에 linear벡터값을 인풋값에 더해 adversarial example을 생성한다.
- 이를 목적함수에 반영해 학습을 진행한다.
    - $\tilde{J}(w,x,y) = \alpha J(w,x,y) + (1-\alpha)J(w,x+\epsilon sign(\nabla_x J(w,x,y)),y)$
