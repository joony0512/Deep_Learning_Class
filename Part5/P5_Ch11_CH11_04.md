# <Generative model series #3> - Latent Variable Models - 4. Variational Autoencoder
## 생성 모델을 위한 기존 auto-encoder는?

- 오토 인코더(auto-encoder)는 input 을 그대로 output로 복사하 듯 생성하는 것을 목표로 하는 신경망이었다!
- Bottle-neck 구조를 취해 의미가 있는 representation 을 뽑아내는데 좋은 구조였다! 
- 생성 모델로서의 오토 인코더는? NO!
  - Auto-encoder들로부터 배우는 space들은 연속적이지 않다. (discontinuous 하다.)
  - 불연속적인 공간에서 샘플링을 한다면, decoder가 생성하는 output은 현실적이지 않고 그 영역에 대해 처리하는 것이 힘들어진다.
  - 또한 Interpolation부분의 generation에 취약하다.

<img width="300" alt="스크린샷 2023-08-11 오후 2 58 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e0ea0fbd-58ff-458e-b2fb-f687d6f7c5f7">
<img width="300" alt="스크린샷 2023-08-11 오후 2 58 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f0d6e79d-f6d1-479e-9f25-509074b32027">

## Variational Auto-encoder(VAE) - 확률적(stochastic)인 encoder와 decoder!
<img width="496" alt="스크린샷 2023-08-11 오후 2 59 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4cbbbd3c-e7ea-435c-baae-168f89a0c70a">
<img width="270" alt="스크린샷 2023-08-11 오후 2 59 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fd8a4a40-3d52-49c6-a0fc-35e802faa7e1">

- 연속적인 잠재 공간(latent space)을 갖게 해주고, sampling과 interpolation이 쉬워진다!

## Variational Auto-encoder(VAE) - Reparameterization trick(pathwise derivative)
VAE역시 path-wise derivative (PD)! (reparameterization trick; 재매개변수화)

<img width="500" src= "https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f6133392-5a25-4e24-b42b-eb59ccdc12aa">
<img width="500" alt="스크린샷 2023-08-11 오후 3 09 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/69036c24-e58a-4dbb-a874-f60f06644a9d">

## Variational Auto-encoder(VAE) - 확률적(stochastic)인 encoder와 decoder
<img width="532" alt="스크린샷 2023-08-11 오후 3 09 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5719f72b-8189-4d52-bb8f-261c9e4be387">

## Variational Auto-encoder(VAE)의 학습
VAE는 path-wise derivative(PD)가 variational inference에 적용된 구조임을 기억하자.
- 인코더 네트워크 $\mu, \sigma$ 파라미터를 가진 $q_{\phi}(z|x)$는 데이터 x를 입력으로 받아 잠재 변수 z의 분포의 모수를 근사한다.
  - 이때, $\phi$는 인코더의 파라미터이다.
- 한편 생성모델인 DNN 디코더는 $p_{\theta}(x|z)$ 로 미분가능한데, z가 다음 노이즈를 따른다고 하면,

  <img width="381" alt="스크린샷 2023-08-11 오후 3 20 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c3ae39e4-ce7e-4696-b673-ee6263a1ed18">가 될것이다.
  - 이때 위 VLB의 gradient계산은 SGD로 $\nabla_\phi, \nabla_\theta$ 둘다 효율적으로 계산될수있다. 

## Variational Auto-encoder(VAE)의 학습
요약하자면
- VAE는 gaussian prior p(z)와 근사된 posterior p(z|x)을 가진 latent variable model이다.
- VAE의 최종 loss식 (VAE objective ; negative -log likelihood; VLB의 최대화) L($\theta, \phi;x_i$)는 아래와 같이 정의된다.

  <img width="466" alt="스크린샷 2023-08-11 오후 3 41 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2504d3bb-2eb7-4689-ac89-af67a47c7da0">

## Variational Auto-encoder(VAE)의 결과
<img width="590" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7cd1f1d8-2f3a-407f-8e7e-3fdfbc361424">
<img width="590" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9b7e4808-7a41-42ea-9ae2-5f4e865a396e">
<img width="590" alt="스크린샷 2023-08-11 오후 3 46 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ee093811-02e7-4e0b-9fe6-18b331d699ac">


## Variational Auto-encoder(VAE) - 요약, Pros & Cons
Auto-encoder를 연속적인 확률로서 확장 -> 생성모델로 활용가능  
intractable density로 정의 -> VLB(variational lower bound)를 유도하고 이를 대신 최적화  

Pros
- 생성모델로의 체계적이고 이론적인 backgrounds
- q(z|x)의 inference가 가능하다.
  - 다른 테스크의 유용한 representationdmf encoding하는 목적으로 사용가능.

Cons
- 목적함수를 직접 최적화하는 것이 아닌, VLB를 간접적으로 최적화.
  - 일반적으로 Auto-regressive모델에 비해 좋은 평가법이 아님.
- GAN에 비해 생성된 모델이 blur한 효과가 있고 낮은 퀄리티처럼 보임.

어떻게 발전시킬 수 있을까?
- Variational inference을 개선
- Decoder를 좀더 flexible하게 만들자. (굳이 encoder와 같지 않아도 된다)
- 좀더 모델 아키텍처를 생성을 잘하도록개선
