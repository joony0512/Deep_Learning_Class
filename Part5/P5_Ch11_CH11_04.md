# <Generative model series #3> - Latent Variable Models - 4. Variational Autoencoder
## 생성 모델을 위한 기존 auto-encoder는?

- 오토 인코더(auto-encoder)는 input 을 그대로 output로 복사하 듯 생성하는 것을 목표로 하는 신경망이었다!
- Bottle-neck 구조를 취해 의미가 있는 representation 을 뽑아내는데 좋은 구조였다! 
- 생성 모델로서의 오토 인코더는? NO!
  - Auto-encoder들로부터 배우는 space들은 연속적이지 않다. (discontinuous 하다.)
  - 불연속적인 공간에서 샘플링을 한다면, decoder가 생성하는 output은 현실적이지 않고 그 영역에 대해 처리하는 것이 힘들어진다.
  - 또한 Interpolation부분의 generation에 취약하다.

<img width="300" alt="스크린샷 2023-08-11 오후 2 58 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e0ea0fbd-58ff-458e-b2fb-f687d6f7c5f7">
<img width="300" alt="스크린샷 2023-08-11 오후 2 58 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f0d6e79d-f6d1-479e-9f25-509074b32027">

## Variational Auto-encoder(VAE) - 확률적(stochastic)인 encoder와 decoder!
<img width="496" alt="스크린샷 2023-08-11 오후 2 59 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4cbbbd3c-e7ea-435c-baae-168f89a0c70a">
<img width="270" alt="스크린샷 2023-08-11 오후 2 59 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fd8a4a40-3d52-49c6-a0fc-35e802faa7e1">

- 연속적인 잠재 공간(latent space)을 갖게 해주고, sampling과 interpolation이 쉬워진다!

## Variational Auto-encoder(VAE) - Reparameterization trick(pathwise derivative)
VAE역시 path-wise derivative (PD)! (reparameterization trick; 재매개변수화)

<img width="500" src= "https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f6133392-5a25-4e24-b42b-eb59ccdc12aa">
<img width="500" alt="스크린샷 2023-08-11 오후 3 09 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/69036c24-e58a-4dbb-a874-f60f06644a9d">

## Variational Auto-encoder(VAE) - 확률적(stochastic)인 encoder와 decoder
<img width="532" alt="스크린샷 2023-08-11 오후 3 09 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5719f72b-8189-4d52-bb8f-261c9e4be387">

## Variational Auto-encoder(VAE)의 학습
VAE는 path-wise derivative(PD)가 variational inference에 적용된 구조임을 기억하자.
- 인코더 네트워크 $\mu, \sigma$ 파라미터를 가진 $q_{\phi}(z|x)$는 데이터 x를 입력으로 받아 잠재 변수 z의 분포의 모수를 근사한다.
  - 이때, $\phi$는 인코더의 파라미터이다.
- 한편 생성모델인 DNN 디코더는 $p_{\theta}(x|z)$ 로 미분가능한데, z가 다음 노이즈를 따른다고 하면,

  <img width="381" alt="스크린샷 2023-08-11 오후 3 20 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c3ae39e4-ce7e-4696-b673-ee6263a1ed18">가 될것이다.
  - 이때 위 VLB의 gradient계산은 SGD로 $\nabla_\phi, \nabla_\theta$ 둘다 효율적으로 계산될수있다. 
