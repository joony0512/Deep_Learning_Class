## CH04_02_경사하강법과뉴턴방법

## 경사하강법(gradient descent method)

하강하는 방향과 크기, 한번에 얼마나 크게 움직일지 결정하고 그 방향으로 이동한다.

- 시작점  x가 f도메인구간에 있다고 가정하고, 다음 3개 연산을 반복한다.
    1. $\Delta x := -\nabla f(x)$
    2. 라인서치(line search)를 한다.
    → 걸음의 크기(step size) t를 고르는 과정 → 매우 중요
    3. 계산값을 업데이트 한다 → $x := x +t\Delta x$
    - 이 반복과정을 멈추는 기준(stopping point)까지 진행한다.
    $||\nabla f(x)||_n \le \epsilon$

## Line search

- Step size를 잘 고르는 것은 중요하다.
    ![Untitled (56)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/958de7bc-3637-4875-bcf9-ba6fb5dfb93f)

    
- 얼마나 한번에 많이 움직일지 결정한다 (step의 크기를 결정한다)
    - Exact line search : $t= arg\ \underset{s\ge0}min f(x+s\Delta x)$
    - Backtracking line search : Inexact line search방법의 일종
        - 경사도 방향(gradient direction) $\Delta x$가 주어졌고
        → $x\in dom \ f, \alpha \in (0, 0.05), \beta \in (0,1), t=1일때,$
        $while\ f(x+t\Delta s) > f(x) + \alpha t\nabla f(x)^T \Delta x$
        $t:=\beta t$
        
        ![Untitled (60)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8f5d418f-1a08-4827-afd5-fe47e89abc1e)

            
     - Exact line search vs Backtracking line search  
![Untitled (57)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4a904497-f95d-43e5-a117-581c476570c2)


            

## 확률론적 경사하강법(stochastic gradient descent)

일반 gradient descent의 문제

- 데이터가 많고 모델이 복잡한 일반적인 경우에는, 실제 경사도 계산에 시간이 매우 많이 소요된다. → 가령 아래문제를 최소롸하는 최적화 문제를 고민해보자.
$\underset{x}min \underset{i=1}{\overset{m}\Sigma} f_i(w)$ 
위함수 $f_i$에 대해 gradient를 구해서 합해야 한다고 하면,
 $\nabla \underset{i}{\overset{m}\Sigma}f_i(w) =\underset{i}{\overset{m}\Sigma} \nabla f_i(w)$ 이고 gradient descent를 단순히 적용하면,
$w^k = w^{k-1} - t_k\underset{i}{\overset{m}\Sigma}\nabla f_i(w^{k-1}), k =1,2,3,...$이다.
- 이는 즉 모든함수 모든데이터에 대해 계산하는 것이므로 오버헤드가 발생한다.
- 따라서 k번째 iteration에서 하나의 함수 인덱스 $i_k$를 선택해서 업데이트한다.
즉 랜덤으로 선택(샘플링)한 예제 먼저 반영하여 업데이트한다.→ stochastic
    - $w^k = w^{k-1} - t_k\nabla f_{ik}(w^{k-1}), i_k = {1,2,3,...m}$
    - 함수인덱스는 순환적으로 $i_k$ =1,2,…m,1,2… 를 고르거나
    - 또는 랜덤으로 $i_k \in$ {1,2,…m} 둘다가능  
![Untitled (58)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bd4a67b1-eace-4a9f-bffb-ae3e2c7cf4cf)

        
- gradient descent(GD) vs stochastic gradient descent(SGD)
    - GD
        - 모든 데이터를 계산한다.
        - 최적의 한스텝을 나아간다
        - 확실한데 너무 느리다 → ML에 적용하기 까다로움
    - SGD
        - 일부데이터만 계산한다.
        - 빠르게 전진한다
        - Scalable하다 → 빠르게 수렴한다
        - ML방법들과 잘 맞는다.
        - Local minima문제를 해결한다
        
        → SGD의 랜덤샘플링이 Local optima의 탈출을 도와준다.
        

## 2차근사

이계도함수의 행렬 값인 Hessian을 컴퓨팅하여 최적화하면, gradient값을 취한 것 대비, 수렴이 빠를 수 있다. 

$\hat{f}(x+p) \cong f(x) +\nabla f(x)p +$ $1\over2$ $p^TH(x)p$
![Untitled (54)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5b936b92-1eea-4429-8452-ae481bdf8338)


→ 연산과정 매우 복잡해서 느림

## Newton step

f에 대한 2차 Taylor approximation $\hat f$는 $\hat{f}(x+p) \cong f(x) +\nabla f(x)p +$ $1\over 2$$p^TH(x)p$일것이다.

Newton step은 p에 대해 미분하여 0이 되는 점을 찾고 정리하면, 

$p=\Delta x_{nt}=-H(x)^{-1}\nabla f(x)$ 가 된다.

이때, Newton decrement (x가 이상적인값인 $x^*$에 가까워지는 지표, $f(x)-f^* =$$1\over2$$\lambda(x)^2$) $\lambda$는
$\lambda (x) =(\nabla f(x)^T H(x)^{-1}\nabla f(x))^{1/2}$이다.

![Untitled (55)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/699eff8a-aac2-428e-a7da-b1dc96d5b89c)

**알고리즘**

시작점 x가 f도메인구간 ($x\in dom\ f)$에 있고, 멈추는 지표인 $\epsilon >0$이 있을때,
다음 4개를 반복한다.

1. Newton Step과 decrement계산
→ $x_{nt}=-H(x)^{-1}\nabla f(x)$, $\lambda^2 =\nabla f(x)^T H(x)^{-1}\nabla f(x)$
2. 멈출지 계속할지 기준에 따라 계산.
→ $\lambda^2 \over2$$\le \epsilon$이면 멈춘다.
3. Line search한다. step size t를 backtracking line search등으로 고른다.
4. 계산값을 업데이트한다 → $x:=x+t\Delta x_{nt}$
- 만약 $\Delta x_{nt} =-|H(x)|^{-1} \nabla f(x)$라면, 실제로는 DNN학습에는 많이 쓰이지 않는다.
- $\Delta x_{nt} =-|H(x)|^{-1} \nabla f(x)$=$-f'(x)\over f''(x)$에서 gradient뿐만아니라 Hessian도 미리 알고있어야 하기 때문이다.
    - 이때 $H^{-1} -> O(d^3) complexity(d=dimx)$로 매우 느리다.
    - 이를 근사하여 Hessian 대신 다른 근사값을 구해 계산할 수 있다.
    - 예) BFGS → 이역시 메모리를 $O(d^2)$만큼 먹기때문에, DNN에 유효하지 않을 수 있다.
        - 이때 limited-memory BFGS사용한다.
