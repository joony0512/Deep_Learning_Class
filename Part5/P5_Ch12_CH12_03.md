 # <Generative model series #4> Implicit Models - 3. Generative Adversarial Network(GAN)
 ## Generative Adversarial Network (GAN)
- 2개의 모델이 경쟁적으로 학습할 수 있지 않을까? (게임 이론과 같이)
- 2014년 Ian Goodfellow에 의해 제안.
  - min max 𝑉 (𝐷, 𝐺) = $𝐸_{x \sim {p_{data}(x)}}$ [log 𝐷 (𝑥)] + $𝐸_{z \sim {p_{z}(z)}}$ [log(1 − 𝐷(𝐺 (𝑧) ))] 
- 2개의 플레이어(생성 모델-generative model(G)와 식별 모델 -discriminative model(D))가 있다고 해보자.
- (D)는 데이터가 진짜(1)인지 생성 모델이 만든 가짜(0)인지 구분하는 binary classifier이다.
- (G)는 생성하는 샘플의 log-probability을 최소화! -> (D)에 의해 가짜로 분류되는 샘플

<img width="500" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/25255982-1d15-478c-9df4-8549e1df971c">

이 게임의 최종 결과는 ? (Nash equilibrium)
- $p_{data}(x) = p_g (x), \forall x$
- 𝐷 (𝑥) = 1/2 , ∀ 𝑥

<img width="162" alt="스크린샷 2023-08-20 오후 5 01 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f5e8796e-44ea-4ea0-81e2-83e4e987a1da">

## Generative Adversarial Network (GAN) - 최적의 discriminator(목적 함수)
<img width="350" alt="스크린샷 2023-08-20 오후 5 05 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8536db77-7033-4102-ae8a-e9d4493b2d56">

Generator를 고정하고 discriminator의 최적 값을 찾으면

<img width="400" alt="스크린샷 2023-08-20 오후 5 02 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0f933d55-20ba-4abe-8961-e4558994c4fa">

여기서 <img width="80" alt="스크린샷 2023-08-20 오후 5 03 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2b9b56ab-8fa4-4403-8cd4-1d94063002ff"> 이되는때를 찾아 최적의값을찾는다.

<img width="300" alt="스크린샷 2023-08-20 오후 5 04 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cf48fed0-5689-46c0-967d-57bad64ac866">

## Generative Adversarial Network (GAN) - 최적의 discriminator & generator(목적 함수)
𝑉 (𝐺, 𝐷) 에 <img width="150" alt="스크린샷 2023-08-20 오후 5 07 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/97629e2f-99ee-41c4-b90c-b5d628c03852"> 을 대입하고 정리하면,   

<img width="450" alt="스크린샷 2023-08-20 오후 5 07 57" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/36e5cc93-fa52-4f2d-bfe4-c521bcd817ac">

-> $p_{data}, p_g$ 에 대한 2*Jensen-Shannon divergence (JSD)이고 >=0 이다!  
-> Divergence이므로, 성질에 따라 $p_{data} = p_g$ 일 때 최소이고 최소값은 0!   
-> 𝑉 𝐺∗,𝐷∗ = −log4 이다!

## Generative Adversarial Network (GAN) - Jensen-Shannon Divergence(JSD) vs KL Divergence vs Maximum mean discrepancy (MMD)
<img width="400" alt="스크린샷 2023-08-20 오후 5 10 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/230cb553-30b4-4a39-88ed-8bf88d83a0cc">    
     
<img width="400" alt="스크린샷 2023-08-20 오후 5 10 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6f934076-1e11-49c9-8a1d-554382379d18">   
  
<img width="300" alt="스크린샷 2023-08-20 오후 5 11 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b6384801-1f85-4c79-9849-b26ddd5f420b">    

## Generative Adversarial Network (GAN) - Generator, discriminator의 목적 함수와 업데이트
<img width="300" alt="스크린샷 2023-08-20 오후 5 14 11" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/af9de431-c1a0-4914-a3c5-84c8cd63e677">

에서 discriminator와 generator에 대해 목적함수를 각각 정의하면,

<img width="525" alt="스크린샷 2023-08-20 오후 5 15 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1ccdd760-8d6c-4fd8-a088-10edaa73f312">
이다.

- 목적 함수에 따라 패러미터는 매 스텝마다 아래와 같이 업데이트 된다.

  <img width="207" alt="스크린샷 2023-08-20 오후 5 15 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b316b0c7-3ba2-4bd8-975d-0bf7c7f0770e">

## Generative Adversarial Network (GAN) - 학습과정
<img width="450"  src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/dd94cc09-03b9-4ee0-ad62-25083af50b61">

## Generative Adversarial Network (GAN) - Discriminator saturation (vanishing gradient problem)
<img width="300" alt="스크린샷 2023-08-20 오후 5 24 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1325bfa7-230b-4e8a-967f-40c323c72a6a">

에서 Generative 부분 <img width="250" alt="스크린샷 2023-08-20 오후 5 25 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a40d4b47-9d3a-4c70-90aa-644238921092"> 을 그대로 최적화하면?

<img width="250" alt="스크린샷 2023-08-20 오후 5 26 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d21e5049-291f-409c-90f2-1b7e22ba121e">

이때, 𝐷(𝑥)는 binary classification이기 때문에 sigmoid을 쓴다.

<img width="190" alt="스크린샷 2023-08-20 오후 5 26 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/64063732-231a-4d98-ad61-3ff3200ac281"> 라면,

<img width="493" alt="스크린샷 2023-08-20 오후 5 27 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fb3c2768-88ba-47fc-8f50-542b281b24dc">이 된다.

만약, discriminator가 confident하다면 위의 값은 0이 되어, generator가 업데이트가 되지 않는 문제가 생긴다. 이를 discriminator saturation 문제라고 한다!

## Generative Adversarial Network (GAN) - Discriminator saturation이 해결된 GAN
<img width="300" alt="스크린샷 2023-08-20 오후 5 28 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/db8fe76b-2868-40b2-9a0f-1f16df234028">

에서 Generative 부분의 목적 함수 <img width="250" alt="스크린샷 2023-08-20 오후 5 28 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/67466bf0-a1e6-40ae-bd6c-6517c6f2be63"> 의 최소화 대신

<img width="300" alt="스크린샷 2023-08-20 오후 5 29 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6afe4f98-486e-4133-9a96-fa276eaaa077"> 을 최소화하여 최적화 한다!

(discriminator는 그대로 유지)

- 교정 전처럼, 두 업데이트가 zero-sum 게임을 하고 있다면, balancing을 하는 것은 매우 어려울 것이다 !
- GAN에서는 위의 교정으로 adversarial(적대적)로 게임을 정의하여 non-zero sum을 만들어 최적화를 용이하게 개선할 수 있었다! - Discriminator saturation 문제를 해결

## Generative Adversarial Network (GAN) - 결과
<img width="350" alt="스크린샷 2023-08-20 오후 5 50 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/50495687-c974-43ff-aacf-4dfc5dd1eff9">

## Generative Adversarial Network (GAN) - GAN을 평가하는 방법
- GAN을 평가하는 방법은 아직 열린 문제이다.
  - Why? : Explicit한 문제와 다르게, likelihood을 test-set에 대해 평가할 수 없기 때문!
- 평가에 무엇이 중요할까?
  1. Fidelity: 이미지의 퀄리티
  2. Diversity: 이미지의 다양성
GAN의 metrics들
- Parzen-Window density estimator
- Inception Score
- Frechet Inception Distance
- Sampling & Truncation
- HYPE (Human eYe Perceptual Evaluation)

## Generative Adversarial Network (GAN) - GAN을 평가하는 방법 ‒ Parzen-Window density estimator
- Kernel density estimator 라고도 알려져 있다!
- Estimator는 kernel K 함수와 bandwidth h로 이루어져 아래 식을 만족한다.

  <img width="150" alt="스크린샷 2023-08-20 오후 5 55 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/977d0aa2-f8f7-4bcf-a08d-7a6f35ca7535">

  <img width="150" alt="스크린샷 2023-08-20 오후 5 54 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/aa5eb530-7470-45ba-a87e-4dcd7883b4b5">

- 생성 모델 평가에서,
  - K는 표준 정규 분포의 밀도 함수로 선택된다.
  - H는 validation set의 분석을 통해 선택한다.

<img width="300" alt="스크린샷 2023-08-20 오후 5 56 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/23662a7c-862d-40ef-a895-fa7e064aad15">

- 그러나 정확하지 않을 수도 있다

  <img width="450" alt="스크린샷 2023-08-20 오후 5 57 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5a3b943b-2fcf-4648-a425-c6a371c8e256">

## Generative Adversarial Network (GAN) - GAN을 평가하는 방법 ‒ Inception score
- Inception p(y|x) classifier 모델 (i.e. pre-training된 inception v3 네트워크사용)
- marginal label distribution (label 들의 확률 분포) <img width="150" alt="스크린샷 2023-08-20 오후 5 58 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/225118a0-7479-42f0-b2db-15495d406964">
- 위 두 분포 간 KL divergence expectation의 지수 제곱

  <img width="400" alt="스크린샷 2023-08-20 오후 5 59 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/21ff7724-3373-4971-b3be-b10069b32a98">

- 가장널리사용되는지표중하나!

## Generative Adversarial Network (GAN) - GAN을 평가하는 방법 ‒ Frechet Inception Distance
<img width="300" alt="스크린샷 2023-08-20 오후 6 05 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6f0b91bb-d94f-4f19-aab8-4f8111623ea8">

- 낮은 FID -> 두 분포의 거리가 가깝다 : 진짜와 가짜가 유사하다!

  <img width="400" alt="스크린샷 2023-08-20 오후 6 14 39" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4f13c14c-906a-468e-bb70-8b2103e6e84d">

- 단점?
  - ImageNet data로 pre-trained된 경우 ImageNet data 이미지의 class와 다른 이미지를 다룰 경우 원하는 특징을 포착하지 못할 수있다.
  - Pre-trained가 아닌 경우, 많은 수의 sample로 학습시키지 않으면 biased feature layer가 생겨서 좋지 못한 FID score를 얻게된다
  - Pre-trained가 아닌경우, 학습을 시키기 위해 오래걸린다.
  - 표본의 분포가 정규 분포가 아닐 경우 제한적인 통계량(평균,분산)만으로는 분포의 차이를 잘못 설명할 수 있다.

## Generative Adversarial Network (GAN) - Wrap-up: GAN의 장단점
장점
1. 이미지 생성 시간(sampling)이 빠르다.
2. Inference가 따로 없다.
3. 이미지 품질이 좋다. (해상도 문제는 기본 GAN은 있다! -> 추후 발전되며 해결!)

단점
1. 본질적인 evaluation metrics 부족
2. 학습의 불안정성 (i.e. mode collapse 문제) -> 추후 어느 정도 해결됨
  - Mode collapse: 각 mode들을 번갈아 돌아가면서 전체적인 데이터 분포를 찾지 못하고 한 번에 하나의 mode에만 weight을 줘, 학습이 잘 되지 않는 형태.
3. 명시적인확률밀도를알수없다.
4. latent vector로 역 변환이 어렵다.
