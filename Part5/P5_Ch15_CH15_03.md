# Deep Reinforcement Learning - 3. Deep RL의 발전 - Value based Model free RL(DQN)

## 강화 학습의 구분 - Model-free RL에서 무엇을 배우느냐?
Model-free RL에서는 크게 2가지로 나눌 수 있다. 
1. Value-based (Q-learning)
2. Policy-based (policy optimization)
   
만약 value function이 완벽하다면 최적의 policy는 자연스럽게 얻을 수 있다!
  - 각 state에서 가장 높은 value를 주는 action만을 선택하면 된다. (= implicit policy)
반대로 Policy가 완벽하다면 value function은 굳이 필요하지 않는다.
  - Value function은 policy를 찾기 위한 중간 계산 과정에 불과하다!
    
Value-based: value function 만을 학습하고 policy는 암묵적으로만 갖고 있는 agent (i.e. DQN, C51...)  
Policy-based: value function이 없이 policy만을 학습하는 agent (i.e. policy gradient, A2C, A3C, PPO...)  
동시에 사용하는 agent도 있다!

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning)

Value-based: 최적의 action-value 함수 $𝑄^*(𝑠, 𝑎)$ 에 대한 근사 함수인 $𝑄^\theta(𝑠, 𝑎)$ 를 학습한다.
- 일반적으로 Bellman 방정식을 기반으로 하는 목적 함수를 사용!
- 최적화는 거의 항상 off-policy로 수행되는데, 이는 데이터가 획득되었을 때 에이전트가 환경을 탐색하는 방법에 관계없이 각 업데이트가 훈련 중에 수집된 데이터를 언제든지 사용할 수 있음을 의미
  - 데이터를 더 효율적으로 활용할 수 있다는 장점!

- 𝑄∗와𝜋∗ 사이의 아래의 연결을 통해 해당정책을 얻는다!
  - $𝑎(𝑠) = argmax_a 𝑄_\theta(𝑠,𝑎)$
 
## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning)
<img width="500" alt="스크린샷 2023-08-29 오후 9 20 14" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/59ac1b72-a4d1-4404-bee4-9db4399e754f">

Q learning이 학습되는 예: https://leonardoaraujosantos.gitbook.io/artificial-inteligence/artificial_intelligence/reinforcement_learning/qlearning_simple

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning) - DQN
<img width="500" alt="스크린샷 2023-08-29 오후 9 21 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f459dca9-2255-45ae-b232-dbf64bef5d45">

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning) ‒ DQN: Replay buffer
기존 Q-learning의 문제
  - 기존 Q-러닝에서는 observation을 생성하고, 바로 Q함수를 업데이트 하는 과정을 반복해서 학습했다.
    - 이전의 관측치가 다음 관측치에 영향을 주어 학습이 잘 안되는 문제.
  - 기존 Q-러닝에서는 바로 데이터가 버려졌었다.
    - 학습에 도움이 되는 데이터가 한번 사용되고 버려지기 때문에 비효율적인 문제가 발생!
해결책
  - Experience replay를 사용하는 agent는 observation을 얻을 때마다 이를 replay buffer에 저장!
  - 저장된 buffer (history)에서 batch만큼 샘플링하여, Q-learning update에 사용
  - 즉, Replay buffer에서 uniform random sampling을 통해 관측 값을 추출! - 데이터 간 상관 관계가 사라진다.

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning) ‒ DQN: Fixed Q target
기존 Q-learning의 문제
  - Q-learning 의 target에 Q함수가 포함이 되어 있으므로, Q함수를 업데이트 하는 과정에서 target 또한 변화한다.
  - 이로 인해 Q함수가 target 에 가깝게 안정적으로 업데이트 되지 않을 수 있다.
해결책
  - target network 의 weight을 "고정" 하고 local network을 먼저 여러 번 업데이트한 뒤 동기화한다.
  - 논문에서는 local network가 4번 업데이트 될 때, target network을 한번에 local network의 파라미터로 업데이트한다!

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning) - DQN
<img width="500" alt="스크린샷 2023-08-29 오후 9 41 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/04e2d1ab-3dc6-4f1e-8207-1c079ded4c7f">

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning) ‒ DQN: 시뮬레이션 예시
<img width="450" alt="스크린샷 2023-08-29 오후 9 41 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/586735e7-2fef-4489-9a01-cd3c2812271e">

Mnih, Volodymyr, et al. "Playing atari with deep reinforcement learning." arXiv preprint arXiv:1312.5602 (2013)

<img width="450" alt="스크린샷 2023-08-29 오후 9 42 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f76931aa-cc36-4116-9439-ae8ff4a452aa">

## 강화 학습의 구분 - Model-free RL ‒ value-based (Q-learning) ‒ DQN: 시뮬레이션 결과
<img width="450" alt="스크린샷 2023-08-29 오후 9 42 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/be1ebaf3-5762-479a-9adf-8804939da245">
