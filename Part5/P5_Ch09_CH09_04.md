# Pretrained Large-Scale Transformer - 4 Hybrid Methods
## XLNet

- Transformer Decoder(i.e. GPT) : Auto-Regressive(AR)방식으로 주어진 컨텍스트에 대해 다음 토큰을 맞추는 단 방향의 학습을 진행

- Transformer Encoder(i.e. BERT): 단 방향 학습을 지양. 
  특정 토큰을 [MASK] 로 치환하고 이를 예측함으로써(일종의 denoising autoencoder), 양방향의 정보를 이용하였다.

- 하지만 아래 문제들이 발생할 수 있다.
1) [MASK]는 pre-training 에만 등장하는 토큰으로 fine-tuning 과 불일치
2) [MASK] 토큰 사이의 의존관계가 무시되는 문제가 발생

- 그렇다면 auto-regressive을 하되, 양방향 context를 알 수 있게 하자!
-> XLNET !

## XLNet - Permutation language model
- "Permutation Language Modeling" + "AutoRegressive Language Modeling"

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/029e9b3f-6cd1-4e52-8282-5a707d6b0edc)

  - 길이가 T인 시퀀스 x에 대해 T! 만큼 autoregressive 인수분해를 수행.
  - model parameter들이 모든 인수분해 순서들에 걸쳐 공유되면, model은 양측의 모든 위치에서 정보를 모으는 방법을 train하게 된다.
  - $Z_T$는 길이 length−T의 index sequence [1,2,...,T]의 모든 가능한 permutation 집합이라 정의한다.
  - $z_t$ 와 $Z _{< t }$ 를 사용하여 permutation z∈ $Z_T$의 t번째 element와, element 1부터 t−1를 나타낸다.
    <img width="330" alt="스크린샷 2023-08-08 오후 7 28 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d0b45a4a-66c1-4e77-be1a-a8e1f3ee74bb">
  - text sequence x에 대해 인수분해 순서 z를 sampling하고 인수분해 순서에 따라 likelihood $p_{\theta}(x)$를 decompose 한다.
  - 동일한 parameter θ가 공유되어 학습되는 동안, $x_t$는 $x_i \ne x_t$ 인 모든 element를 보기 때문에 bi-directional context를 capture할 수 있다.
  - Remark on Permutation: 제안하는 방식은 sequence 순서가 아닌 인수분해 순서만 바꾼다.
    - 즉 원래의 sequence 순서를 유지하고 원본 sequence에 해당하는 positional encoding을 사용하여 인수분해 순서 permutation에 해당하는 attention mask를 얻는다.

## XLNet - Two-Stream Self-Attention for Target-Aware Representations 모델
- permutation language modeling을 standard transformer에 naive하게 적용하기는 어려움이 있음.
- 위와 같은 문제를 확인하기 위해 softmax를 사용하여 next-token의 distribution $p_{\theta}(X_{zt} = x | x_{Z < t})$를 parameter화 한다고 가정.
<img width="393" alt="image" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/04f7e896-6351-46c6-adf9-989c0c168290">

- $h_θ (x_{z < t} )$는 masking후 transformer로 부터 생성된 $x_{z < t}$ 의 hidden representation 을 의미한다.
   - representation $h_θ(x_{z < t})$는 $z_t$에 의존성이 없다는 것을 주목하자.
   - 결과적으로 유용한 representation을 배울 수 없는 위치에 상관없이 동일한 distribution이 예측하게 된다.
   - 위의 문제를 해결하기 위해 next-token의 distribution이 target position을 인식할수 있게 re-parameterize 한다.<img width="401" alt="스크린샷 2023-08-08 오후 8 06 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/54bc6830-a4e2-4829-8c9f-f14cf3999b16">
   - $g_θ(x_{z < t} ,z_t)$는 target position $z_t$ 를 추가적으로 받는 새로운 유형의 representation을 나타낸다.


- 식 $g_θ(x_{z < t} ,z_t)$를 어떻게 수식화?
  - token $x_{z_t},g_θ(x_{z < t},z_t)$를 예측하기 위해 position $z_t$ 만 사용해야 한다. 
  - j>t일 때 다른 token $x_{zj}$ 를 예측하기 위해 $x_{z_t}$ 는 완전한 context information을 제공하기 위해 content를 encoding 해야한다.
    - 위를 해결하기 위해 하나가 아닌 두 세트의 hidden representation을 사용하도록 제안한다
  - content representation(normal self-attention) : $h_θ(x_{z ≤ t})$ transformer의 standard hidden state와 비슷한 역할을 한다.

    ![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5277c08f-443f-43d0-bdd4-2f538ea8d498)

    - 이 representation은 content와 $x_{z ≤ t}$를 모두 encoding 한다.
  - query representation(self-attention X, token position X) : $g_θ(x_{z < t}, z_t )$는 contextual information $x_{z < t}$ 와 position $z_t$ 에 접근할 수 있지만 
    content $x_{z_t}$ 에는 접근할 수 없다.    <img width="874" alt="스크린샷 2023-08-08 오후 8 34 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a7748618-3d1c-4cb7-9ed6-c528de4a72ab">


    ![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cc8047bc-a9c0-4417-9629-ca64241ccb90)

## XLNet - Partial prediction
Partial Prediction
- permutation language modeling은 장점이 있지만, 순열로 인해 최적화가 어렵고 수렴이 오래걸린다. 최적화의 어려움을 해결하기 위해, 인수분해 순서에서 마지막 token만 예측한다. 
- z를 non-target subsequence z≤c와 target subsequence z> c로 분할한다. 
- c는 cutting point이다. objective는 non-target subsequence에서 target subsequence conditioned의 log-likelihood를 maximize하는 것이다.

## XLNet - Pre-training / 구현 디테일

- Pre-training에 사용한 데이터: 
  BERT에서 이용했던 BooksCorpus, Wikipedia에 추가적으로 Giga5, ClueWeb 2020-B, Common Crawl까지 총 5개의 데이터 셋 이용
  SentencePiece를 이용 후 tokenizing 된 후 총 32.89B token 정도.

- Hyperparameter
  모델 크기: BERT-Base/BERT-Large와 각각 같은 크기를 갖게 설정 🡪 XLNet-Base/XLNet-Large
  시퀀스 길이: 패딩 없는 512길이 입력.(RoBERTa와 같은 설정)
  배치 사이즈: 8192를 이용했습니다.
  Optimizer/Scheduler: Adam weight decay/ linear learning rate decay 이용.
  장치/소요시간: 512 TPU v3로 약 5.5일 소요

- 양방향 데이터 파이프라인(Bidirectional Data Pipeline)
  recurrence memory을 이용할 때, 양방향으로 모두 장기 의존성을 학습할 수 있도록 배치를 정방향/역방향의 sequence을 반반으로 되도록 진행.
  정방향의 경우 현재 시점 이전 시퀀스들이 메모리로 제공되고, 역방향의 경우 현재 시점 이후 시퀀스들이 메모리로 제공되어 양방향의 장기의존성을 학습.

- Span기반의 예측
  Language Model의 특정 시점에서 주어진 컨텍스트에 대해 하나의 토큰만 예측하는 것이 아니라 여러 토큰들의 span을 예측. 
  즉, 길이 L∈[1,...,5] 을 랜덤으로 선택하고, 연속적인 길이 L의 span을 선택한 다음 KL개의 토큰들을 타겟으로 학습!

<img width="469" alt="스크린샷 2023-08-08 오후 9 06 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/feca9688-0028-4838-81dd-2536455ab938">
<img width="420" alt="스크린샷 2023-08-08 오후 9 06 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5ddfc524-635e-4552-994f-c2615b36a68b">

## BART - Pre-trained seq2seq transformer

