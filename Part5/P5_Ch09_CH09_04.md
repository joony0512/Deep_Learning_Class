# Pretrained Large-Scale Transformer - 4 Hybrid Methods
## XLNet

- Transformer Decoder(i.e. GPT) : Auto-Regressive(AR)ë°©ì‹ìœ¼ë¡œ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ í† í°ì„ ë§ì¶”ëŠ” ë‹¨ ë°©í–¥ì˜ í•™ìŠµì„ ì§„í–‰

- Transformer Encoder(i.e. BERT): ë‹¨ ë°©í–¥ í•™ìŠµì„ ì§€ì–‘. 
  íŠ¹ì • í† í°ì„ [MASK] ë¡œ ì¹˜í™˜í•˜ê³  ì´ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨(ì¼ì¢…ì˜ denoising autoencoder), ì–‘ë°©í–¥ì˜ ì •ë³´ë¥¼ ì´ìš©í•˜ì˜€ë‹¤.

- í•˜ì§€ë§Œ ì•„ë˜ ë¬¸ì œë“¤ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
1) [MASK]ëŠ” pre-training ì—ë§Œ ë“±ì¥í•˜ëŠ” í† í°ìœ¼ë¡œ fine-tuning ê³¼ ë¶ˆì¼ì¹˜
2) [MASK] í† í° ì‚¬ì´ì˜ ì˜ì¡´ê´€ê³„ê°€ ë¬´ì‹œë˜ëŠ” ë¬¸ì œê°€ ë°œìƒ

- ê·¸ë ‡ë‹¤ë©´ auto-regressiveì„ í•˜ë˜, ì–‘ë°©í–¥ contextë¥¼ ì•Œ ìˆ˜ ìˆê²Œ í•˜ì!
-> XLNET !

## XLNet - Permutation language model
- "Permutation Language Modeling" + "AutoRegressive Language Modeling"

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/029e9b3f-6cd1-4e52-8282-5a707d6b0edc)

  - ê¸¸ì´ê°€ Tì¸ ì‹œí€€ìŠ¤ xì— ëŒ€í•´ T! ë§Œí¼ autoregressive ì¸ìˆ˜ë¶„í•´ë¥¼ ìˆ˜í–‰.
  - model parameterë“¤ì´ ëª¨ë“  ì¸ìˆ˜ë¶„í•´ ìˆœì„œë“¤ì— ê±¸ì³ ê³µìœ ë˜ë©´, modelì€ ì–‘ì¸¡ì˜ ëª¨ë“  ìœ„ì¹˜ì—ì„œ ì •ë³´ë¥¼ ëª¨ìœ¼ëŠ” ë°©ë²•ì„ trainí•˜ê²Œ ëœë‹¤.
  - $Z_T$ëŠ” ê¸¸ì´ lengthâˆ’Tì˜ index sequence [1,2,...,T]ì˜ ëª¨ë“  ê°€ëŠ¥í•œ permutation ì§‘í•©ì´ë¼ ì •ì˜í•œë‹¤.
  - $z_t$ ì™€ $Z _{< t }$ ë¥¼ ì‚¬ìš©í•˜ì—¬ permutation zâˆˆ $Z_T$ì˜ të²ˆì§¸ elementì™€, element 1ë¶€í„° tâˆ’1ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    <img width="330" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-08 á„‹á…©á„’á…® 7 28 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d0b45a4a-66c1-4e77-be1a-a8e1f3ee74bb">
  - text sequence xì— ëŒ€í•´ ì¸ìˆ˜ë¶„í•´ ìˆœì„œ zë¥¼ samplingí•˜ê³  ì¸ìˆ˜ë¶„í•´ ìˆœì„œì— ë”°ë¼ likelihood $p_{\theta}(x)$ë¥¼ decompose í•œë‹¤.
  - ë™ì¼í•œ parameter Î¸ê°€ ê³µìœ ë˜ì–´ í•™ìŠµë˜ëŠ” ë™ì•ˆ, $x_t$ëŠ” $x_i \ne x_t$ ì¸ ëª¨ë“  elementë¥¼ ë³´ê¸° ë•Œë¬¸ì— bi-directional contextë¥¼ captureí•  ìˆ˜ ìˆë‹¤.
  - Remark on Permutation: ì œì•ˆí•˜ëŠ” ë°©ì‹ì€ sequence ìˆœì„œê°€ ì•„ë‹Œ ì¸ìˆ˜ë¶„í•´ ìˆœì„œë§Œ ë°”ê¾¼ë‹¤.
    - ì¦‰ ì›ë˜ì˜ sequence ìˆœì„œë¥¼ ìœ ì§€í•˜ê³  ì›ë³¸ sequenceì— í•´ë‹¹í•˜ëŠ” positional encodingì„ ì‚¬ìš©í•˜ì—¬ ì¸ìˆ˜ë¶„í•´ ìˆœì„œ permutationì— í•´ë‹¹í•˜ëŠ” attention maskë¥¼ ì–»ëŠ”ë‹¤.

## XLNet - Two-Stream Self-Attention for Target-Aware Representations ëª¨ë¸
- permutation language modelingì„ standard transformerì— naiveí•˜ê²Œ ì ìš©í•˜ê¸°ëŠ” ì–´ë ¤ì›€ì´ ìˆìŒ.
- ìœ„ì™€ ê°™ì€ ë¬¸ì œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ next-tokenì˜ distribution $p_{\theta}(X_{zt} = x | x_{Z < t})$ë¥¼ parameterí™” í•œë‹¤ê³  ê°€ì •.
<img width="393" alt="image" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/04f7e896-6351-46c6-adf9-989c0c168290">

- $h_Î¸ (x_{z < t} )$ëŠ” maskingí›„ transformerë¡œ ë¶€í„° ìƒì„±ëœ $x_{z < t}$ ì˜ hidden representation ì„ ì˜ë¯¸í•œë‹¤.
   - representation $h_Î¸(x_{z < t})$ëŠ” $z_t$ì— ì˜ì¡´ì„±ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì£¼ëª©í•˜ì.
   - ê²°ê³¼ì ìœ¼ë¡œ ìœ ìš©í•œ representationì„ ë°°ìš¸ ìˆ˜ ì—†ëŠ” ìœ„ì¹˜ì— ìƒê´€ì—†ì´ ë™ì¼í•œ distributionì´ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.
   - ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ next-tokenì˜ distributionì´ target positionì„ ì¸ì‹í• ìˆ˜ ìˆê²Œ re-parameterize í•œë‹¤.<img width="401" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-08 á„‹á…©á„’á…® 8 06 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/54bc6830-a4e2-4829-8c9f-f14cf3999b16">
   - $g_Î¸(x_{z < t} ,z_t)$ëŠ” target position $z_t$ ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë°›ëŠ” ìƒˆë¡œìš´ ìœ í˜•ì˜ representationì„ ë‚˜íƒ€ë‚¸ë‹¤.


- ì‹ $g_Î¸(x_{z < t} ,z_t)$ë¥¼ ì–´ë–»ê²Œ ìˆ˜ì‹í™”?
  - token $x_{z_t},g_Î¸(x_{z < t},z_t)$ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ position $z_t$ ë§Œ ì‚¬ìš©í•´ì•¼ í•œë‹¤. 
  - j>tì¼ ë•Œ ë‹¤ë¥¸ token $x_{zj}$ ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ $x_{z_t}$ ëŠ” ì™„ì „í•œ context informationì„ ì œê³µí•˜ê¸° ìœ„í•´ contentë¥¼ encoding í•´ì•¼í•œë‹¤.
    - ìœ„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•˜ë‚˜ê°€ ì•„ë‹Œ ë‘ ì„¸íŠ¸ì˜ hidden representationì„ ì‚¬ìš©í•˜ë„ë¡ ì œì•ˆí•œë‹¤
  - content representation(normal self-attention) : $h_Î¸(x_{z â‰¤ t})$ transformerì˜ standard hidden stateì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•œë‹¤.

    ![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5277c08f-443f-43d0-bdd4-2f538ea8d498)

    - ì´ representationì€ contentì™€ $x_{z â‰¤ t}$ë¥¼ ëª¨ë‘ encoding í•œë‹¤.
  - query representation(self-attention X, token position X) : $g_Î¸(x_{z < t}, z_t )$ëŠ” contextual information $x_{z < t}$ ì™€ position $z_t$ ì— ì ‘ê·¼í•  ìˆ˜ ìˆì§€ë§Œ 
    content $x_{z_t}$ ì—ëŠ” ì ‘ê·¼í•  ìˆ˜ ì—†ë‹¤.    <img width="874" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-08 á„‹á…©á„’á…® 8 34 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a7748618-3d1c-4cb7-9ed6-c528de4a72ab">


    ![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cc8047bc-a9c0-4417-9629-ca64241ccb90)

## XLNet - Partial prediction
Partial Prediction
- permutation language modelingì€ ì¥ì ì´ ìˆì§€ë§Œ, ìˆœì—´ë¡œ ì¸í•´ ìµœì í™”ê°€ ì–´ë µê³  ìˆ˜ë ´ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤. ìµœì í™”ì˜ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ê¸° ìœ„í•´, ì¸ìˆ˜ë¶„í•´ ìˆœì„œì—ì„œ ë§ˆì§€ë§‰ tokenë§Œ ì˜ˆì¸¡í•œë‹¤. 
- zë¥¼ non-target subsequence zâ‰¤cì™€ target subsequence z> cë¡œ ë¶„í• í•œë‹¤. 
- cëŠ” cutting pointì´ë‹¤. objectiveëŠ” non-target subsequenceì—ì„œ target subsequence conditionedì˜ log-likelihoodë¥¼ maximizeí•˜ëŠ” ê²ƒì´ë‹¤.

## XLNet - Pre-training / êµ¬í˜„ ë””í…Œì¼

- Pre-trainingì— ì‚¬ìš©í•œ ë°ì´í„°: 
  BERTì—ì„œ ì´ìš©í–ˆë˜ BooksCorpus, Wikipediaì— ì¶”ê°€ì ìœ¼ë¡œ Giga5, ClueWeb 2020-B, Common Crawlê¹Œì§€ ì´ 5ê°œì˜ ë°ì´í„° ì…‹ ì´ìš©
  SentencePieceë¥¼ ì´ìš© í›„ tokenizing ëœ í›„ ì´ 32.89B token ì •ë„.

- Hyperparameter
  ëª¨ë¸ í¬ê¸°: BERT-Base/BERT-Largeì™€ ê°ê° ê°™ì€ í¬ê¸°ë¥¼ ê°–ê²Œ ì„¤ì • ğŸ¡ª XLNet-Base/XLNet-Large
  ì‹œí€€ìŠ¤ ê¸¸ì´: íŒ¨ë”© ì—†ëŠ” 512ê¸¸ì´ ì…ë ¥.(RoBERTaì™€ ê°™ì€ ì„¤ì •)
  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: 8192ë¥¼ ì´ìš©í–ˆìŠµë‹ˆë‹¤.
  Optimizer/Scheduler: Adam weight decay/ linear learning rate decay ì´ìš©.
  ì¥ì¹˜/ì†Œìš”ì‹œê°„: 512 TPU v3ë¡œ ì•½ 5.5ì¼ ì†Œìš”

- ì–‘ë°©í–¥ ë°ì´í„° íŒŒì´í”„ë¼ì¸(Bidirectional Data Pipeline)
  recurrence memoryì„ ì´ìš©í•  ë•Œ, ì–‘ë°©í–¥ìœ¼ë¡œ ëª¨ë‘ ì¥ê¸° ì˜ì¡´ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë°°ì¹˜ë¥¼ ì •ë°©í–¥/ì—­ë°©í–¥ì˜ sequenceì„ ë°˜ë°˜ìœ¼ë¡œ ë˜ë„ë¡ ì§„í–‰.
  ì •ë°©í–¥ì˜ ê²½ìš° í˜„ì¬ ì‹œì  ì´ì „ ì‹œí€€ìŠ¤ë“¤ì´ ë©”ëª¨ë¦¬ë¡œ ì œê³µë˜ê³ , ì—­ë°©í–¥ì˜ ê²½ìš° í˜„ì¬ ì‹œì  ì´í›„ ì‹œí€€ìŠ¤ë“¤ì´ ë©”ëª¨ë¦¬ë¡œ ì œê³µë˜ì–´ ì–‘ë°©í–¥ì˜ ì¥ê¸°ì˜ì¡´ì„±ì„ í•™ìŠµ.

- Spanê¸°ë°˜ì˜ ì˜ˆì¸¡
  Language Modelì˜ íŠ¹ì • ì‹œì ì—ì„œ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ í•˜ë‚˜ì˜ í† í°ë§Œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ í† í°ë“¤ì˜ spanì„ ì˜ˆì¸¡. 
  ì¦‰, ê¸¸ì´ Lâˆˆ[1,...,5] ì„ ëœë¤ìœ¼ë¡œ ì„ íƒí•˜ê³ , ì—°ì†ì ì¸ ê¸¸ì´ Lì˜ spanì„ ì„ íƒí•œ ë‹¤ìŒ KLê°œì˜ í† í°ë“¤ì„ íƒ€ê²Ÿìœ¼ë¡œ í•™ìŠµ!

<img width="469" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-08 á„‹á…©á„’á…® 9 06 10" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/feca9688-0028-4838-81dd-2536455ab938">
<img width="420" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-08 á„‹á…©á„’á…® 9 06 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5ddfc524-635e-4552-994f-c2615b36a68b">

## BART - Pre-trained seq2seq transformer

