# Pretrained Large-Scale Transformer- 2. Pre-trained Transformer Decoder - Generative Pre-training(GPT)
## Generative Pre-training (GPT)
- Transformer decoder(Autoregressive Generation)사용
  -> 참고 : 시퀀스로 주어지는 문장에서 단어가 나올 확률울 찾는 모델을 언어모델(Language Model : LM)이라고 하며, auto-regressive모델에 속한다.

  <img width="300" alt="스크린샷 2023-08-08 오후 1 18 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/44d6d953-b4e3-4804-a90d-2312c2eb8941">

- Unsupervised pre-training /semi-supervised fine-tuning을 수행한다.
  -> 데이터가 많은 task로 Unsupervised pre-training을 진행하고, Target task에 down-streaming로 fine-tuning(transfer learning)을 진행한다.
  
- 모델의 검증
  - natural language inference
  - question answering
  - semantic similarity
  - text classification
 
## Generative Pre-training (GPT) - Step 1. unsupervised pre-training
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e3b998d8-bf4c-40b1-a168-3534cc52ee64)

- 큰 corpus(말뭉치)로 LM을 먼저 학습시킨다. 그다음 fine-tune을 진행한다.
- 데이터는 다음을 이용한다.
  - Pretrainig : BookCorpus(발간되지 않은 약 7000개의 책, downstream tasks의 test set에 최대한 있지 않을것이라 예상되는)
  - LM모델은 아래와 같이 나타낼 수 있다.

    <img width="400" alt="스크린샷 2023-08-08 오후 1 41 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/70d31776-42fd-48cd-a59b-168f3128a1b8">
    
    - 손실함수
      
    <img width="400" alt="스크린샷 2023-08-08 오후 1 41 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8f5be8d7-76d6-4d3e-835c-14972422e0ee">
    - U : input context token
    - W_e : token embedding matrix
    - W_p : positional embedding
    - n : layer 개수


## Generative Pre-training (GPT) - Step 2. supervised fine-tune & defining task-specific inputs 
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/becd421e-2be5-430a-a752-d0fb0f5826fd)

그 이후, transformer output에 linear output layer을 붙여 supervised learning을 구현할 수 있다.

<img width="300" alt="스크린샷 2023-08-08 오후 1 50 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/80d641a6-420c-4397-8c3c-193483e0a4a8">

Defining task-specific inputs
- text classification같은 특정 태스크들의 경우에는 위 그림처럼 바로 fine-tuning할수있다.
- QA나 textual entailment 같은 경우등 ordered sentence pair, triplet of document, question, and answer 같은 structured input을 처리하기 위해 input의 형태를 바꾸어 준다.

## Generative Pre-training (GPT) - Results
<img width="300" alt="스크린샷 2023-08-08 오후 1 54 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/58a80cd9-1a4a-4631-9486-12b851ab204c">

- 나왔던 당시, GPT는 많은 NLP 문제와 데이터에서 독보적인 퍼포먼스를 보였다.

## Generative Pre-training (GPT2) - GPT을 multi-task learners / 제로 샷 특성(zero-shot behavior)을 가지도록 발전
Multi-task 학습 task conditioning 
- GPT-2는 GPT-1과 다르게 동일한 unsupervised모델을 사용하여 여러 작업을 학습하는 것을 목표로 한다.
- 학습 목적 함수를 P(output | input, task)로 수정필요!
- 이 수정 사항을 task conditioning이라 하고, 이러한 모델은 동일한 입력이 주어졌을 때에도 작업에 따라 서로 다른 출력을 생성해야 한다.
- 이때 일부 task의 경우, task conditioning을 위해 architectural level에서 작업 조건화를 구현한다.
  
제로 샷 학습 & 제로 샷 task transfer
- 제로 샷 학습은 zero-shot transfer의 특수한 경우로, 예제(example; train data)를 전혀 제공하지 않으며 모델은 단지 주어진 지침에 따라 작업을 이해하는 학습을 말한다. 
- Fine-tune을 위해 GPT-1에서 했던 것처럼 순서를 재정렬하는 대신 GPT-2에 대한 입력은 모델이 작업의 특성을 이해하고 답을 제공할 것으로 예상하는 형식으로 이루어진다. 
  - 이 작업은 제로 샷 작업 전송 동작을 모방하기 위해 디자인되었다!
  - 예로 영어-프랑스어 번역 작업의 경우 모델에는 단지 영어 문장과 프랑스어 단어를 구분하기 위한 
  - 프롬프트(:)가 제공된다. 
  - 모델은 번역과제임을 이해하고 영어 문장의 프랑스어 번역을 바로 수행한다.

## Generative Pre-training (GPT2) - 데이터 셋 & 모델
데이터 셋 
- 저자들은 광범위하고 우수한 품질의 데이터 셋을 만들기 위해 Reddit플랫폼을 스크랩하고 높은 투표율을 기록한 기사의 아웃바운드 링크에서 데이터를 가져왔다.
- 8백만 이상의 문서에서 약 40GB의 텍스트데이터를 받아 이를 WebText라 부른다.
- 이는 gpt1의 book corpus에 비해 매우 큰 데이터!

모델 architecture & 실행 디테일
- GPT1이 117M parameters에 비해, 1.5B의 Parameters을 가진다.
- GPT2는 48개의 layers와 1600 dimensional vector을 가진 word embedding을 사용한다.
- 50,257 tokens이 사용되었다. 즉, 보다 큰 vocabulary을 가진다.
- batch size 는 512, toke context window는 1024로 지정하였다.
- layer normalization 의 위치가 sub-block의 input부분으로 이동하였고, 추가적인 layer normalization이 마지막 self-attention block뒤에 추가 되었다.
- 초기화할때, residual layer들은 $1 \over \sqrt N$로 scaling 하였다. (N은 residual layer의 개수)

## Generative Pre-training (GPT2) - Results
Zero-shot 결과 (학습데이터나 fine-tune이 없음)  

<img width="800" alt="스크린샷 2023-08-08 오후 2 06 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3068fad7-43d4-45b8-af09-2dba2b4466c9">

그밖의 결과들 (요약)
- Children’s Book Dataset은 명사, 전치사, 명명된 실체 등과 같은 단어 카테고리에 대한 언어 모델의 성능을 평가한다.
  - GPT-2는 일반 명사와 명명된 실체 인식에 대해 SOTA 정확도를 약 7% 증가시켰다.
- LAMBADA 데이터 집합은 모델의 성능을 평가하여 긴 범위의 종속성을 식별하고 문장의 마지막 단어를 예측한다.
  - GPT-2는 복잡성을 99.8에서 8.6로 줄이고 정확도를 크게 개선했다.
- GPT-2는 제로 샷 설정에서 읽기 이해 과제에서 기준 모델 4개 중 3개를 능가했다.
- 프랑스어부터 영어까지 번역 과제에서 GPT-2는 제로 샷 설정에서 대부분의 unsupervised model보다 성능이 뛰어났지만 최신 unsupervised model을 능가하지는 못했다.
- GPT-2는 텍스트 요약에서 좋은 성과를 낼 수 없었고 그 성능은 요약을 위해 훈련된 고전 모델과 비슷하거나 낮았다.

## Generative Pre-training (GPT3) - Few-shot learner로서의 GPT3
NLP 모델은 태스크와 무관한 representation을 학습하는 방향으로 발전
	- ELMo, BERT, GPT 등 모델은 down-stream task와 상관없이 large scale corpus을 이용해 
		학습된 모델은 fine-tuning을 통해 성공적인 퍼포먼스를 달성했지만… 
	- 이러한 방법들의 사전 학습은 여전히 "태스크에 따라 매번 fine-tuning이 필요하다"라는 한계가 있었다. 

문제점
  - 많은 레이블 데이터가 필요
  - Fine-tuning 기반의 방법들은 다량의 지식을 학습하지만, 특정 task에 fine-tuning되는 과정에서 특정 task외의 문제에 대한 일반화 능력 상실 🡪 좋은 성적인 것처럼 보이지만, 실제로는 아니다
  - 사람은 언어를 활용할 때 대규모 레이블 필요 X 🡺 간단한 예시만으로 가능!
  

GPT3: Meta learning로 해결해보자.
- Meta learning: 훈련 시 다양한 기술과 패턴 인식 방법을 학습 후 추론 시에 down stream task에 
빠르게 적응하도록 하는 방법을 의미.

- 많은 parameter 수: transformer을 이용 🡪 모델 사이즈를 1750억 개로 크게 늘림.
  - 직전 연구들에서 parameter가 늘어날 수록 downstream task에서 상당한 성능 개선이 이루어졌고
    실제 log loss가 규모가 커질수록 개선되는 추세였다.
    (i.e. GPT1: 1억, BERT: 3억, GPT-2: 14억, Megatron: 80억, T5: 100억…)

## Generative Pre-training (GPT3) - Few-shot learner로서의 GPT3
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f4a8206f-e6d6-4fae-a84b-dc43198792e3)

모델 평가방법
1. few-shot learning:
  - 모델의 10/100개의 문맥 창(context window)에 맞는 설명 또는 예시(demonstration)을 허용하는 문맥 내 학습 조건
2. one-shot learning:
  - 딱 한 개의 예시 만을 허용하는 조건,
3. zero-shot learning:
  - 어떤 예시도 허용되지 않고, 모델에 주어지는 것은 오직 자연어로 된 지시문(prompt)인 조건.
4. Fine-tune (down-streaming test)
  - 전통적인 방법 GPT2에서 쓰였으나, 여기서 사용되지는 X

Gradient update나 fine-tune 거치지 않는, 조건으로 주어지는 예시 수만 조절했을 때,

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ebe840fc-c159-4908-b1c9-f5a87c36f2a8)

- 모델 크기가 커지면, In context learning 능력이 향상됨
- Task에 대한 지시문(prompt)가 주어지면 모델 성능이 향상됨
- 모델 문맥에 주어지는 예시 (Example in context)의 숫자가 많아지면 모델 성능이 향상됨

모델 구조
- GPT2와 거의 동일!
  - 단, Transformer 레이어에서 Dense 한 Attention 과 locally banded sparse attention 을 번갈아 사용하였음.
- 규모에 따라 8개의 모델 학습 & 테스트. 약 3000억 개의 Token에 대해 학습!
  
<img width="400" alt="스크린샷 2023-08-08 오후 2 15 25" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a4cf8922-06dd-4d82-8168-2e4a7a997506">

데이터 셋
- 훈련 데이터셋은 아래와 같다.

<img width="400" alt="스크린샷 2023-08-08 오후 2 15 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b0ea65e3-9908-4410-8f5f-8c0c51c03f9f">

결과
- 모델 성능이 모델 사이즈가 커짐에 따라 좋은 결과!
  
<img width="400" alt="스크린샷 2023-08-08 오후 2 16 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ef9f1166-3606-48c7-afa0-817f8cd27c01">

전통적 LM 및 관련 Task

<img width="400" alt="스크린샷 2023-08-08 오후 2 18 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/56c58b66-070b-4260-86e4-eeeefeef7f81">

LAMBADA
- 문장 완성하기 / 언어의 장기 의존성을 모델링 하는 task

<img width="400" alt="스크린샷 2023-08-08 오후 2 18 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6e01710e-e0c8-4b26-b737-46394a136575">

StoryCloze
- 다섯 문장의 긴 글을 끝맺기에 적절한 문장을 고르는 task

HellaSwag
- 짧은 글이나 지시사항을 끝맺기에 가장 알맞는 문장을 고르는 task
-> 보통 사람은 쉬워 하지만 AI LM은 어려워하는 문제!

Closed Book Question Answering
- 광범위한 사실적 지식(broad factual knowledge)에 대한 QA 능력을 측정

<img width="400" alt="스크린샷 2023-08-08 오후 2 20 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8570057e-4c97-4071-9b99-d74334ff77dd">

번역
- Few-shot leaning인데도, supervised setting과 견줄만하거나 더 나은 결과를 보여주는 번역도 있었다!

<img width="400" alt="스크린샷 2023-08-08 오후 2 20 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2cff0ecb-43cf-4098-99c9-83ca0333b383">

Winograd-style task (대명사 지칭 문제)
- 기존 SOTA 및 사람과 비교해도 큰 차이가 없는 결과!

<img width="300" alt="스크린샷 2023-08-08 오후 2 21 57" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b47ae4e3-6817-41c1-b4b7-723076cf8044">
<img width="300" alt="스크린샷 2023-08-08 오후 2 22 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f15cfdc8-fb30-4057-a6d7-b10dc131ae8a">

Common sense reasoning
- 파라미터가 클 경우 기존 SOTA를 능가하는 결과를 보여주기도 하였음.
- PhysicalQA(PIQA)는 어떻게 물리학적으로 세계가 움직이고 세상에 대한 현실 이해를 관찰하는 것을 목적으로 한 상식 질문 데이터
- ARC(AI2 Reason challenge) 3~9학년 과학 시험에서 모은 7787개의 다지선다 문제 데이터
- OpenBookQA
  - 초등 수준의 과학적 사실을 다루는 질문으로 구성된 QA데이터

<img width="500" alt="스크린샷 2023-08-08 오후 2 26 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/33d7dd8e-0030-46d4-b5df-bd27b4ecda94">

<img width="500" alt="스크린샷 2023-08-08 오후 2 27 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/23b91395-ab3f-4329-bcb9-9611af705815">

기사 생성 & Grammar 교정
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/720cafb8-22d6-4294-abf1-5d25b8613fcd)
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fc727f6d-b4ed-4d6a-b6f2-4fe1d5416485)
![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/27418d70-b90a-4ad4-9bf3-14deacf8fd04)

한계 (요약)
[데이터 셋의 효율성]
- GPT-3는 사전 학습 동안 사람이 평생 보는 것보다도 더 많은 텍스트를 보지만, 사람만큼의 성능을 내지 못한다. 데이터 샘플의 효율성을 개선하는 것도 매우 중요한 연구 과제이다.

[Few-shot learning의 해석 불가능함]
- Few-shot learning 방식이 모델로 하여금 추론 중에 처음부터 새로운 과제를 학습하도록 하는 것인지, 학습했던 task를 식별해서 작동하는 것인지 구분이 모호하다.

[사용성]
- GPT-3 규모가 크고 그에 따라 비용이 비싸고, 추론 과정이 불편하다는 점 
  - 추후 경량화? (i.e. distillation?)

[AI의 블랙박스 문제]
- GPT-3 또한 다른 딥러닝 모델과 같이 결과를 설명하기 힘들다.
- 새로운 입력에 대해서도 보정이 일어나지 않으며, 훈련데이터의 편향도 유지된다.

