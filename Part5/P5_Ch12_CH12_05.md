# <Generative model series #4> - Implicit Models - 5. GAN의 발전 - part 2
## Wasserstein GAN (WGAN; 2016-2017) - Wasserstein 거리 + Kantorovich Rubinstein Duality

Vanilla GAN

<img width="300" alt="스크린샷 2023-08-20 오후 8 13 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/00ad736b-5af7-4f2c-8d2b-7e6642aecb5b">

Wasserstein GAN

<img width="300" alt="스크린샷 2023-08-20 오후 8 13 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/455222a3-4326-42ac-95fa-7b0915824ee9">

- Optimal Transport 이론에서 영감을 얻어 만든 distance -> Earth Mover distance (EMD)를 이용!

  <img width="287" alt="스크린샷 2023-08-20 오후 8 14 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5b039b3f-6079-48c4-b779-2c7b495a4fa1">

- Estimate하기 intractable하다!
- Kantorovich Rubinstein duality 최적화 기법으로 근사! (1-Lipschitz 함수에서 search)
  
  <img width="315" alt="스크린샷 2023-08-20 오후 8 15 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/71565cac-6cca-4d3c-87c7-6c21144eea1d">

## Wasserstein GAN (WGAN; 2016-2017) - Distance/Divergence의 종류
1. Total variation(TV)
- 두 확률 분포의 측정값이 벌어질 수 있는 가장 큰값

 <img width="200" alt="스크린샷 2023-08-20 오후 8 34 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4b5e0ca1-d4ab-4f26-a887-9a52578c5b51">

2. Kullback-Leibler(KL) divergence

 <img width="200" alt="스크린샷 2023-08-20 오후 8 35 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a844db5f-19b1-405b-9017-e289d5644992">

3. Jensen-Shannon(JS) divergence

   <img width="200" alt="스크린샷 2023-08-20 오후 8 35 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6429ba92-3ca7-45f4-97e3-5d1adf0e3c6a">

4. Earth-Mover (EM) distance

   <img width="200" alt="스크린샷 2023-08-20 오후 8 36 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/35cf4f3e-f5a2-4a65-bea6-dd418bae4636">

  - 두 확률 분포의 결합확률 분포 $\pi (𝑃_r, 𝑃_g) )$ 중에서 d(X,Y)(x와y의거리)의 기대값을 가장 작게 추정한 값

## Wasserstein GAN (WGAN; 2016-2017) - EM 거리의 타당성
<img width="400" alt="스크린샷 2023-08-20 오후 8 39 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d53315d4-1e42-476c-bef3-01726ee7e49c">

- 임의의 distribution 𝑃R, 𝑃? 를 정의하고 그 확률 사이의 거리를 구하면,
- Wasserstein 거 리 (EM distance) 는 theta 에 관계없이 일정한 수식으로 이루어져 있으나, 다른 수식은 상수이거나 발산하는 등의 문제가 있다!
- 만약, θS → 0로가면, 유일하게 수렴하는 것은 EM 거리 뿐! -> 학습에 사용하기 쉽다!

## Wasserstein GAN (WGAN; 2016-2017) - WGAN의 critic
<img width="508" alt="스크린샷 2023-08-20 오후 8 41 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/af3c8b22-e960-4c4c-8efb-e3150ab330b5">

- WGAN은 discriminator대신 critic을 사용!
- Discriminator가 진짜/가짜를 구분하기 위한 sigmoid를 씌운 확률값이라면,
- Critic의 결과는 (EM distance)로 부터 얻은 scalar이다.  
-> Step function처럼 작동하지 않기 때문에, 더이상 discriminator, generator의 밸런스를 위해 고민하지 않아도 된다!

## Wasserstein GAN (WGAN; 2016-2017) - Wasserstein distance는 샘플의 퀄리티와 상호 연관 관계가 있다.
<img width="548" alt="스크린샷 2023-08-20 오후 8 42 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0568b80d-3c41-4478-9c82-a95b1692b2f6">

## Wasserstein GAN (WGAN; 2016-2017) - Wasserstein 거리 + Kantorovich Rubinstein Duality
- Kantorovich Rubinstein duality 최적화 기법으로 근사! (1-Lipschitz 함수에서 search)
  - $||f_L|| \le 1 $: (임의의 두점사이의 평균변화율이 1을 넘지 않는 함수)

    <img width="300" alt="스크린샷 2023-08-20 오후 8 45 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4526d2c8-7c01-47ea-87d0-788682d0f7d2">

- 선형 함수 공간에 대한 supremum(상한)을 계산 -> K-Lipschitz에 대해 검색 한다면, K배의 Wasserstein 거리를 줄 것이다!
- 만약, $𝑑_Y (𝑓 (𝑥_1), 𝑓 (𝑥_2)) ≤ 𝐾𝑑_x(𝑥_1,𝑥_2)$ 을 만족한다면, 𝑓 : 𝑋 → 𝑌 는 K-Lipschitz 이다!
- 𝑤 ∈ 𝑊에서, $𝑓_W$ 를찾는중이면,<img width="500" alt="스크린샷 2023-08-20 오후 9 07 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/669a7172-c893-4fa3-b32f-f4dd1dd45e11">
- $𝑔_\theta(𝑧)$ 에 의해 유도되는 $𝑃_Z$ 에 대해, <img width="350" alt="스크린샷 2023-08-20 오후 9 08 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4153de55-8ad1-48a4-9dc4-c439eba0097d"> 로 backpropagation을 하여 근사할 수 있다!
- 만약 보다 자세한 내용이 궁금하다면? https://www.alexirpan.com/2017/02/22/wasserstein-gan.html

## Wasserstein GAN (WGAN; 2016-2017) - Pseudo Code
<img width="500" alt="스크린샷 2023-08-20 오후 9 09 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/45f21776-df06-459c-8abe-dfc616b820e8">

## Wasserstein GAN (WGAN; 2016-2017) - 결과
<img width="350" alt="스크린샷 2023-08-20 오후 9 10 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/decaa549-fe47-41cc-a35b-ba9ffa602ba6">

## Wasserstein GAN (WGAN; 2016-2017) - 결과 ‒ WGAN은 아키텍처의 변화에도 robust하다!
<img width="350" alt="스크린샷 2023-08-20 오후 9 11 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4240c179-7440-43ea-b71f-ebb77c276f67">

## Wasserstein GAN (WGAN; 2016-2017) - 정리
장점
- Generator를 최적화하기 위한 분포간 거리를 새롭게 정의
- Janson-Shannon divergence (sigmoid cross entropy)의 불안정성을 해결
- Architecture 선택에 robust!
- Gan 학습을 안정화시키는 Lipschitzness 아이디어를 제시!
단점
- Lipschitz 는 만능이 아니다! (제약조건이 있다)
  - Lipschitz constraint를 지켜야 하며, 이를 위해 weight clipping이 강제되어야 한다.
  - 만약 이 clipping parameter가 크다면, 최적화하는데 매우 오래 걸릴 것이고, 작다면, gradient vanishing 이 일어날 가능성 역시 내포하고 있다.
  
-> Gradient penalty를 이용하면 clipping 없이 weight를 제한할 수 있다!

## Gradient penalty for Lipschitzness (WGAN-GP; 2017) - Gradient Penalty for Lipschitzness
<img width="400" alt="스크린샷 2023-08-20 오후 9 17 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/25aa3dc8-6653-4f03-9d2a-8bab10989929">

<img width="400" alt="스크린샷 2023-08-20 오후 9 17 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8f87113d-100f-4f43-8e5d-8c468e1639ea">

<img width="400" alt="스크린샷 2023-08-20 오후 9 18 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cbcbce47-06df-4df8-b9cd-e7bac0bc95ba">

$\hat 𝑥 ← 𝜖 𝑥 + (1 − 𝜖) \tilde 𝑥$

-  Weight clipping 시 발생할 수 있는 의도하지 않는 behavior(weight explode/vanish 등)를 완화!

<img width="450" alt="스크린샷 2023-08-20 오후 9 25 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/519dfa77-fc12-406d-a5bf-f6b5d14bba5f">

## Gradient penalty for Lipschitzness (WGAN-GP; 2017) - Pseudo code
<img width="700" alt="스크린샷 2023-08-20 오후 9 26 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bcf36acc-6c7a-434a-ad2f-50bd3a219020">

## Gradient penalty for Lipschitzness (WGAN-GP; 2017) - 결과
<img width="350" alt="스크린샷 2023-08-20 오후 9 27 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ba5e81be-4a90-468a-9b10-78a5a90d84b7">

<img width="350" alt="스크린샷 2023-08-20 오후 9 27 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/34af9e16-40e5-4d79-b6a1-89201265b634">

<img width="350" alt="스크린샷 2023-08-20 오후 9 27 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12678847-4f86-49a7-809a-374221667975">

## Progressive Growing of GAN (PGGAN; 2017)
<img width="550" alt="스크린샷 2023-08-20 오후 9 28 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4b181129-61ad-4275-832b-abe09cb57d8c">

- GAN은 고해상도 데이터 생성 학습이 어려움!
- 해상도가 낮으면 GAN에서 핵심이 되는 Generator가 Discriminator를 속이기 쉽다!
- 하지만 고화질이 될수록 Generator가 Discriminator를 속이기 힘들어지기에, 학습 속도도 매우 느리다!
- 이를 해결하기 위해 PRGAN이 한 것은,
- 먼저 낮은 해상도로 학습을 진행하고, 점진적으로 더 높은 해상도로 학습을 하길 반복하는 형식을 사용하면 어떨까?
  - Unet과 비슷한 아이디어!
- 결과: 보다 해상도 좋은 이미지!

## Progressive Growing of GAN (PGGAN; 2017) - 결과
<img width="359" alt="스크린샷 2023-08-20 오후 9 29 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2c908c0e-7887-4481-934b-68dffcc95d0d">

## Spectral Normalization GAN (SNGAN) (2018)
- Spectral normalization: Discriminator의 학습을 안정화시키는 일종의 weight normalization 방법!
- Lipschitz constant 을 control하기 위해 사용되고, tuning되어야하는 유일한 hyper-parameter.
  - Hyperparameter가 뭐여도 크게 상관없이 robust한 편!
- 실행이 간단하고 추가적인 계산 량이 적다.
- GAN: model의 분포와 discriminator을 번갈아가며 학습시켜 model의 분포가 target분포를 따라가게 만드는 것
  - 모두 학습할 때에 각 스텝에서 optimal한 discriminator에 의해 측정됐다는 가정이 있음
  - 그 만큼 discriminator의 성능이 GAN의 성능에 크게 중요!!
- But...high dimensional space 에서는?
  - 어렵다! discriminator가 학습하는 동안 부정확! - generator는 target distribution을 학습하는데어려움.
    - spectral normalization이 해결책이 될 수 있다

## Spectral Normalization GAN (SNGAN) (2018)
f가 있고 W를 파라미터, a를 activation함수라고 할 때, discriminator를 수식으로 표현해보면,

<img width="300" alt="스크린샷 2023-08-20 오후 9 34 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1b88db42-9bbb-49aa-837a-2b9b12a830d7">

Discriminator 의 output 은 아래와 같다.
- A: 거리 측정 척도(i.e. divergence)에 해당하는 activation 함수라 한다면,
- D(x,θ) =A(f(x,θ))

이 때, GAN 의 목적 함수 minmaxV(G,D)에서, optimal한 G는 아래와 같이 표현될 수 있다.

<img width="250" alt="스크린샷 2023-08-20 오후 9 35 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf9fe7e5-5d37-4a38-91d0-9c5df26a5317">

위 식의 gradient가 Lipschitz constraint를 만족할 수 있도록(WGAN과 같이),
Lipschitz constant를 bound 함으로써 D의 gradient가 explode하는 것을 막을 수 있다!

<img width="150" alt="스크린샷 2023-08-20 오후 9 36 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/296bedbb-ff22-4752-a35d-418aa82ffd9d">

Spectral normalization의 핵심 아이디어:
- Discriminator의 Lipschitzness를 각 layer의 spectral norm에 연결!

각 layer g: $h_in → h_out$ 가 주어졌을 때, spectral norm에 Lipschitz constraint를 건다!
- g에 대한 Lipschitz norm은 <img width="167" alt="스크린샷 2023-08-20 오후 9 38 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c125641b-f668-4813-a2ea-00b082e2425b"> 을 만족한다.
- linear layer 𝑔(h) = 𝑊h 고, spectral norm 식 𝝈(𝑨) 는 matrix A의 spectral norm이고, A의 L2 matrix norm이다.

<img width="170" alt="스크린샷 2023-08-21 오후 1 16 33" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4e1a6cda-e395-4cef-aa53-dbf2098035bd"> 이고 A의 가장큰 singular value므로,

<img width="250" alt="스크린샷 2023-08-21 오후 1 17 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a8923bd1-0b8f-4a23-a903-e0c111ebc9a6">으로 간단해진다.

만약, activation function $a_l$ 의 Lipschitz norm $||a_l||_Lip$ 이 1(i.e. ReLU, leaky ReLU)라면,
<img width="200" alt="스크린샷 2023-08-21 오후 1 18 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bdf8a2c1-fafb-416a-ba69-47f5c5b846ac">
가 된다.

<img width="300" alt="스크린샷 2023-08-21 오후 1 20 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3b95fc50-498e-4fcb-bc32-9437a41c5ce1">
에서,  

Discriminator를 구성하는 함수 <img width="300" alt="스크린샷 2023-08-21 오후 1 21 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4eda35e3-a534-484b-9e19-ff632b83735b">
의 Lipschitz norm으로 일반화하면, 아래의 bound를 만족한다!

<img width="500" alt="스크린샷 2023-08-21 오후 1 22 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/48fad701-b325-43fd-bbf5-4b9afbb6baf9">

최종적으로 Lipschitz constraint 𝜎(𝑊) = 1 을 만족하는 weight matrix W의 spectral norm은 아래와 같다.

<img width="120" alt="스크린샷 2023-08-21 오후 1 23 04" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/466716a0-9f59-4ac3-92a4-3af9bf1d6520">


최종적으로, 모든 layer의 weight matrix 𝑊% 을 spectral normalization 한다면, f의 Lipschitz norm 이 1이하로 bounded 된다!
각 update는 아래와 같이 hinge loss로 계산.

<img width="450" alt="스크린샷 2023-08-21 오후 1 24 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/365b9eca-6c93-49ec-9340-ca52866a0e2c">


## Spectral Normalization GAN (SNGAN) (2018) - 모델의 디테일
<img width="430" alt="스크린샷 2023-08-21 오후 1 25 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2835c71b-d4ed-48dc-ad64-c0b7e01e599a">
<img width="300" alt="스크린샷 2023-08-21 오후 1 26 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/112e2bc2-ef5f-4144-8cba-739781041a3e">

## Spectral Normalization GAN (SNGAN) (2018) - 결과
<img width="600" alt="스크린샷 2023-08-21 오후 1 26 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1d9b8ed8-1fe4-475b-bc1b-d52099cc420b">

- 높은 퀄리티 샘플 (imagenet정도의 scale에서)
- Class condition 상태에서 최초의 full imagenet에서도 작동하는 GAN (백만 샘플)
- WGAN-GP보다 빠르다!
- 프로그래밍 구현 난이도는 쉬운편!

## Conditional GAN / projection discriminator (2018) - Conditional 한 input을 어떻게 넣을까?
<img width="650" alt="스크린샷 2023-08-21 오후 1 37 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5666b9ff-8d47-4ae6-ae62-abb825ffd39b">
<img width="200" alt="스크린샷 2023-08-21 오후 1 39 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/796e7de4-2b04-4c91-ac54-00d200425313">

- GAN에서 conditional 한 input을 넣어 원하는 스타일로 GAN을 생성할 수 있다.
- concat, adversarial loss + classification, projection discriminator 등 여러 방법 존재 

## Self Attention GAN (SAGAN) (2018-2019)
<img width="500" alt="스크린샷 2023-08-21 오후 1 53 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e2610159-93d7-4626-9728-2160951fe133">

<img width="270" alt="스크린샷 2023-08-21 오후 1 55 14" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f1264df3-be68-4727-812a-77f6fff2f43b">

$\gamma$ : 0으로 시작했다가 점차 증가하여 점점 문제를 어렵게

## Self Attention GAN (SAGAN) (2018-2019) - 학습
Hinge loss 버전의 GAN Loss 사용

<img width="500" alt="스크린샷 2023-08-21 오후 1 58 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7a913c0f-2c2a-4365-b240-2f8b02a16f25">


- Spectral normalization을 generator와 discriminator에서 사용
  - Generator에도 적용할 시, 실험적으로 더 잘 됨 !
- Generator & discriminator의 다른 learning rate 선택
  - Discriminator가 regularized되어있으면 학습이 느리다 ! • Two time-scale update rule(TTUR ) 사용

## Self Attention GAN (SAGAN) (2018-2019) - 결과
<img width="400" alt="스크린샷 2023-08-21 오후 2 00 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4c55b3e2-eaf8-4e95-afde-1a61ceec79d5">

<img width="360" alt="스크린샷 2023-08-21 오후 2 01 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c73be787-a6cf-4293-85f8-a36a39c4f6ed">

## Self Attention GAN (SAGAN) (2018-2019) - Spectral normalization과 TTUR의 효과
<img width="400" alt="스크린샷 2023-08-21 오후 2 01 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7cbc0cf0-7fc3-4feb-917a-9f3af121ecaa">

## Self Attention GAN (SAGAN) (2018-2019) - 정리
- Hinge loss 버전의 GAN Loss 사용
- Spectral normalization을 generator와 discriminator에서 사용
  - 이론적으로 discriminator만 약하게 하거나 regularize 하면 된다는 유명한 믿음에 대한 반례.
- Generator & discriminator의 다른 learning rate 선택
  - Discriminator가 regularized되어있으면 학습이 느리다 !
  - Two time-scale update rule(TTUR ) 사용

- Unconditional 상태에서 최초의 full imagenet에서도 좋게 작동하는 GAN

## BIG GAN (2018-2019) - 이제 깊게 쌓아보자...?
<img width="450" alt="스크린샷 2023-08-21 오후 2 03 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/df29ba95-c0e0-4d00-a807-a694fd9ca383">

<img width="250" alt="스크린샷 2023-08-21 오후 2 03 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d2b9b2b2-33bf-4ffc-aa96-9fb02d57d464">

<img width="450" alt="스크린샷 2023-08-21 오후 2 04 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1ef583b5-645f-473b-a7a9-856b79774eb6">

<img width="250" alt="스크린샷 2023-08-21 오후 2 04 57" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5926f6eb-5551-4e1b-a93f-7553c113c454">


## BIG GAN (2018-2019) - Contribution details...?
- Batch를 최대한 크게! 모델 사이즈를 크게!
- 깊게한만큼이미지를넓게(크게쓰는것)도도움이된다!
- Class information을 각 layer level에 넣으면 도움이 된다!
- Hinge Loss를 사용
- Truncation trick이 쓰였다.
  - GAN에서는 보통 임의의 p(z)를 이용하나 대부분 기존 모델은 N(0, 1) 혹은 U(-1, 1)에서 z추출!
  - 대신, Z~N(0, I)에서 truncated normal에서 샘플링을 적용하면 IS, FID를 향상시킬 수 있다.
- L2 orthonormal regularization 이 쓰였다. (beta=10^-4 사용)

  <img width="250" alt="스크린샷 2023-08-21 오후 2 09 48" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a99a6683-f7c8-4356-8ab2-9a5faa24f617">

<img width="253" alt="스크린샷 2023-08-21 오후 2 10 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c2812b3d-f052-4339-abe7-a581e0a1fa2e">

## 그밖에 주목할 만한 GAN...
https://github.com/nightrome/really-awesome-gan
