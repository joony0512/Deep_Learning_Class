# <Generative model series #4> - Implicit Models - 5. GAN의 발전 - part 2
## Wasserstein GAN (WGAN; 2016-2017) - Wasserstein 거리 + Kantorovich Rubinstein Duality

Vanilla GAN

<img width="300" alt="스크린샷 2023-08-20 오후 8 13 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/00ad736b-5af7-4f2c-8d2b-7e6642aecb5b">

Wasserstein GAN

<img width="300" alt="스크린샷 2023-08-20 오후 8 13 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/455222a3-4326-42ac-95fa-7b0915824ee9">

- Optimal Transport 이론에서 영감을 얻어 만든 distance -> Earth Mover distance (EMD)를 이용!

  <img width="287" alt="스크린샷 2023-08-20 오후 8 14 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5b039b3f-6079-48c4-b779-2c7b495a4fa1">

- Estimate하기 intractable하다!
- Kantorovich Rubinstein duality 최적화 기법으로 근사! (1-Lipschitz 함수에서 search)
  
  <img width="315" alt="스크린샷 2023-08-20 오후 8 15 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/71565cac-6cca-4d3c-87c7-6c21144eea1d">

## Wasserstein GAN (WGAN; 2016-2017) - Distance/Divergence의 종류
1. Total variation(TV)
- 두 확률 분포의 측정값이 벌어질 수 있는 가장 큰값

 <img width="200" alt="스크린샷 2023-08-20 오후 8 34 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4b5e0ca1-d4ab-4f26-a887-9a52578c5b51">

2. Kullback-Leibler(KL) divergence

 <img width="200" alt="스크린샷 2023-08-20 오후 8 35 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a844db5f-19b1-405b-9017-e289d5644992">

3. Jensen-Shannon(JS) divergence

   <img width="200" alt="스크린샷 2023-08-20 오후 8 35 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6429ba92-3ca7-45f4-97e3-5d1adf0e3c6a">

4. Earth-Mover (EM) distance

   <img width="200" alt="스크린샷 2023-08-20 오후 8 36 22" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/35cf4f3e-f5a2-4a65-bea6-dd418bae4636">

  - 두 확률 분포의 결합확률 분포 $\pi (𝑃_r, 𝑃_g) )$ 중에서 d(X,Y)(x와y의거리)의 기대값을 가장 작게 추정한 값

## Wasserstein GAN (WGAN; 2016-2017) - EM 거리의 타당성
<img width="400" alt="스크린샷 2023-08-20 오후 8 39 21" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d53315d4-1e42-476c-bef3-01726ee7e49c">

- 임의의 distribution 𝑃R, 𝑃? 를 정의하고 그 확률 사이의 거리를 구하면,
- Wasserstein 거 리 (EM distance) 는 theta 에 관계없이 일정한 수식으로 이루어져 있으나, 다른 수식은 상수이거나 발산하는 등의 문제가 있다!
- 만약, θS → 0로가면, 유일하게 수렴하는 것은 EM 거리 뿐! -> 학습에 사용하기 쉽다!

## Wasserstein GAN (WGAN; 2016-2017) - WGAN의 critic
<img width="508" alt="스크린샷 2023-08-20 오후 8 41 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/af3c8b22-e960-4c4c-8efb-e3150ab330b5">

- WGAN은 discriminator대신 critic을 사용!
- Discriminator가 진짜/가짜를 구분하기 위한 sigmoid를 씌운 확률값이라면,
- Critic의 결과는 (EM distance)로 부터 얻은 scalar이다.  
-> Step function처럼 작동하지 않기 때문에, 더이상 discriminator, generator의 밸런스를 위해 고민하지 않아도 된다!

## Wasserstein GAN (WGAN; 2016-2017) - Wasserstein distance는 샘플의 퀄리티와 상호 연관 관계가 있다.
<img width="548" alt="스크린샷 2023-08-20 오후 8 42 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0568b80d-3c41-4478-9c82-a95b1692b2f6">

## Wasserstein GAN (WGAN; 2016-2017) - Wasserstein 거리 + Kantorovich Rubinstein Duality
- Kantorovich Rubinstein duality 최적화 기법으로 근사! (1-Lipschitz 함수에서 search)
  - $||f_L|| \le 1 $: (임의의 두점사이의 평균변화율이 1을 넘지 않는 함수)

    <img width="300" alt="스크린샷 2023-08-20 오후 8 45 27" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4526d2c8-7c01-47ea-87d0-788682d0f7d2">

- 선형 함수 공간에 대한 supremum(상한)을 계산 -> K-Lipschitz에 대해 검색 한다면, K배의 Wasserstein 거리를 줄 것이다!
- 만약, $𝑑_Y (𝑓 (𝑥_1), 𝑓 (𝑥_2)) ≤ 𝐾𝑑_x(𝑥_1,𝑥_2)$ 을 만족한다면, 𝑓 : 𝑋 → 𝑌 는 K-Lipschitz 이다!
- 𝑤 ∈ 𝑊에서, $𝑓_W$ 를찾는중이면,<img width="500" alt="스크린샷 2023-08-20 오후 9 07 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/669a7172-c893-4fa3-b32f-f4dd1dd45e11">
- $𝑔_\theta(𝑧)$ 에 의해 유도되는 $𝑃_Z$ 에 대해, <img width="350" alt="스크린샷 2023-08-20 오후 9 08 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4153de55-8ad1-48a4-9dc4-c439eba0097d"> 로 backpropagation을 하여 근사할 수 있다!
- 만약 보다 자세한 내용이 궁금하다면? https://www.alexirpan.com/2017/02/22/wasserstein-gan.html

## Wasserstein GAN (WGAN; 2016-2017) - Pseudo Code
<img width="500" alt="스크린샷 2023-08-20 오후 9 09 37" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/45f21776-df06-459c-8abe-dfc616b820e8">

## Wasserstein GAN (WGAN; 2016-2017) - 결과
<img width="350" alt="스크린샷 2023-08-20 오후 9 10 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/decaa549-fe47-41cc-a35b-ba9ffa602ba6">

## Wasserstein GAN (WGAN; 2016-2017) - 결과 ‒ WGAN은 아키텍처의 변화에도 robust하다!
<img width="350" alt="스크린샷 2023-08-20 오후 9 11 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4240c179-7440-43ea-b71f-ebb77c276f67">

## Wasserstein GAN (WGAN; 2016-2017) - 정리
장점
- Generator를 최적화하기 위한 분포간 거리를 새롭게 정의
- Janson-Shannon divergence (sigmoid cross entropy)의 불안정성을 해결
- Architecture 선택에 robust!
- Gan 학습을 안정화시키는 Lipschitzness 아이디어를 제시!
단점
- Lipschitz 는 만능이 아니다! (제약조건이 있다)
  - Lipschitz constraint를 지켜야 하며, 이를 위해 weight clipping이 강제되어야 한다.
  - 만약 이 clipping parameter가 크다면, 최적화하는데 매우 오래 걸릴 것이고, 작다면, gradient vanishing 이 일어날 가능성 역시 내포하고 있다.
  
-> Gradient penalty를 이용하면 clipping 없이 weight를 제한할 수 있다!

## Gradient penalty for Lipschitzness (WGAN-GP; 2017) - Gradient Penalty for Lipschitzness
<img width="400" alt="스크린샷 2023-08-20 오후 9 17 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/25aa3dc8-6653-4f03-9d2a-8bab10989929">

<img width="400" alt="스크린샷 2023-08-20 오후 9 17 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8f87113d-100f-4f43-8e5d-8c468e1639ea">

<img width="400" alt="스크린샷 2023-08-20 오후 9 18 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cbcbce47-06df-4df8-b9cd-e7bac0bc95ba">

$\hat 𝑥 ← 𝜖 𝑥 + (1 − 𝜖) \tilde 𝑥$

-  Weight clipping 시 발생할 수 있는 의도하지 않는 behavior(weight explode/vanish 등)를 완화!

<img width="450" alt="스크린샷 2023-08-20 오후 9 25 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/519dfa77-fc12-406d-a5bf-f6b5d14bba5f">

## Gradient penalty for Lipschitzness (WGAN-GP; 2017) - Pseudo code
<img width="700" alt="스크린샷 2023-08-20 오후 9 26 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bcf36acc-6c7a-434a-ad2f-50bd3a219020">

## Gradient penalty for Lipschitzness (WGAN-GP; 2017) - 결과
<img width="350" alt="스크린샷 2023-08-20 오후 9 27 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ba5e81be-4a90-468a-9b10-78a5a90d84b7">

<img width="350" alt="스크린샷 2023-08-20 오후 9 27 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/34af9e16-40e5-4d79-b6a1-89201265b634">

<img width="350" alt="스크린샷 2023-08-20 오후 9 27 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12678847-4f86-49a7-809a-374221667975">

## Progressive Growing of GAN (PGGAN; 2017)
<img width="550" alt="스크린샷 2023-08-20 오후 9 28 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4b181129-61ad-4275-832b-abe09cb57d8c">

- GAN은 고해상도 데이터 생성 학습이 어려움!
- 해상도가 낮으면 GAN에서 핵심이 되는 Generator가 Discriminator를 속이기 쉽다!
- 하지만 고화질이 될수록 Generator가 Discriminator를 속이기 힘들어지기에, 학습 속도도 매우 느리다!
- 이를 해결하기 위해 PRGAN이 한 것은,
- 먼저 낮은 해상도로 학습을 진행하고, 점진적으로 더 높은 해상도로 학습을 하길 반복하는 형식을 사용하면 어떨까?
  - Unet과 비슷한 아이디어!
- 결과: 보다 해상도 좋은 이미지!

## Progressive Growing of GAN (PGGAN; 2017) - 결과
<img width="359" alt="스크린샷 2023-08-20 오후 9 29 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2c908c0e-7887-4481-934b-68dffcc95d0d">

## Spectral Normalization GAN (SNGAN) (2018)
- Spectral normalization: Discriminator의 학습을 안정화시키는 일종의 weight normalization 방법!
- Lipschitz constant 을 control하기 위해 사용되고, tuning되어야하는 유일한 hyper-parameter.
  - Hyperparameter가 뭐여도 크게 상관없이 robust한 편!
- 실행이 간단하고 추가적인 계산 량이 적다.
- GAN: model의 분포와 discriminator을 번갈아가며 학습시켜 model의 분포가 target분포를 따라가게 만드는 것
  - 모두 학습할 때에 각 스텝에서 optimal한 discriminator에 의해 측정됐다는 가정이 있음
  - 그 만큼 discriminator의 성능이 GAN의 성능에 크게 중요!!
- But...high dimensional space 에서는?
  - 어렵다! discriminator가 학습하는 동안 부정확! - generator는 target distribution을 학습하는데어려움.
    - spectral normalization이 해결책이 될 수 있다

## Spectral Normalization GAN (SNGAN) (2018)
f가 있고 W를 파라미터, a를 activation함수라고 할 때, discriminator를 수식으로 표현해보면,

<img width="300" alt="스크린샷 2023-08-20 오후 9 34 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1b88db42-9bbb-49aa-837a-2b9b12a830d7">

Discriminator 의 output 은 아래와 같다.
- A: 거리 측정 척도(i.e. divergence)에 해당하는 activation 함수라 한다면,
- D(x,θ) =A(f(x,θ))

이 때, GAN 의 목적 함수 minmaxV(G,D)에서, optimal한 G는 아래와 같이 표현될 수 있다.

<img width="250" alt="스크린샷 2023-08-20 오후 9 35 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf9fe7e5-5d37-4a38-91d0-9c5df26a5317">

위 식의 gradient가 Lipschitz constraint를 만족할 수 있도록(WGAN과 같이),
Lipschitz constant를 bound 함으로써 D의 gradient가 explode하는 것을 막을 수 있다!

<img width="150" alt="스크린샷 2023-08-20 오후 9 36 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/296bedbb-ff22-4752-a35d-418aa82ffd9d">

Spectral normalization의 핵심 아이디어:
- Discriminator의 Lipschitzness를 각 layer의 spectral norm에 연결!

각 layer g: $h_in → h_out$ 가 주어졌을 때, spectral norm에 Lipschitz constraint를 건다!
- g에 대한 Lipschitz norm은 <img width="167" alt="스크린샷 2023-08-20 오후 9 38 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c125641b-f668-4813-a2ea-00b082e2425b"> 을 만족한다.
- linear layer 𝑔(h) = 𝑊h 고, spectral norm 식 𝝈(𝑨) 는 matrix A의 spectral norm이고, A의 L2 matrix norm이다.
