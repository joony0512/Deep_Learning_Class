# Deep Learning Production - 3. Framework Agnostic & Model Exchange
## 프레임워크 불가지론(Framework Agnostic)적 설계 - 딥러닝 서비스를 위해 프레임워크를 따라가야 할까?
- “프레임워크는 도구일 뿐, 삶의 방식은 아니다” ‒ 로버트 C. 마틴 ‒ 클린 아키텍처 (소프트웨어 구조와 설계의 원칙)  
- SW 아키텍처적으로 서비스가 프레임워크에 모두 묶여버리는 것은 좋지 않다. 몇몇 버려진 불우의 오픈소스 ML 프로젝트들을 보자
  - i.e. TensorPack ‒ TF1 시절 촉망 받던 3rd-party 프로젝트였으나, TF2로 변환되며 Support을 그만두었다.
- Recap: Model Formatter & Edge Computing
  - 모델을 통합하려는 목적 혹은 edge / mobile computing을 위해 모델의 포맷을 바꿀 수 있다.
  - 모델 변환이 가능하다!
    - 다이렉트로 변환(Tensorflow checkpoint -> Pytorch, Pytorch StateDict -> Tensorflow)
   
## 다이렉트 모델 변환 - 모델 변환 (TF -> Pytorch)
필요한 것
- Tensorflow 에 해당하는 코드를 Pytorch nn.Module로 재구현이 필요
- Checkpoint에서라면, variable의 이름을 tf.train.list_variables 등으로 미리 파악 필요

모델에서 변환 방법의 예 (각각 tf.Module, torch.nn.Module 일 때) 
- tf_weights = tf_model.get_weights()
- torch_model.weight.data = torch.from_numpy(tf_weights[0])

Checkpoint 파일에서 변환 방법의 예
- numpy_weight = tf.train.load_variables(”tf_checkpoint_path", "variable_name")
- torch_model.weight.data = torch.from_numpy(numpy_weight)

잠깐!
- torch.model.weight.data? -> torch.nn.parameter.Parameter를 weight로 사용하기 때문

## 다이렉트 모델 변환 - 모델 변환 (Pytorch -> TF)
필요한 것
- Pytorch 모델을 Tensorflow로 재구현이 필요
  
모델에서 변환 방법의 예 (각각 tf.Module, torch.nn.Module 일 때)  
  w = torch_model.state_dict()["weight"].detach().numpy() # state_dict이후 weight numpy 변환   
  b = torch_model.state_dict()["bias"].detach().numpy() # state_dict이후 bias numpy 변환  
  tf_model.set_weights([w, b])

Checkpoint 파일(Pytorch State Dict)에서 변환 방법의 예  
  torch_state_dict = torch.load(“model_path”, map_location=torch.device("cpu")) # cpu로 로드   
  torch_state_dict = {k: v.detach().numpy() for k, v in torch_state_dict.items()} # numpy 변환   
  tf_model.set_weights( [  
    torch_state_dict["some-keys-for-weight"],  
    torch_state_dict["some-keys-for-bias"] ] ) # set_weights로 설정.  

## ONNX를 통한 모델 변환 - ONNX?
Direct의 모델 변환은 받는 쪽에서 Tensorflow, Pytorch로의 모델을 정의 해줘야 한다. 
  - 경우에 따라 오히려 유지관리 비용이 X2 배가 되기도 한다. 둘 중 한 곳의 버전업 등으로 모듈이 바뀐다면...?
    
ONNX (Open Neural Network eXchange): https://github.com/onnx/onnx  
- 목적: 프레임워크간 상호 운용성과 공통된 최적화
- Facebook(현 Meta) Pytorch Team에 의해 최초로 시작되었다가, MS 등에 의해 공용개발
- 현재는 Linux Foundation AI에 의해 관리되고 있음.

- 장점: 상당수의 모델에 호환이 되게 작동하며, 작동이 완료되면 모델의 저장, 관리 등에서 상당 부분 단순해진다 !
- 단점: ONNX가 TF와 Pytorch보다 느리다. 복잡한 연산이거나 최신 기능(low-level API)일 수록 호환이 안될 수도 있다.

## ONNX를 통한 모델 변환 - ONNX 가 호환되는 프레임워크/컨버터들
<img width="500" alt="스크린샷 2023-09-03 오후 5 56 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d1781a7d-5925-4ac3-8840-194128b9e048">

## ONNX를 통한 모델 변환 - ONNX를 통한 모델 변환 및 서비스 활용은...?
<img width="500" alt="스크린샷 2023-09-03 오후 6 00 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/29363bac-f6c8-4158-9e1b-1f23c30f9a70">

## 참고: 만약 GPU Inference을 보다 효율적으로 이용하고자 한다면?
<img width="300" alt="image" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0eef7dda-6540-4a16-ac26-26f68cde3ab9">

GPU 친화적인 Nvidia 사의 TensorRT로 변환 하거나, Triton Inference Server(Formerly TensorRT Inference Server)을 이용하는 것을 검토 가능하다.
- Traffic이 많거나 모델이 매우 큰 경우에 주로 필요하면서 상대적으로 공수가 있는 편이기에, 스타트업보다는 대기업 중심으로 주로 도입되는 추세.
- Pytorch, Tensorflow, ONNX 모두 Support하거나 변환 가능!
