## CH04_08_보다나은최적화를위한다른알고리즘들_

## 좌표 경사 하강(coordinate descent)

- Cooldinate descent는 나머지를 고정하고 하나의 파라미터씩 최적화하는 방법을 말한다.
    
    <img width="298" alt="스크린샷 2023-06-27 오후 8 37 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/311c8bda-f05e-4e11-8e06-c3e5f5f4054d">

    
    - Good
        - 각 변수들이 다른 그룹으로 깔끔하게 분리 될 수 있을때
        - 각 변수를 따로 최적화하는 것이 한번에 최적화하는 것보다 효율적일때
    - Not good
        - 한 변수의 값이 다른 최적화 값에 영향을 줄 수 있을때
            
            <img width="298" alt="스크린샷 2023-06-27 오후 9 01 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b1b9a930-156a-4664-b839-f8135f595481">

            

## Polyak 평균 (Polyak Averaging)

- 여러 포인트에서 step을 한 것을 평균을 내어 최종 update을 결정한다.
- 즉 gradient descent가 각 지점 $\theta^1,...,\theta^k$을 방문했을때, Polyak averaging 값은 아래를 만족한다.
$\hat \theta^t = {{1}\over{t} }\Sigma _i \theta_i$
- Non convex 문제에서는 모든 trajectory를 계산하기 어렵기 때문에, 큰범위를 고르지 않기 위해 exponentially decaying average기법등을 보통 같이 적용한다.
$\hat \theta^t = \alpha \hat \theta^{t-1} + (1-\alpha) \theta^t$
    
    <img width="129" alt="스크린샷 2023-06-27 오후 9 10 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/cf62951e-9870-4fc4-984d-e02ccd4d8627">

    

## 지속적방법 (continuation methods)

- Continuation methods는 지속적으로 목표 함수 자체를 업데이트 하며, 초기화하여 학습하는 방법들을 말한다!
    - 한번에 목적 손실 함수의 전 범위의 구조를 파악하기 어려울 때 효과가 있다!
    - 보통 실제 풀어야 하는 최적화 문제보다 쉬운 최적화 문제로 blurring 등 한 후, 점점 실제 최적화 해야하는 함수로 간다.
    - Non-convex 함수에서 이득!
        
  <img width="155" alt="스크린샷 2023-06-27 오후 9 13 09" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7106f418-a705-4869-aad4-6465a5b8fe2b">

        

## 커리큘럼학습 (curriculum learning)

- 실제 사람의 학습과정에 기반하여, 간단한 것에서 시작해서 점점 더 복잡한 방법을 더 의미있는 순서로 구성해 학습한다. 즉, 처음에는 모델에 쉬운 샘플만 보여주다가 점차 어려운 샘플을 보여주는 것.
- Non-convex 학습 기준에서 큰 성능 향상을 나타내는 것으로 보통 알려져있다.
- Continuation method의 일종.
- 일반화(generalization)와 빠른 수렴 속도!
- 쉬운 샘플을 정의하는 방법 예시:
    - 첫 번째는 노이즈의 개수로 판단하는 것
    - Gaussian 분포의 boundary 에서 margin 거리를 활용하는 방법
        - margin 거리가 가까울수록 쉽고, 멀수록 어려운 샘플이라 정의
     <img width="172" alt="스크린샷 2023-06-27 오후 9 16 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/66ba9217-a78c-48b9-8ecb-2a4f0e9157ba">

            
