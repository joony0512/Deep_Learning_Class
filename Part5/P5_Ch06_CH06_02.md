# 순환 신경망 (RNN) - 2. Backpropagation Through Time과 Long-term Dependency

## RNN의 Backpropagation Through Time (BPTT)
<img width="395" alt="스크린샷 2023-07-13 오후 9 13 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/78bce9de-0d77-43d6-839d-3ccb16b6679e">

- RNN에 cross entropy를 사용한 error는 time 단위로 자르고 더한값인 <img width="292" alt="스크린샷 2023-07-13 오후 9 14 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/54443e04-a0a2-4bfd-bf7b-be74e6cb540e"> 일 것이다.

- 이때 파라미터 V, W에 대해 backpropagation해보자.
  - $E_t = - y_t log \hat{y}_t$
  - V 에 대해 : <img width="292" alt="스크린샷 2023-07-13 오후 9 16 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a5c7e6a4-4e3d-42d7-8d48-620f0f8f8bef">
  - W 에 대해 : <img width="182" alt="스크린샷 2023-07-13 오후 9 20 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2c2bb875-46b9-481f-82dc-8b415a824a09">
    - 이때 W가 모든 timestamp에서 공유되므로, <img width="128" alt="스크린샷 2023-07-13 오후 9 21 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/705a6f50-896b-401f-ac19-92dea5cc1604">

## Vanishing gradient 문제와 long-term dependency 
<img width="190" alt="스크린샷 2023-07-13 오후 9 22 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/30c6cce3-751f-4cc7-8287-44b4f09317e6">
- 만약 인풋과 아웃풋 사이가 커진다면
  - tanh, sigmoid등 함수의 미분값은 1보다 작으므로, gradient 값은 더 작아질 것이다.
  - gradient가 사라진다
  - 이것이, RNN이 Long-term Dependency를 학습하는것을 어렵게 한다.
- 한편, gradient가 매 스텝별로 크다면, gradientsms 반대로 explode할 것이다.
  - Gradient clipping으로 해결 할 수 있다. <img width="510" alt="스크린샷 2023-07-13 오후 9 26 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5b6be930-b49e-4cd7-83a0-8dd472d94ae0">

## long-term dependency 해결방법
1. 정규화기법
   - Gradient vector $\partial E \over \partial h_t$ 의 크기를 유지하게 강제하면, <img width="134" alt="스크린샷 2023-07-13 오후 9 30 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/20642f06-12d9-461b-a9f6-317455624979"> 로 표현할 수 있다.
   - 이를 regulizer로 표현하면, <img width="177" alt="스크린샷 2023-07-13 오후 9 31 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf56d90a-2bcf-4b13-907e-c4cd51d12a2e">

2. ReLU 활성화
   - 한편, ReLU activation을 사용하면, 어느정도 문제를 완화시킬 수 있다.
   - ReLU의 미분값은 0, 1이므로 tanh, sigmoid 등 대비 Gradient 가 saturation 되는 것을 막을 수 있다.
  
3. 모델의 업데이트
   - 모델 발전시키기
     <img width="533" alt="스크린샷 2023-07-13 오후 9 33 07" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/81ea2128-d7fb-46dc-885c-5a03a8bb4a8a">

