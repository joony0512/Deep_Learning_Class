# CH01_05.[TheorySession1]경험적위험도(empirical risk)와ML의일반화(generalization)

### 일반화에러(genealization error), 실제위험도(treu risk)

- 새로운 데이터에 대하여 모델f의 실제 위험도를 
$R(f) = E[ L(f(x)), y ] = ∫ L(f(x)), y dP(x,y)$ 라고 정의한다.
- 이때 이 값을 일반화에러(genealization error)라고도 한다
- 딥러닝을 포함한 머신러닝의 목표는 일반화 에러를 최소화 하는 최고의 모델 f를 찾는것이다.
$f* = arg min R(f)  (f∈F)$ : 실제 위험도 최소화하기

### 경험적위험도

- 우리는 실제 환경 데이터의 확률 분포 $P(x,y)$를 알지 못한다.
- 즉, 실제위험도(risk error)를 알 수 없다.
- 그렇기 때문에 경험적 위험도(empirial risk)를 대신 최소화하여 근사한다!
    - $R_{emp}(f) = 1/n *  \Sigma L(f(x_i),y_i)$
    - $\hat{f}= arg\min R_{emp}(f)$
- 이러한 최소화 과정을 경험적 위험도 최소화(ERM)라고한다. → train/test를 따로 두는이유

### 모델 수용량(model capacity)조절

- 모델 수용량을 조절하는 것은 앞의 모델을 의미하는 f ∈ F에서 F를 조절한다는 의미
    - F : 가설(모델) f가 될 수 있는 모든 경우를 나타내는 집합, 가설공간(hypothesis space)
    - 쉽게 2차원원으로 이야기하면, 모델의 학습 가능하여 최적화가 가능한 부분이 $\theta$라고 한다면, $f(\theta)$가 될 수 있는 모든 공간을 가설공간이라고 할 수있다.
     $f(\theta) = \hat{y} = b + \theta x$
    - ANN은 레이어의 수나 뉴런의 수 조절
    
- 간단한 모델 일수록 일반화가 잘되지만, 보다 나은 성능을 위해 적합한 복잡성을 가진 모델을 골라야한다
    - 전형적으로 모델 복잡성이 커지면 커질수록 train error는 한계까지 계속 내려간다.
    
    ![Untitled (7)](https://user-images.githubusercontent.com/109457820/229129006-9a20fd68-d560-4941-b77a-156ec9f47bca.png)
