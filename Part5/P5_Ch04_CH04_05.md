
## 확률론적 경사 하강법(stochastic gradient descent; SGD)
<img width="399" alt="스크린샷 2023-06-26 오후 7 15 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3d951455-63e3-4d2e-be8c-9344a31802ce">


- SGD는 ML, DNN을 위해 가장 널리 사용되는 Optimizer이다.
    - 중요한 부분은 학습률(learning rate)인 $\epsilon$ 을 고르는 것이다.
        
        <img width="469" alt="스크린샷 2023-06-26 오후 7 17 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5617d01a-f7bb-4211-a730-2f5f2ca7b99f">

        
    - 학습률을 고르는 것에 따라 발산하기도 하고 최적의 값으로 수렴하기도 한다.

## 학습률(learning rate)

- 학습률인 $\epsilon_k$를 고정하고 생각해보자.
    - SGD에서 $\epsilon_k$가 아래를 만족하면 수렴함이 증명되어있다고 한다.
        
        <img width="184" alt="스크린샷 2023-06-26 오후 7 20 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/92896480-ba02-4cb3-a8ed-2d295c683e28">

        
    - 실제로는 학습률을 고정하지는 않고, 점차적으로 감소시키는 decay를 많이 적용한다.
    - $\epsilon_\tau$까지 학습률을 decay한다면 :
    $\epsilon_k = (1-\alpha) \epsilon_0 + \alpha \epsilon_\tau$
- SGD의 excess error(최적값과의 차이)
$J(\theta) - min_\theta J(\theta)$ 이다.
- 만약 SGD가 컨벡스 최적화 문제에 적용된다면, excess error는 k번째 업데이트후, $O(1/ \sqrt k)$ 이다.
- 만약 강한 convex라면, $O(1/ k)$로 알려져 있다. → $O(1/ k)$보다 더 빠르게 줄일수 없을까?

## 모멘텀(momentum)

- SGD가 느리게 수렴하는 것을 개선하고, local optima에 빠지는 걸 조금 방향성을 주어 최적을 가속화한다면 어떨까?
- 모멘텀 사용
    
    <img width="502" alt="스크린샷 2023-06-26 오후 7 31 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6a105f77-05e5-4d30-a4f5-b0781f32882f">

    
    $v \leftarrow \alpha v -\epsilon \nabla_\theta$  $1\over m$ $\Sigma^m_i L(f(x^{(i)} ; \theta), y^{(i)}))$ : $\alpha v$가 momentum step, $\epsilon$뒤가 gradient step이다.
    
    $\theta \leftarrow \theta + v$
    
- 이는 크게 SGD의 2가지 문제를 해결해준다.
    1. **Hessian matrix의 poor conditioning**
        - Hessian이 poor condition이면, 미분 값이 축에 따라 어떤 방향으로는 급격하게 커지고, 다른 쪽은 천천히 커지는 상태가 되어 결과적으로 SGD의 퍼포먼스가 떨어지게 된다.
    2. **SGD의 지나친 분산(variance)**
        - 학습의 안정성

## ****Nesterov 모멘텀 (Nesterov accelerated gradient method; Nesterov momentum)****

- $v \leftarrow \alpha v -\epsilon \nabla_\theta$  $1\over m$ $\Sigma^m_i L(f(x^{(i)} ; \theta + \alpha v), y^{(i)}))$ : $\alpha v$가 momentum step, $\epsilon$뒤가 gradient step이다.
- 현재 속도를 반영하고 gradient를 계산 : 일종의 교정시도
    - convex이고, batch gradient로 계산한다면, ****Nesterov**** 모멘텀의convergence rate of excess error는 
    $O(1/ k)$ ~$O(1/ k^2)$
    정도로 낮아진다 (SGD는 변화 X)
    - 또한 일반 모멘텀보다 좋다고 실험적으로 알려져있다.

## ****적응할 수 있는 학습율 (Adaptive learning rate)****

- 여러 하이퍼 파라미터들 중,
    
    learning rate는 모델 성능에 엄청난 영향을 미치기 때문에, 튜닝 중 가장 중요한 하이퍼 파라미터이고 또 튜닝하기 어렵다고 알려져 있다.
    
    - 보통, 손실 함수는 파라미터 공간에 어떤 특정 구역에서만 민감하고 나머지 구역에서는 민감하지 않다!이때, 일부 차원에서 큰 learning rate을 허락하고 나머지는 작은, 상황에 따라 적응하게(adaptive) 하면 어떨까?
- Learning rate를 각 파라미터 별로 나누자!

## AdaGrad

- 모든 모델의 파라미터의 learning rate을 각각 적응시킨다.
- Gradient의 RMS(root mean square)의 누적값을 이용해 파라미터를 scaling한다.
- 알고리즘 ($\bigodot$는 element wise multiplication)
    1. Gradient계산  ;$g \leftarrow$  $1\over m$ $\Sigma^m_i L(f(x^{i} ; \theta), y^{i}))$
    2. Gradient의 제곱을 축적 : $r \leftarrow r + g \bigodot g$
    3. 업데이트 진행 : $\theta \leftarrow -$  $\epsilon \over \delta + \sqrt r$ $\bigodot g$
    
    여기서 $\sqrt r$ 즉, historical gradient가 크다면 작은 learning rate가 된다.
    
    큰 부분 미분값을 가진 파라미터 → learning rate의 급격한 변화
    
    작은 부분 미분값을 가진 파라미터 → learning rate의 작은 변화
    

## RMSProp

- 만약 AdaGrad가 non-convex함수에 적용되면 전체 gradient의 제곱( $r \leftarrow r + g \bigodot g$ )에 취해진값을 기반으로 얻은 learning raterk wnfdjemsek.
    - learning rate 가 convex structure에 들어오기 전에 이미 너무 작아져 버리게 된다.
- Exponentially weighted moving average 를 취해서 개선하자. by Hinton
    
    $r \leftarrow pr + (1-p)g\bigodot g$
    
- RMSprop은 과거의 정보를 조금 날려 AdaGrad의 단점을 완화한다.

## Adam

- RMSProp와 momentum을 결합하자.
- momentum에서 moving average:
    - $s \leftarrow p_1 s + (1-p_2)g$, $\hat s \leftarrow$  $s\over 1-p_t ^1$
- RMSProp :
    - $r \leftarrow p_2r + (1-p_2)g\bigodot g$, $\hat r \leftarrow$ $r\over 1-p_2^t$
- Update :
    - $\nabla \theta = -\epsilon$  $\hat s \over \delta + \sqrt {\hat r}$

## AdamP

- 한편 Adam에서 momentum 부분은 업데이트는 빠르나 너무 Noam이 큰 단점이 있다.
- Projection을 하여 momentum- induced effective step size decreases를 막자.
- projection으로 radical한 부분 제거 :
        
        
<img width="168" alt="스크린샷 2023-06-26 오후 8 25 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1bb58ad8-c2f9-4b69-95d5-6156cd20b6f3">

        
  - update rule:
            
<img width="274" alt="스크린샷 2023-06-26 오후 8 27 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/12368344-1310-4976-bcdb-13b8db7e41fd">

<img width="392" alt="스크린샷 2023-06-26 오후 8 27 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d6b5ea34-3aa5-4765-9e7b-37cad2bed759">

            

        
        
        
    
    ## SGD optimizer 비교
    
<img src ="https://cs231n.github.io/assets/nn3/opt2.gif">
    
<img src = "https://cs231n.github.io/assets/nn3/opt1.gif">
    
    - 모멘텀 기반이 빠르지만, overshoot 되는 현상!
    - 단순 SGD는 Saddle point를 탈출 못했다.
