# <Generative model series #4> - Implicit Models - 1. Implicit Models(암시적 모델)
## Recap: Deep generative model의 종류 - 암시적 모델 (Implicit Models)
<img width="402" alt="스크린샷 2023-08-18 오후 3 06 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ccde27bf-96b2-4935-bd68-c27799843e76">

Implicit density estimation 
- $p_{model} (x)$ 를 직접 구하지 않아도 샘플을 생성할 수 있다.

Explicit density estimation
- $p_{model} (x)$ 를 구하기 위해 loss등에 직접 반영하고 최적화한다.
- 우리가 지금까지 봐왔던 부분이다.
- Glow는 배우지 않았다. Flow model을 먼저 이해해야함.
- Approximate density? 직접 구하나, exact값을 취하는 것이 아닌 '근사'하는 방법을 택하여 학습을 진행한다.

## 암시적 모델 (Implicit Models) - Sampler 생성 모델
Implicit density estimation 
- $p_{model} (x)$ 를 직접 구하지 않아도 샘플을 생성할 수 있다.
- 모델의 목적이 어떤 data의 분포를 학습하고자 하는 것이 아닌, 진짜 같은 샘플을 generate 하는
것을 목적으로 하는 함수이다.
- 모델에 대한 명시적인 정의가 없는 상태에서도, latent vector (i.e. noise)로부터 샘플을 만들 수
있는 모델을 생성한다.

반면, Auto-regressive, flow, latent variable 모델들은 ? 
- likelihood(exact) 기반이거나 근사(approximate) 한다.
  - 보통 좋은 latent variable을 위해 compression을 진행한다.
  - 사실, 생성 모델을 위해 정확한 compression은 안 필요할 수도 있다. -> 샘플링!

## 암시적 모델 (Implicit Models) - Sampler 생성 모델
Sampler 생성모델의 조건
- 데이터 포인트에 “내재하는 분포”(underlying distribution)을 생성 모델이 이해해야한다.
- Training 샘플들 간의 샘플들을 부드럽게 interpolation이 가능해야한다.
- 생성 모델의 생성된 샘플들이 training 샘플들과 유사하여야 하지만, 같아서는 안된다.
- Training data의 분포 안에서의 내재하는 분포들의 대표적(representative)인 샘플들을
생성하여야 한다.
  - 예) 특정A가웃는표정(A가웃는것은학습데이터에없음)
조건이 아니어도 괜찮은 것
- 학습 데이터와 정확히 같은 포인트에서 똑같이 생성하는 것 (복구하는 것)

## 암시적 모델 (Implicit Models) - Implicit Models의 특성
Implicit models의 일반적인 특성은?

<img width="115" alt="스크린샷 2023-08-18 오후 3 31 00" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a341a849-b22b-4132-a832-572f74b93319">

- 고정된 노이즈 소스 분포(i.e. uniform, gaussian dist.)로부터 z을 샘플링 한다.
- 신경망에 노이즈를 넣고, sample x를 넣는다.
- 이때, density estimation 을 직접 정의하지 않고 신경망을 학습시킨다.
- 이를 정리하면,
  - 데이터 분포: $p_{data} : x_1, x_2, ..., x_n$, 노이즈소스분포:𝑧~𝑝(𝑧) ,
  - DNN 생성 모델: $q_\phi$ (𝑧) = 𝐷𝑁𝑁(𝑧; 𝜙) 이주어졌을때,𝑥=  $q_\phi$ (𝑧) 으로 $p_{model}$  을추론한다.
  - 즉,  $p_{model}, p_{data}$  로의 직접적인 정의는 없이 sampling만 한다.
  - 𝜙을 학습시킴으로써 $p_{model}$을 $p_{data}$ 로 가깝게 되기를 기대한다.
    - 그런데 어떻게...?
    
## 암시적 모델 (Implicit Models) - Maximum likelihood을 쓸 수 있을까? -> NO!
우리는 지금까지 $p_{data} : x_1, x_2, ..., x_n$ 와 $p_{model}$ 가 얼마나 멀리있는지 계산해왔다.  
즉, $D_{KL} (p_{data} || p_{model}$ 을 사용하였는데 이것을 통해 우리는 $E_{x \sim p_{data}} log p_\theta (x)$을   
직접 정의하거나 계산하거나 근사하는 것이 일반적이었다.  
  
그러나, Implicit model은 $p_\theta (x)$을 직접 정의하지 않는다 !
-> 이는 곧, 최적화 과정이 maximum likelihood와 같지 않을 것!
- 대신 다른 방법들을 찾아볼 수 있을 것이다.

## 대표적인 암시적 모델 (Implicit Models)의 종류
Generative Moment Matching Network (GMMN; 2015)
 - Li, Yujia, Kevin Swersky, and Rich Zemel. "Generative moment matching networks." International Conference on Machine Learning. PMLR, 2015.

<img width="266" alt="스크린샷 2023-08-18 오후 3 56 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/00dec39e-48b5-4549-b30e-2280aaed1194">

Generative Adversarial Network (GAN; 2014)
- Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems 27 (2014).

<img width="381" alt="스크린샷 2023-08-18 오후 3 55 43" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9386c087-86dd-40ef-ad7e-acc3a4e75909">


