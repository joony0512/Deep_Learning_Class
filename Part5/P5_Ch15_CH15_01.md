# Deep Reinforcement Learning - 1. Reinforcement Learningì˜ í•µì‹¬ ê°œë… - Part 1
## ê°•í™” í•™ìŠµ(reinforcement learning; RL)
<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 46 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9c83781e-d5bb-4949-bb04-6a0546ef8cdb">

ì§€ë„ í•™ìŠµ ëŒ€ë¹„ ... challenge !    

Why? : Agentê¸°ë°˜ ëª¨ë¸ë§ìœ¼ë¡œ environmentì™€ ê³„ì† ìƒí˜¸ ì‘ìš©ì´ ë˜ì–´ì•¼ í•œë‹¤!  
- íƒí—˜ (exploration)
- ì‹ ë¢° í• ë‹¹ ë¬¸ì œ (ë§ˆì§€ë§‰ ìˆœê°„ì˜ ìƒíƒœëŠ” ì¤‘ê°„ ìƒíƒœì˜ ì˜í–¥ì„ ë°›ì•„ ì„±ë¦½ë˜ì—ˆëŠ”ë°, ë³´ìƒì„ ì´ì „ ìƒíƒœì—ë„ ë‚˜ëˆ ì£¼ì–´ì•¼ í•˜ëŠ”ê°€?)
- ì•ˆì •ì„±
- Online í•™ìŠµ

ëª©í‘œ : <img width="140" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 47 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a3c23a5b-d3d0-42fe-9976-ea54525a2ba6">

$ğ‘ _t : state, ğœ‹_\theta : ğ‘  â†’ ğ‘: policy, ğ‘…: ğ‘† â†’ ğ‘Ÿ: reward function$

## ê¹Šì€ ê°•í™” í•™ìŠµ(deep reinforcement learning; Deep RL)ì˜ ì„±ê³µ
<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 48 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e9f6aed2-fd06-4154-88bc-c00d20f88e72">

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 49 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e1d4e2da-f95d-4d68-9483-6d5f257c3ecd">

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 49 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c091351c-3203-4070-b9b1-a3493776ca8b">

<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 50 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5d3244e9-a037-4f3c-8a86-b8a715f7c0e4">

## ê¹Šì€ ê°•í™” í•™ìŠµ(deep reinforcement learning; Deep RL)ì˜ ì„±ê³µ â€’ ë³´í–‰ (locomotion)
<img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 50 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9e35cc29-0b2d-4a32-aa2d-2b18c184cd23">

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Key concepts & Terminology
ê°•í™”í•™ìŠµì€ agent ê¸°ë°˜ ëª¨ë¸ë§ ê¸°ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.
- Agentì™€ environmentê°€ ìˆê³  ìƒí˜¸ ì‘ìš©í•˜ëŠ” ê²ƒì´ íŠ¹ì§•

  <img width="400" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 46 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9c83781e-d5bb-4949-bb04-6a0546ef8cdb">


1. RL agentëŠ” ë§¤ interaction stepë§ˆë‹¤, agentëŠ” í™˜ê²½(environment; world)ì˜ ìƒíƒœ(state)ì˜ ê´€ì¸¡(observation)ìœ¼ë¡œ ìƒí™©ì„ íŒŒì•…í•œë‹¤.
2. Observationì„ íŒŒì•…í•œ ë’¤ ì •ì±…(policy) í˜¹ì€ ê°€ì¹˜ í•¨ìˆ˜(value function)ì— ë”°ë¼ í–‰ë™(action)ì„ ê²°ì •í•œë‹¤.
3. ì´ë•Œ environmentëŠ” agentê°€ í–‰ë™í•¨ì— ë”°ë¼(stepì´ ì§€ë‚¨ì— ë”°ë¼) stateë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. - agentì— ë…ë¦½ì ìœ¼ë¡œ ë³€í•  ìˆ˜ë„ ìˆìŒ!
4. AgentëŠ” ì´ë•Œ environmentë¡œ ë¶€í„° ë³´ìƒ(reward)ë¥¼ ë°›ëŠ”ë°, ì´ëŠ” agentì˜ í˜„ì¬ ìƒíƒœê°€ environmentê³¼ì˜ interactionì—ì„œ ì¢‹ê³  ë‚˜ì¨ì— ë”°ë¼ ê²°ì •ëœë‹¤.
5. ê¶ê·¹ì ì¸ agentì˜ ëª©í‘œëŠ” ëˆ„ì  ë³´ìƒ(cumulative return; return)ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤! - ì¢‹ì€ ì •ì±…, ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒê³¼ ì—°ê²°

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - State & Observations
<img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 5 57 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9ecd88e4-a212-466d-ba93-b45aa8fcef1b">

ìƒíƒœ(state; s): worldì˜ ìƒíƒœì— ëŒ€í•œ ì™„ì „í•œ ì •ë³´ -> stateë¡œë¶€í„° ìˆ¨ê²¨ì ¸ ìˆëŠ” worldì˜ ì •ë³´ëŠ” ì—†ë‹¤.
    
ê´€ì°°(observation; o): stateì— ëŒ€í•œ ë¶€ë¶„ì ì¸ ì„¤ëª…ìœ¼ë¡œ, ì •ë³´ê°€ ìƒëµë  ìˆ˜ ìˆë‹¤.

- Deep RLì—ì„œ stateì™€ observation spaceëŠ” ê±°ì˜ í•­ìƒ ì‹¤ì œ ê°’ ë²¡í„°, ë§¤íŠ¸ë¦­ìŠ¤ ë˜ëŠ” ê³ ì°¨ì› í…ì„œë¡œ ì •ì˜!
  - (DNNì˜ íš¨ìœ¨ ì¢‹ì€ representation ë•Œë¬¸!)
  - i.e. í”½ì…€ ê°’ì˜ RGB ë§¤íŠ¸ë¦­ìŠ¤, ë¡œë´‡ íŒ”ì˜ joint angleê³¼ ê·¸ ì†ë„
  
- Fully observed: agentê°€ í™˜ê²½ì˜ ì „ì²´ ìƒíƒœë¥¼ ê´€ì°°í•  ìˆ˜ ìˆì„ ë•Œ
- Partially observed: agentê°€ í™˜ê²½ì˜ ì¼ë¶€ ë§Œì„ ê´€ì°°í•  ìˆ˜ ìˆì„ ë•Œ

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Action spaces
<img width="300" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-08-29 á„‹á…©á„’á…® 6 00 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf20b607-2dca-4143-86ab-506a493f2928">

í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ actionì´ í—ˆìš©ë˜ê±°ë‚˜ ì œí•œëœë‹¤
  - ì‘ì—… ê³µê°„(action spaces): ì£¼ì–´ì§„ í™˜ê²½ì—ì„œ ìœ íš¨í•œ ëª¨ë“  ì‘ì—… ì§‘í•©
    
Discrete action space: agentê°€ ì œí•œëœ ìˆ˜ì˜ ë™ì‘ë§Œì„ ì·¨í•  ìˆ˜ ìˆë‹¤. 
  - i.e. Atari(ì¡°ì´ìŠ¤í‹± ì‹­ìí‚¤, A, B), Go(19x19 ë°”ë‘‘íŒ ìœ„ì˜ í•œ ì )
    
Continuous action space: agent ê°€ ì—°ì†ëœ ìŠ¤ì¹¼ë¼/ë²¡í„° ê°’ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ë™ì‘ì„ ì·¨í•  ìˆ˜ ìˆë‹¤.
  - i.e. ë¡œë´‡ íŒ” ì œì–´(ê° ê´€ì ˆì˜ ê°ë„ì™€ ìš´ë™ëŸ‰)

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Policy
ì •ì±…(Policy): agentê°€ ìˆ˜í–‰í•  actionì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê·œì¹™(rule)!  
ê²°ì •ë¡  ì (deterministic)ì¼ ìˆ˜ë„ ìˆê³ , í™•ë¥ ë¡  ì (stochastic)ì¼ ìˆ˜ë„ ìˆë‹¤.  
  - Deterministic policy $(ğœ‡): ğ‘_t = ğœ‡(ğ‘ _t)$
  - Stochastic policy $(ğœ‹): ğ‘_t \sim ğœ‹( \centerdot |ğ‘ _t)$
    
ì •ì±…ì€ ë³¸ì§ˆì ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ brainì´ë‹¤!
  - policyë¥¼ agentë¡œ í˜¼ìš©í•´ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ë‹¤. (i.e. "ì •ì±…ì€ ë³´ìƒì„ ê·¹ëŒ€í™”í•˜ë ¤ê³  ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.â€)

Deep RLì—ì„œëŠ” policyì„ DNN networkë¡œ ë§Œë“¤ê³  ë™ì‘ì„ ë³€ê²½í•˜ë„ë¡ ì¡°ì •í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°(i.e. weight & bias)ì— ì˜ì¡´í•˜ëŠ” ê³„ì‚° ê°€ëŠ¥í•œ í•¨ìˆ˜ì¸ ì •ì±…ì„ ë‹¤ë£¨ê¸°ë„ í•œë‹¤.
  - ì˜ˆ)ğœƒ,ğœ™ ì¨ì„œ í‘œí˜„ : $ğ‘_t = ğœ‡(ğ‘ _t) ,ğ‘_t \sim ğœ‹( \centerdot |ğ‘ _t)$

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Deterministic Policy
Deterministic policy (ğœ‡)
  - stateê°€ ê°™ìœ¼ë©´ í•­ìƒ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤!
  - $ğ‘_t = ğœ‡(ğ‘ _t)$
  - i.e. multi-layer perceptron, argmax ì— value functionì„ ì·¨í•˜ëŠ” ë“±

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Stochastic Policy
Stochastic policy (ğœ‹): stateê°€ ê°™ë”ë¼ë„ ì·¨í•˜ëŠ” actionì´ í™•ë¥ ì ìœ¼ë¡œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤! 
  - $ğ‘_t \sim ğœ‹( \centerdot |ğ‘ _t)$
    
ì•„ë˜ 2ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ì„œ stochastic policy êµ¬í˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. 
  - Policyë¡œë¶€í„° actionì„ samplingí•œë‹¤.
  - íŠ¹ì •í•œ actionì— ëŒ€í•œ policyì˜ log likelihood( $ğ‘™ğ‘œğ‘” ğœ‹_\theta (ğ‘|ğ‘ ) = ğ‘™ğ‘œğ‘” [P_\theta (s)]_a$ )ì„ ê³„ì‚°í•˜ì—¬ ìµœì í™” í•œë‹¤.

Categorical policy
  - Discrete ê°’ì— ëŒ€í•´ ì·¨í•œë‹¤. categorical, multinomial ë¶„í¬ì—ì„œ sampling.

Continuous policy
  - Continuous ê°’ì— ëŒ€í•´ ì·¨í•œë‹¤. Normal ë¶„í¬ ë“±ì„ í†µí•´ í†µí•´ samplingí•œë‹¤.
  - Gaussianì˜ê²½ìš°, $ğ‘§ \sim ğ‘ (0,ğ¼)$ , $ğ‘= ğœ‡_\theta (ğ‘ )$ + $ğœ_\theta (s) â¨€ ğ‘§$ ë¡œ reparametrizeí•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Trajectories & State transitions
- Trajectory(ê¶¤ë„) ğœ: stateì™€ actionì˜ sequenceì— ëŒ€í•œ ê¸°ë¡ (episode, rolloutì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤! ) 
  - ğœ = ($ğ‘ _0,ğ‘_0,ğ‘ _1,ğ‘_1,...$)
- ì´ˆê¸°ìƒíƒœ $ğ‘ _0$: start-state distribution ($ğœŒ_0$) ë¡œë¶€í„° ìƒ˜í”Œë§ í•˜ì—¬ ì‹œì‘.
  - $ğ‘ _0 \sim ğœŒ_0$ 
- State transition í•¨ìˆ˜(ğ‘“) í˜¹ì€ í™•ë¥ (ğ‘ƒ) ëŠ” environmentì˜ ì •í•´ì§„ ë²•ì¹™ì— ë”°ë¼ ê²°ì •ë˜ë©°, ê°€ì¥ ìµœê·¼ì˜ agentì˜ actionì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤.
- ê²°ì •ë¡  ì (deterministic)ì¼ ìˆ˜ë„ ìˆê³ , í™•ë¥ ë¡  ì (stochastic)ì¼ ìˆ˜ë„ ìˆë‹¤. 
  - Deterministic: $ğ‘ _{t+1} = ğ‘“(ğ‘ _t,ğ‘_t)$
  - Stochastic: $ğ‘ _{t+1} \sim ğ‘ƒ(\centerdot | ğ‘ _t,ğ‘_t )$
 
## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Reward & Return
ë³´ìƒ í•¨ìˆ˜(reward function; R): $r_t = R(s_t, a_t, s_{t+1}$
- ê°•í™” í•™ìŠµì—ì„œ ë§¤ìš° ì¤‘ìš”!
- í˜„ì¬ìƒíƒœ, ë°©ê¸ˆ ì·¨í•´ì§„ í–‰ë™, ê·¸ë¦¬ê³  ë‹¤ìŒìƒíƒœì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.

ì¢…ì¢… í˜„ì¬ ìƒíƒœì— ëŒ€í•´ í˜¹ì€ ìƒíƒœ-ì‘ìš© ìŒì— dependentí•˜ê²Œ ë‹¨ìˆœí™” ë  ìˆ˜ ìˆë‹¤.
- $ğ‘Ÿ_t =ğ‘…(ğ‘ _t) or ğ‘Ÿ_t = ğ‘…(ğ‘ _t,ğ‘_t)$
  
Recall: agentì˜ ëª©í‘œëŠ” trajectoryì— ê±¸ì³ cumulative reward (return)ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ.
- Returnì„ R(ğœ) ë¡œ í‘œê¸°í•œë‹¤. (ëª¨ë“  RLì— ë™ì¼í•œ ì‹ì´ ì ìš© ê°€ëŠ¥!)

## ê°•í™” í•™ìŠµ(reinforcement learning; RL) - Returnì˜ ì¢…ë¥˜
Finite-horizon undiscounted return: $ğ‘ (ğ›•) = \Sigma^T_{t=0} r_t$ 
  - ì •í•´ì§„ ë‹¨ê³„ ë§Œí¼ì˜ ë³´ìƒì„ ë”í•¨.
Infinite-horizon discounted return:  $ğ‘ (ğ›•) = \Sigma^\infty_{t=0} \gamma^t r_t$   
  - ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ í• ì¸ìœ¨ì„ ì ìš©í•˜ë©° ëª¨ë“  ë‹¨ê³„ì˜ ë³´ìƒì„ ë”í•¨.
  - í• ì¸ ì¸ì(discount factor): ğ›¾ âˆˆ (0, 1)ë¥¼ í¬í•¨!
    - ìˆ˜í•™ì  ìˆ˜ë ´ì„ ìœ„í•¨ (think: ë¬´í•œ ë“±ë¹„ ìˆ˜ì—´ì˜ í•©) + ì‹ ë¢° í• ë‹¹ ë¬¸ì œ

ì°¸ê³ : ìœ„ ë‘ ë°©ë²•ì€ ì „í†µì ì¸ RLì—ì„œ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì‚¬ìš©í•˜ì§€ë§Œ, Deep RLì—ì„œëŠ” í˜¼ìš©í•˜ê¸°ë„ í•œë‹¤!
  - i.e. ì–´ë–¤ RL ì•Œê³ ë¦¬ì¦˜ì€ ì¢…ì¢… í• ì¸ë˜ì§€ ì•Šì€ ìˆ˜ìµë¥ ì„ ìµœì í™”í•˜ë ¤ í•˜ì§€ë§Œ, ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì •ì—ëŠ” í• ì¸ ìš”ì¸ì„ ì‚¬ìš©í•œë‹¤.
