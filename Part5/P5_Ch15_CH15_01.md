# Deep Reinforcement Learning - 1. Reinforcement Learning의 핵심 개념 - Part 1
## 강화 학습(reinforcement learning; RL)
<img width="400" alt="스크린샷 2023-08-29 오후 5 46 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9c83781e-d5bb-4949-bb04-6a0546ef8cdb">

지도 학습 대비 ... challenge !    

Why? : Agent기반 모델링으로 environment와 계속 상호 작용이 되어야 한다!  
- 탐험 (exploration)
- 신뢰 할당 문제 (마지막 순간의 상태는 중간 상태의 영향을 받아 성립되었는데, 보상을 이전 상태에도 나눠주어야 하는가?)
- 안정성
- Online 학습

목표 : <img width="140" alt="스크린샷 2023-08-29 오후 5 47 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a3c23a5b-d3d0-42fe-9976-ea54525a2ba6">

$𝑠_t : state, 𝜋_\theta : 𝑠 → 𝑎: policy, 𝑅: 𝑆 → 𝑟: reward function$

## 깊은 강화 학습(deep reinforcement learning; Deep RL)의 성공
<img width="400" alt="스크린샷 2023-08-29 오후 5 48 46" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e9f6aed2-fd06-4154-88bc-c00d20f88e72">

<img width="400" alt="스크린샷 2023-08-29 오후 5 49 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/e1d4e2da-f95d-4d68-9483-6d5f257c3ecd">

<img width="400" alt="스크린샷 2023-08-29 오후 5 49 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c091351c-3203-4070-b9b1-a3493776ca8b">

<img width="400" alt="스크린샷 2023-08-29 오후 5 50 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5d3244e9-a037-4f3c-8a86-b8a715f7c0e4">

## 깊은 강화 학습(deep reinforcement learning; Deep RL)의 성공 ‒ 보행 (locomotion)
<img width="400" alt="스크린샷 2023-08-29 오후 5 50 36" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9e35cc29-0b2d-4a32-aa2d-2b18c184cd23">

## 강화 학습(reinforcement learning; RL) - Key concepts & Terminology
강화학습은 agent 기반 모델링 기법 중 하나이다.
- Agent와 environment가 있고 상호 작용하는 것이 특징

  <img width="400" alt="스크린샷 2023-08-29 오후 5 46 03" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9c83781e-d5bb-4949-bb04-6a0546ef8cdb">


1. RL agent는 매 interaction step마다, agent는 환경(environment; world)의 상태(state)의 관측(observation)으로 상황을 파악한다.
2. Observation을 파악한 뒤 정책(policy) 혹은 가치 함수(value function)에 따라 행동(action)을 결정한다.
3. 이때 environment는 agent가 행동함에 따라(step이 지남에 따라) state를 업데이트 한다. - agent에 독립적으로 변할 수도 있음!
4. Agent는 이때 environment로 부터 보상(reward)를 받는데, 이는 agent의 현재 상태가 environment과의 interaction에서 좋고 나쁨에 따라 결정된다.
5. 궁극적인 agent의 목표는 누적 보상(cumulative return; return)을 최대화하는 것이다! - 좋은 정책, 가치 함수를 찾아내는 것과 연결

## 강화 학습(reinforcement learning; RL) - State & Observations
<img width="300" alt="스크린샷 2023-08-29 오후 5 57 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9ecd88e4-a212-466d-ba93-b45aa8fcef1b">

상태(state; s): world의 상태에 대한 완전한 정보 -> state로부터 숨겨져 있는 world의 정보는 없다.
    
관찰(observation; o): state에 대한 부분적인 설명으로, 정보가 생략될 수 있다.

- Deep RL에서 state와 observation space는 거의 항상 실제 값 벡터, 매트릭스 또는 고차원 텐서로 정의!
  - (DNN의 효율 좋은 representation 때문!)
  - i.e. 픽셀 값의 RGB 매트릭스, 로봇 팔의 joint angle과 그 속도
  
- Fully observed: agent가 환경의 전체 상태를 관찰할 수 있을 때
- Partially observed: agent가 환경의 일부 만을 관찰할 수 있을 때

## 강화 학습(reinforcement learning; RL) - Action spaces
<img width="300" alt="스크린샷 2023-08-29 오후 6 00 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bf20b607-2dca-4143-86ab-506a493f2928">

환경에 따라 다른 종류의 action이 허용되거나 제한된다
  - 작업 공간(action spaces): 주어진 환경에서 유효한 모든 작업 집합
    
Discrete action space: agent가 제한된 수의 동작만을 취할 수 있다. 
  - i.e. Atari(조이스틱 십자키, A, B), Go(19x19 바둑판 위의 한 점)
    
Continuous action space: agent 가 연속된 스칼라/벡터 값으로 표현되는 동작을 취할 수 있다.
  - i.e. 로봇 팔 제어(각 관절의 각도와 운동량)

## 강화 학습(reinforcement learning; RL) - Policy
정책(Policy): agent가 수행할 action을 결정하기 위해 사용하는 규칙(rule)!  
결정론 적(deterministic)일 수도 있고, 확률론 적(stochastic)일 수도 있다.  
  - Deterministic policy $(𝜇): 𝑎_t = 𝜇(𝑠_t)$
  - Stochastic policy $(𝜋): 𝑎_t \sim 𝜋( \centerdot |𝑠_t)$
    
정책은 본질적으로 에이전트의 brain이다!
  - policy를 agent로 혼용해 사용하는 경우도 많다. (i.e. "정책은 보상을 극대화하려고 노력하고 있습니다.”)

Deep RL에서는 policy을 DNN network로 만들고 동작을 변경하도록 조정할 수 있는 파라미터(i.e. weight & bias)에 의존하는 계산 가능한 함수인 정책을 다루기도 한다.
  - 예)𝜃,𝜙 써서 표현 : $𝑎_t = 𝜇(𝑠_t) ,𝑎_t \sim 𝜋( \centerdot |𝑠_t)$

## 강화 학습(reinforcement learning; RL) - Deterministic Policy
Deterministic policy (𝜇)
  - state가 같으면 항상 같은 결과가 나온다!
  - $𝑎_t = 𝜇(𝑠_t)$
  - i.e. multi-layer perceptron, argmax 에 value function을 취하는 등

## 강화 학습(reinforcement learning; RL) - Stochastic Policy
Stochastic policy (𝜋): state가 같더라도 취하는 action이 확률적으로 달라질 수 있다! 
  - $𝑎_t \sim 𝜋( \centerdot |𝑠_t)$
    
아래 2개의 프로세스를 통해서 stochastic policy 구현하는 것이 일반적이다. 
  - Policy로부터 action을 sampling한다.
  - 특정한 action에 대한 policy의 log likelihood( $𝑙𝑜𝑔 𝜋_\theta (𝑎|𝑠) = 𝑙𝑜𝑔 [P_\theta (s)]_a$ )을 계산하여 최적화 한다.

Categorical policy
  - Discrete 값에 대해 취한다. categorical, multinomial 분포에서 sampling.

Continuous policy
  - Continuous 값에 대해 취한다. Normal 분포 등을 통해 통해 sampling한다.
  - Gaussian의경우, $𝑧 \sim 𝑁 (0,𝐼)$ , $𝑎= 𝜇_\theta (𝑠)$ + $𝜎_\theta (s) ⨀ 𝑧$ 로 reparametrize하는 방법이 있다.

## 강화 학습(reinforcement learning; RL) - Trajectories & State transitions
- Trajectory(궤도) 𝜏: state와 action의 sequence에 대한 기록 (episode, rollout이라고도 불린다! ) 
  - 𝜏 = ($𝑠_0,𝑎_0,𝑠_1,𝑎_1,...$)
- 초기상태 $𝑠_0$: start-state distribution ($𝜌_0$) 로부터 샘플링 하여 시작.
  - $𝑠_0 \sim 𝜌_0$ 
- State transition 함수(𝑓) 혹은 확률(𝑃) 는 environment의 정해진 법칙에 따라 결정되며, 가장 최근의 agent의 action에 영향을 받는다.
- 결정론 적(deterministic)일 수도 있고, 확률론 적(stochastic)일 수도 있다. 
  - Deterministic: $𝑠_{t+1} = 𝑓(𝑠_t,𝑎_t)$
  - Stochastic: $𝑠_{t+1} \sim 𝑃(\centerdot | 𝑠_t,𝑎_t )$
 
## 강화 학습(reinforcement learning; RL) - Reward & Return
보상 함수(reward function; R): $r_t = R(s_t, a_t, s_{t+1}$
- 강화 학습에서 매우 중요!
- 현재상태, 방금 취해진 행동, 그리고 다음상태에 따라 달라진다.

종종 현재 상태에 대해 혹은 상태-작용 쌍에 dependent하게 단순화 될 수 있다.
- $𝑟_t =𝑅(𝑠_t) or 𝑟_t = 𝑅(𝑠_t,𝑎_t)$
  
Recall: agent의 목표는 trajectory에 걸쳐 cumulative reward (return)을 최대화하는 것.
- Return을 R(𝜏) 로 표기한다. (모든 RL에 동일한 식이 적용 가능!)

## 강화 학습(reinforcement learning; RL) - Return의 종류
Finite-horizon undiscounted return: $𝐑 (𝛕) = \Sigma^T_{t=0} r_t$ 
  - 정해진 단계 만큼의 보상을 더함.
Infinite-horizon discounted return:  $𝐑 (𝛕) = \Sigma^\infty_{t=0} \gamma^t r_t$   
  - 시간이 지날수록 할인율을 적용하며 모든 단계의 보상을 더함.
  - 할인 인자(discount factor): 𝛾 ∈ (0, 1)를 포함!
    - 수학적 수렴을 위함 (think: 무한 등비 수열의 합) + 신뢰 할당 문제

참고: 위 두 방법은 전통적인 RL에서 명확히 구분하여 사용하지만, Deep RL에서는 혼용하기도 한다!
  - i.e. 어떤 RL 알고리즘은 종종 할인되지 않은 수익률을 최적화하려 하지만, 가치 함수 추정에는 할인 요인을 사용한다.
