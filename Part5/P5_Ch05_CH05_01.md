# 합성곱 신경망(CNN) -1 Convolutional Neural Networks
## 이미지 인식(visual recognition)
- 이산적(discrete)으로 구성된 레이블을 각각의 학습 데이터(이미지)로부터 학습시키고 분류한다.
- x : 데이터, y : 레이블
- Train 
$D = {(x_1, y_1),...(x_n, y_n)}$ -> f(x) : 학습로직 -> h
- Test 
x* -> h -> $\hat y$

## Recap: 딥러닝 VS 전통적 방식 (계층적 표현 학습; hierarchical representation learning)
  <img width="176" alt="스크린샷 2023-06-30 오후 5 01 51" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/23788ec4-5db9-4d52-bc2f-aec16bc02b7e">

- 전통적머신러닝 : Hand-craft 피처 엔지니어링  
  <img width="291" alt="스크린샷 2023-06-30 오후 5 02 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b9e1e855-3724-47e3-8271-e9daf1fb5232">

- 딥러닝 : 피처엔지니어링을 DNN이 어느정도 알아서 진행 (표현학습과 분류를 동시에)
  - '계층적(hierarchical)'으로 학습한다.
    
  <img width="249" alt="스크린샷 2023-06-30 오후 5 03 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/492adb4f-1d55-4132-a1ee-80f5658945dd">

## 계층적 표현 학습 (hierarchical representation learning)
- 사람의 뇌의 시각령(visual cortex) 부분도  계층적으로 이미지 자극을 해석한다.

## 컴퓨팅 시뮬레이션
- 사람과 달리, 일반적인 신경망은 여러 큰 이미지에 범용적으로 잘 통용되지 않는다. (컴퓨터에 정확히 시뮬레이션 되어야 하기 때문)
- 신경망의 여러 파라미터들은 입력 값과 함께 붙어 있는다. 예를들어, 32x32x1 이미지에 대해서는, 첫번째 심층 신경망의 weight 수는 32x32x1 = 1024개를 갖는다.
- 이런 제한과 컴퓨터 리소스 상황에 따라 신경망 구조를 잘 정해야 한다.
  <img width="416" alt="스크린샷 2023-06-30 오후 5 06 08" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a92e3d15-509a-4efa-b0fa-efb38dbd9174">

## 최초의 합성곱 신경망(CNN)
<img width="233" alt="스크린샷 2023-06-30 오후 5 07 05" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4273498d-ce19-4b15-a1e2-a85b41cf3bda">

- Fukushima, Kunihiko, and Sei Miyake. "Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition." Competition and cooperation in neural nets. Springer, Berlin, Heidelberg, 1982. 267-285. Image is from Fukushima, Kunihiko. "Recent advances in the deep CNN neocognitron." Nonlinear Theory and Its Applications, IEICE 10.4 (2019): 304-321.

- Neocognitron (1982), Kunihiko, Fukushima et al.
  - 합성곱 레이어(convolution layer)와 다운 샘플링 레이어(down sampling)인 풀링(pooling) 개념이 제시됨 ! 

- LeNet (1989), Yann LeCun et al.
  - 우리가 익숙한 현대적인 의미의 CNN.
  - Neocognitron의 개념을 보다 발전시키고, “역전파(back-propagation)”을 적용시킴!
  - Convolutional kernel 을 manual하게 발전시켰음
## AlexNet (CNN)
<img width="344" alt="스크린샷 2023-06-30 오후 5 09 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fcfe2a39-6f7c-4298-b079-a95cc15ee6a7">

- LeNet과 비슷하지만, 기존 CNN을 깊게 쌓고, 더 나은 활성함수와 정형화(regularization) 기법을 적용. (ReLU, local response normalization, data augmentation, dropout 등)
  - 딥러닝 기반의 최초의 ImageNet challenge (ILSVRC-2012 competition) 우승한 모델!
