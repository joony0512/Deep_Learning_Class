# <Generative model series #2> Flow Models - 2. Flow와 목적 함수(objective function)
## Flow 기반 생성 모델
역 변환 가능한(**invertible**)한 transformation의 연속으로 구성되어 데이터의 분포를 
“명시적으로(explicitly하게)”(i.e. minimize negative log-likelihood) 학습하는 모델. 

<img width="515" alt="스크린샷 2023-08-09 오후 3 24 35" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/11cd32b7-f937-4c87-bde3-a6848b83603c">

## Flow을 정의하기 전에 - Recap: 밀도 모델 / 가우시안 혼합 모델(Gaussian Mixture Model)

**Density model(밀도모델)은?**
- 매개변수화된(parametrized) probability densitygkatndlek.
- 예) 가우시안 혼합 -> gaussian pdf

  <img width="384" alt="스크린샷 2023-08-09 오후 3 35 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0d0e8d3e-715d-4a6a-8b72-0dd22956fea4">

  - parameter : 각 컴포넌트, mixture weights의 평균과 분산으로 표현가능하다.<img width="200" alt="스크린샷 2023-08-09 오후 3 33 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/fec05b4e-2d02-4fa3-8fcf-2894a8494ca8">
  - Maximum likelihood로 최적화(fit)할수있다.<img width="200" alt="스크린샷 2023-08-09 오후 3 34 29" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/339c9cec-d827-4ea0-a95a-e7955e79aee6">

## Flow을 정의하기 전에 - 가우시안 혼합 모델(Gaussian Mixture Model)이 높은 차원(high-dimensional) 데이터에서 잘 작동할까?
GMM 에서 새로운 데이터를 샘플링하는 과정은 아래와 같다.
- Cluster Center 중 하나를 선택한다.
- Cluster Center(Mean, Variance)에 Gaussian noise을 더한다. (=클러스터 내에서 정규분포처럼 샘플링)
- 그러나.. 높은 차원에서는 잘 작동하지 않는다.

일반적인 이미지에서 기존 데이터가대부분 Cluster Center 에 위치한다고 가정
- 새롭게 샘플링 하는 데이터가 Center 에 가깝지 않다면 결코 Realistic 할 수 없다.
- 차원이 높아지면 샘플링 데이터가 cluster center 에 가까운 경우가 더욱 줄어들 것이다.

<img width="260" alt="스크린샷 2023-08-09 오후 3 39 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c8bfd8c9-a3dc-4e4c-a796-bffd55781f8b">

## Flow을 정의하기 전에 - PDF(확률 밀도 함수)가 아닌, CDF(누적 분포 함수)의 역(inverse)에서 sampling을 해보면 어떨까?
- p(x)을 pdf로 가지는 cdf f(x) = z에서, - $\infty \le x \le \infty, 0 \le f(x) \le 1$  
<img width="100" alt="스크린샷 2023-08-09 오후 4 00 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a8d0f2a8-1e4f-4424-bb57-975e7626342c">을 만족하였다.  
- 이때 역함수를 표현하면, <img width="100" alt="스크린샷 2023-08-09 오후 4 01 38" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/381d7e56-f573-421d-841f-e591752d739f">이고, z는 0-1사이의 값을 가지므로 아래와 같이 균등분포(uniform distribution)로 샘플링할 수 있다.  
<img width="100" alt="스크린샷 2023-08-09 오후 4 02 31" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/af5abd44-0709-421b-a476-9855ff242c27">  
- 이때, pdf,cdf관계에 따라 cdf는 미분가능하다.

## 1D CDF(누적 분포 함수)
- x-> z로의 흐름 (flow)
  - x 에서 z까지 가역(invertible)하고 미분가능한 (diffentiable)함수
- pdf($p_\theta (x)$을 학습하는 것은, uniform distribution[0,1]을 data distribution로 mapping하도록 cdf($f_\theta (x)$)을 학습하는 것과 같다.

  <img width="400" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/864e1033-9bc4-4b15-836b-ed166b13c28b">

- 마찬가지로 inverse cdf (역누적분포함수)을 통한 매핑 uniform([0,1])이 데이터 분포를 산출하도록 cdf를 학습한다.

## 1D Flow
**Flow란?**  
- x(data) -> z(noise)로의 미분가능(diffentiable)하고, 가역적(invertible)인 mapping을 뜻한다.
- 데이터 분포가 기본분포(base distribution;p(z))로 변하도록 학습한다.
  - 기본분포의 선택 : uniform, standard normal(gaussian)
- 이방법으로, $z \sim \pi (z)$을 flow의 역(inverse)로 매칭한다면, clustering기반의 샘플링보다 좋은 sample이 생성될 것이다.

  <img width="641" alt="스크린샷 2023-08-09 오후 4 25 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/40bc32ab-e2d5-40c1-927a-980242670419">

## Flow의 목적 함수 / fitting
- Flow를 maximum likelihood로 fitting 시킬것이다

  <img width="158" alt="스크린샷 2023-08-09 오후 4 29 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bffc0ac9-9616-489f-bebe-90ddbff04cba">
- $p_\theta (x)$는 다음과 같이 정의한다.
  - flow $f_\theta (x)$ =z 는 sampling을 통해 얻을 수 있는 x에 대한 distribution일때, 아래 조건을 만족한다.

    <img width="158" alt="스크린샷 2023-08-09 오후 4 31 28" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/885010d9-adfd-442d-8b51-9f8f1eacbb39">
  - $p_\theta (x)$는 x 의 pdf이며, 아래와 같이 계산할 수 있다.
  
    <img width="456" alt="스크린샷 2023-08-09 오후 4 32 18" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/928c0852-37a5-4c08-98f7-9eaeb58ddb65">

## Recap: 변수 변환 (change of variable) – Determinants와 Volumes (Geometry view)
<img width="406" alt="스크린샷 2023-08-09 오후 4 35 02" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bb2584db-0bd3-4bc6-a122-57e913895b87">  
$Z \in [0,1]^n$을 uniform random vector라 가정.  

이때, $X = AZ, Z = WX 즉, A = W^{-1}$ 이도록 affine-transform(선형변환)을 한다면, 기하학적으로 W행렬은 X을 평행체(parallelotope) Z로 보낸다.  

이때 평행체의 용량/면적(volume)은 행렬의 determinant와 동일하다.  
X는 평행체에 uniformly distributed되어 있으므로, <img width="178" alt="스크린샷 2023-08-09 오후 4 38 53" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6446fcaf-77f9-412b-b66b-797232b9a9de">식을 만족한다.

## Flow Models & 학습
관찰할 수 있는 변수 X(데이터)와 잠재변수 (latent variable) Z가 주어졌다고 해보자.  
그리고 x,z가 연속적이고 같은 차원을 가지고 있다면, Normalizing flow model에서 Z와 X사이의 mapping을 $f_\theta : R^n -> R^n$ 은 확정적(deterministic)하고 가역(invertible)하다.  
즉, $X = f_theta ^{-1} (Z), Z = f_\theta (X)$ 을 만족한다.  
<img width="178" alt="스크린샷 2023-08-09 오후 4 48 54" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/53084148-9498-47e6-be66-b05742293899">  
변수변환(change of variables)을 통해, x에 대한 density, 즉 주변 가능도 (marginal likelihood) $p_x (x;\theta)$ 은 <img width="234" alt="스크린샷 2023-08-09 오후 4 50 24" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/a368f9d5-b1f2-478f-ac01-85608b658f5c">이다.  
이 Flow를 maximum likelihood로 fitting시킬것이다.  <img width="429" alt="스크린샷 2023-08-09 오후 4 59 47" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/29d67ffa-5e08-476a-93af-58a3e8de56ce">  
이떄 Jacobian determinant가 쉽게 계산되고 미분되어야 한다.

## Composition of flows, flows of transformations
Flows는 여러개의 sequential한 연산으로 구성될 수 있다.  
이를 composition of flows이라고도 하고, flows of transformations이라고도한다. 즉  
<img width="200" alt="스크린샷 2023-08-09 오후 5 02 14" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/50fb06cb-883d-4827-9ff0-c0eda58e8496">  
로 여러개의 합성함수로 구성가능하다.   
이를 <img width="200" alt="스크린샷 2023-08-09 오후 5 02 56" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/91355cf9-a54b-42ad-a562-c0973e2696e6">에 적용시키고 log likelihood계산으로 바꿔주면, 

<img width="600" alt="스크린샷 2023-08-09 오후 5 03 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/18f2e73a-539d-432e-946f-f880f7598148">  
로 표현 될 수 있다. 결과적으로, 모델의 표현력(expressiveness)를 키울 수 있다.  
이때, 연속적인 분포들 $\pi_i$로 부터 형성되는 full chain 형태를 정규화 플로우(normalizing flow)라고 한다. 

## Flow 변환의 조건 요약
Flow model은 아래 목적함수를 최적화하고   
<img width="100" alt="스크린샷 2023-08-09 오후 5 09 26" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b8d9b8b8-219b-481d-87f9-ed0070d5cbae">  
이를 log-likelihood로 풀어 아래식을 최적화 하는 것을 목표로 하는 모델이다.   
<img width="200" alt="스크린샷 2023-08-09 오후 5 10 13" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ae200e8f-e904-4b1d-9c3a-c6b6bc212f4d">

이때 flow변환은 성능 보장을 위해 아래 조건들을 만족하여야 한다. 
조건1. 변환행렬이 가역적(invertible)이어야한다.  
조건2. 변환행렬의 Jacobian 결정계수(determinant)
<img width="76" alt="스크린샷 2023-08-09 오후 5 12 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/81156981-31a2-4514-8397-91a8ff3ca238">가 쉽게 계산되어야 한다. 
- 즉 log determinant가 쉽게 계산되어야 한다. 
