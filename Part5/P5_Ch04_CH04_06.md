## CH04_06_Learningratescheduler

## Learning rate Decay

- ML을 학습할때, learning rate를 줄이는 (decay)등의 하이퍼파라미터 튜닝은 ML모델의 성능 &학습속도를 줄일 수 있다.
1. 앞에 이야기한 adaptive learning rate 을 사용하는 optimizer (AdaGrad, RMSProp, Adam, AdamP)를 사용하는 방법
2. Learning rate schedule을 통해 직접 learning rate을 조절하는 방법이 있다.
    
    
    <img width="392" alt="스크린샷 2023-06-27 오전 1 26 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0e77257f-2017-457e-a710-b86fff4c40e0">


## Learning rate schedules

- 에포크기준 decay : 시간기준 (time-based)
$lr \leftarrow lr *$ $1 \over (1+decay*epoch)$
    
    <img width="135" alt="스크린샷 2023-06-27 오전 1 33 30" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/bd3be52b-5a38-407b-b54f-57039c191e58">

    
- drop 기준 (step 기준)
$lr \leftarrow lr *droprate^{floor({epoch \over epoch_{drop}})}$
    
    
    <img width="135" alt="스크린샷 2023-06-27 오전 1 33 41" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/67421479-307f-4d6f-80e9-3ec7dd6f8a1b">


## Gradual warm-up LR-schedules

- ResNet, Transformer 등 딥러닝의 진보를 이끌었던 연구에서 초기 Learning Rate을 안정화 하기 위해, Adam등 optimizer에 learning rate를 서서히 올리는 기법 (warm-up) 을 ‘실험적으로 성능이 좋아’ 사용
    <img width="208" alt="스크린샷 2023-06-27 오전 1 35 42" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/5bb8842f-5e86-4056-9b9a-60c0fc4e4e12">

    
    

## Gradual warm-up이 왜 잘되었을까?

- Adaptive learning rate optimizer (Adam, RMSProp 등)은 모두 안정적이고 빠른 성능을 목표로 하지만, 실제로는 local optima에 많이 빠진다.
- Liu, Liyuan (2019)는 해당 문제를 training 앞부분 매우 큰 gradient의 분산(variance) 때문이라고 분석 !
- 즉, minibatch training에서 초기 제한된, optimization이 이뤄져, local optima에 머물게됨
- 그렇기에 초기 상태에서, warm-up이 필요했었던 것!
    
    
    
<img width="222" alt="스크린샷 2023-06-27 오전 1 38 50" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/77270065-28cb-42c9-9c31-1b5f036b0e71">

## RAdam(Rectified Adam)

- Variance rectification term을 추가하자
- 만약 Variance 계산이 불가하면, un-adapted momentum을 적용하자.
    
    
    <img width="364" alt="스크린샷 2023-06-27 오전 1 40 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/3f9d2984-6ede-49ee-b75e-f4d389a02494">


## RAdam(Rectified Adam)의 성능

- 전통적인 Adam, SGD에 비해 LR과 관계없이 안정적으로 수렴함을 보인다 !
    - 잘 모르겠으면 RAdam을 먼저 써보자!
- 단, learning rate튜닝이 매우 잘 된 SGD에 비해 성능이 안나올 때도 있다! (항상 정답은 아님)
    
    
<img width="347" alt="스크린샷 2023-06-27 오전 1 42 15" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/899dd0dc-0d2b-46a9-82c6-1fdfbd3d04d1">

    
    - RAdam은 variance의 발산 상태에 따라 adaptive learning rate를 켜거나 끈다.
        - Dynamic warmup은 파라미터로 turn on/off가 필요가 없다.
        - 저자의 실험에서는 RAdam의 성능을 Adam으로 같은 성능을 내려면 learning rate 조절의 튜닝을 반복해야한다.
  <img width="362" alt="스크린샷 2023-06-27 오전 1 43 59" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0adbb13e-4c8b-42b8-8bbe-be0b0144b2c6">
