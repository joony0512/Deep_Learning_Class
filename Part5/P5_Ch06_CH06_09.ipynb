{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2799f03f44dd4f38a0dcb721f0b96e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a80daed5fa9d4ed18bb9be8bacf0e659",
              "IPY_MODEL_2d85aff9d2e54df5ac1a56c82f1ca66a",
              "IPY_MODEL_a4a36f0e37b5475c852b43aa72502515"
            ],
            "layout": "IPY_MODEL_c1b62997f2084108b9c2cd57a4b4a05c"
          }
        },
        "a80daed5fa9d4ed18bb9be8bacf0e659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36e89b93f90241d7a4efb8ca51300b86",
            "placeholder": "​",
            "style": "IPY_MODEL_f60411d1ffab44cb96c5aff4d12e919d",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "2d85aff9d2e54df5ac1a56c82f1ca66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d5bbf93333a48b38d85b61f4ad30cf6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a4b8752a3ed494ab359aa53a16acdfe",
            "value": 2
          }
        },
        "a4a36f0e37b5475c852b43aa72502515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401e0315c5c3436f84a6f06a7087e1a1",
            "placeholder": "​",
            "style": "IPY_MODEL_f86306777ecb45558bd45f01e8e6ec4b",
            "value": " 2/2 [00:00&lt;00:00,  8.60it/s]"
          }
        },
        "c1b62997f2084108b9c2cd57a4b4a05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "36e89b93f90241d7a4efb8ca51300b86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f60411d1ffab44cb96c5aff4d12e919d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d5bbf93333a48b38d85b61f4ad30cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4b8752a3ed494ab359aa53a16acdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "401e0315c5c3436f84a6f06a7087e1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f86306777ecb45558bd45f01e8e6ec4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fd47fe92dcc4f5e9fc9f48f5e1e46e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f099a4891b74a1ab8b39f90da08c18f",
              "IPY_MODEL_db185360f7644f17a717d4036268b828",
              "IPY_MODEL_2f7c0a4a5f7c4ebaa258b3fc7bbbd61e"
            ],
            "layout": "IPY_MODEL_07e561d8afe14ab4b409c7a4fd435c5a"
          }
        },
        "9f099a4891b74a1ab8b39f90da08c18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72917c7f2cda4e098e93a8324415c75c",
            "placeholder": "​",
            "style": "IPY_MODEL_6736af2e43804ee5a06d4e6be6493f0d",
            "value": "Epoch 2:  44%"
          }
        },
        "db185360f7644f17a717d4036268b828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d37e1cbb114ea7962881bf6602701f",
            "max": 227,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffd36a1654244adb90a44f258b9924e5",
            "value": 100
          }
        },
        "2f7c0a4a5f7c4ebaa258b3fc7bbbd61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6087d06126c64405bbf6ef8e518e5017",
            "placeholder": "​",
            "style": "IPY_MODEL_2cc7849ea7ae49929b3d91ab21fffab4",
            "value": " 100/227 [00:38&lt;00:48,  2.59it/s, v_num=g1_0]"
          }
        },
        "07e561d8afe14ab4b409c7a4fd435c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "72917c7f2cda4e098e93a8324415c75c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6736af2e43804ee5a06d4e6be6493f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96d37e1cbb114ea7962881bf6602701f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd36a1654244adb90a44f258b9924e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6087d06126c64405bbf6ef8e518e5017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc7849ea7ae49929b3d91ab21fffab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d905b29a1c1b4359a7024ac267887a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1e2f9dffeb0448ba69098cbebd81c3b",
              "IPY_MODEL_695a58b53b064cb09422286309e6e8dc",
              "IPY_MODEL_a8e230b424c24f7786fa3e5cbaedd56b"
            ],
            "layout": "IPY_MODEL_955bdbae46fa4806bd16cf7c598b5637"
          }
        },
        "d1e2f9dffeb0448ba69098cbebd81c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1875d792c0574e79a17f798c986aa223",
            "placeholder": "​",
            "style": "IPY_MODEL_61ec06e1ff954e8ea051b4706db301d9",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "695a58b53b064cb09422286309e6e8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb2a48efbcc84e588b1c486ef667a1b8",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d82e59a88ac940fb8afad8baa07befef",
            "value": 32
          }
        },
        "a8e230b424c24f7786fa3e5cbaedd56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37bdf34c8d2347e197bf02e7b53fb470",
            "placeholder": "​",
            "style": "IPY_MODEL_9c7e157b9f6d4612908c2f9aa6a3afd1",
            "value": " 32/32 [00:01&lt;00:00, 17.06it/s]"
          }
        },
        "955bdbae46fa4806bd16cf7c598b5637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "1875d792c0574e79a17f798c986aa223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ec06e1ff954e8ea051b4706db301d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb2a48efbcc84e588b1c486ef667a1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d82e59a88ac940fb8afad8baa07befef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37bdf34c8d2347e197bf02e7b53fb470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7e157b9f6d4612908c2f9aa6a3afd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6415d451fd7a44efa22513a5003b06a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc047986b7814a97ab69431bc1f844c1",
              "IPY_MODEL_2cee393505154b7f9f7fe2c6fa03501b",
              "IPY_MODEL_3db2911db72f4ddaae6aa80200184527"
            ],
            "layout": "IPY_MODEL_affba57a3f1940cd935834aec3f0100d"
          }
        },
        "fc047986b7814a97ab69431bc1f844c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3116240132a64a4e899560930618bb75",
            "placeholder": "​",
            "style": "IPY_MODEL_cf8c53d96bd94ae29c8156e06f39a05e",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "2cee393505154b7f9f7fe2c6fa03501b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e65ecb4645734a5b8b9bfe9cd87c03df",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23e1f7819e254bdfa5ad03dc22fd4ea3",
            "value": 32
          }
        },
        "3db2911db72f4ddaae6aa80200184527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9038b0185256420a8ba939c84b5bf1fb",
            "placeholder": "​",
            "style": "IPY_MODEL_4442255c2e3f4af0b8b6afcb0f52f43c",
            "value": " 32/32 [00:01&lt;00:00, 21.72it/s]"
          }
        },
        "affba57a3f1940cd935834aec3f0100d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "3116240132a64a4e899560930618bb75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf8c53d96bd94ae29c8156e06f39a05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e65ecb4645734a5b8b9bfe9cd87c03df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e1f7819e254bdfa5ad03dc22fd4ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9038b0185256420a8ba939c84b5bf1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4442255c2e3f4af0b8b6afcb0f52f43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joony0512/Deep_Learning_Class/blob/main/Part5/P5_Ch06_CH06_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention-based RNN Seq2Seq"
      ],
      "metadata": {
        "id": "QlAMybdqOPEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "id": "dmyaBzFdlM-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6bd372-4b98-4346-a43d-3d8966040551"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lRrRsp4e1a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n",
        "sys.path.append(drive_project_root)\n",
        "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2gozTx8e_5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da7ca89-a8be-4b1a-d2b5-49fadb3c55ef"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 24 07:54:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf\n",
        "!pip install torch_optimizer\n",
        "!pip install wandb\n",
        "!pip install efficientnet_pytorch==0.7.1\n",
        "!pip install hydra-core==1.1\n",
        "!pip install pytorch-lightning\n",
        "!pip install --upgrade torchmetrics\n",
        "!pip install --upgrade pytorch-lightning"
      ],
      "metadata": {
        "id": "-hxbJwYrd3IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy5EQxBhg1Ot"
      },
      "source": [
        "# For data loading.\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "from typing import Union\n",
        "from typing import Any\n",
        "from typing import Optional\n",
        "from typing import Iterable\n",
        "from abc import abstractmethod\n",
        "from abc import ABC\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pprint import pprint\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator, vocab\n",
        "import spacy\n",
        "\n",
        "# For configuration\n",
        "from omegaconf import DictConfig\n",
        "from omegaconf import OmegaConf\n",
        "import hydra\n",
        "from hydra.core.config_store import ConfigStore\n",
        "\n",
        "# For logger\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb\n",
        "os.environ[\"WANDB_START_METHOD\"]=\"thread\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/drive/MyDrive/#fastcampus\n",
        "from data_utils import dataset_split\n",
        "from config_utils import flatten_dict\n",
        "from config_utils import register_config\n",
        "from config_utils import configure_optimizers_from_cfg\n",
        "from config_utils import get_loggers\n",
        "from config_utils import get_callbacks\n",
        "%cd /content\n"
      ],
      "metadata": {
        "id": "appgBNlkgVE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb33c29-9639-4eb5-a5f8-21dbbd2030b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/#fastcampus\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rJQ1e5cjIaL"
      },
      "source": [
        "# download eng/d data.\n",
        "!python -m spacy download en\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvFCUvp0JVpi"
      },
      "source": [
        "# practice data first go to dataconfig\n",
        "\n",
        "# data configs\n",
        "data_spacy_de_en_cfg = {\n",
        "    \"name\": \"spacy_de_en\",\n",
        "    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n",
        "    \"tokenizer\": \"spacy\",\n",
        "    \"src_lang\": \"de\",\n",
        "    \"tgt_lang\": \"en\",\n",
        "    \"src_index\": 0,\n",
        "    \"tgt_index\": 1,\n",
        "    \"vocab\": {\n",
        "        \"special_symbol2index\": {\n",
        "            # Define special symbols and indices\n",
        "            # UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "            # Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "            '<unk>': 0,\n",
        "            '<pad>': 1,\n",
        "            '<bos>': 2,\n",
        "            '<eos>': 3,\n",
        "        },\n",
        "        \"special_first\": True,\n",
        "        \"min_freq\": 2\n",
        "    }\n",
        "}\n",
        "\n",
        "data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahy7rqKvVHj6"
      },
      "source": [
        "# get dataset\n",
        "# data_root = os.path.join(os.getcwd(), \"data\")\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n",
        "\n",
        "test_data = to_map_style_dataset(test_data)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTTXS5ptVE01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0aa84b-5ec4-48c3-ba02-204721ee3afa"
      },
      "source": [
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "# python -m spacy download de_core_news_sm\n",
        "\n",
        "def get_token_transform(data_cfg: DictConfig) -> dict:\n",
        "    token_transform = {}\n",
        "    token_transform[data_cfg.src_lang] = get_tokenizer(\n",
        "        data_cfg.tokenizer, language=data_cfg.src_lang\n",
        "    )\n",
        "    token_transform[data_cfg.tgt_lang] = get_tokenizer(\n",
        "        data_cfg.tokenizer, language=data_cfg.tgt_lang\n",
        "    )\n",
        "    return token_transform\n",
        "\n",
        "token_transform = get_token_transform(data_cfg)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZTKeabmTPI2"
      },
      "source": [
        "# helper function to yield list of tokens\n",
        "def yield_tokens(\n",
        "    data_iter: Iterable, lang: str, lang2index: Dict[str, int]\n",
        ") -> List[str]:\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[lang](data_sample[lang2index[lang]])\n",
        "\n",
        "def get_vocab_transform(data_cfg: DictConfig) -> dict:\n",
        "    vocab_transform = {}\n",
        "    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n",
        "        # Training data Iterator\n",
        "        train_iter = Multi30k(\n",
        "            split='train', language_pair=(data_cfg.src_lang, data_cfg.tgt_lang)\n",
        "        )\n",
        "        # Create torchtext's Vocab object\n",
        "        vocab_transform[ln] = build_vocab_from_iterator(\n",
        "            yield_tokens(\n",
        "                train_iter,\n",
        "                ln,\n",
        "                {\n",
        "                    data_cfg.src_lang: data_cfg.src_index,\n",
        "                    data_cfg.tgt_lang: data_cfg.tgt_index\n",
        "                }\n",
        "            ),\n",
        "            min_freq=data_cfg.vocab.min_freq,\n",
        "            specials=list(data_cfg.vocab.special_symbol2index.keys()),\n",
        "            special_first=data_cfg.vocab.special_first,\n",
        "        )\n",
        "\n",
        "    # Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n",
        "        vocab_transform[ln].set_default_index(\n",
        "            data_cfg.vocab.special_symbol2index[\"<unk>\"]\n",
        "        )\n",
        "    return vocab_transform\n",
        "\n",
        "vocab_transform = get_vocab_transform(data_cfg)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "495ATXpjVaNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7000808-cd20-44af-a6e3-c337f97dd6ab"
      },
      "source": [
        "print(vocab_transform[\"de\"][\"<unk>\"])\n",
        "print(vocab_transform[\"en\"][\"<unk>\"])\n",
        "print(vocab_transform[\"en\"][\"hello\"], vocab_transform[\"en\"][\"world\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "5465 1870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IhDtDo2S2gq"
      },
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int], bos_index: int, eos_index: int):\n",
        "    return torch.cat((torch.tensor([bos_index]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([eos_index])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "def get_text_transform(data_cfg):\n",
        "    text_transform = {}\n",
        "    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n",
        "        text_transform[ln] = sequential_transforms(\n",
        "            token_transform[ln], #Tokenization\n",
        "            vocab_transform[ln], #Numericalization\n",
        "            partial(\n",
        "                tensor_transform,\n",
        "                bos_index=data_cfg.vocab.special_symbol2index[\"<bos>\"],\n",
        "                eos_index=data_cfg.vocab.special_symbol2index[\"<eos>\"],\n",
        "            )\n",
        "        ) # Add BOS/EOS and create tensor\n",
        "    return text_transform\n",
        "\n",
        "text_transform = get_text_transform(data_cfg)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfrgrCnKVZJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2f1fa4-4a8b-477b-b643-111f173aa3a1"
      },
      "source": [
        "print(text_transform[\"en\"](\"hello\"))\n",
        "print(text_transform[\"en\"](\"hello,\"))\n",
        "print(text_transform[\"en\"](\"hello, how\"))\n",
        "print(text_transform[\"en\"](\"hello, how are you ?\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   2, 5465,    3])\n",
            "tensor([   2, 5465,   15,    3])\n",
            "tensor([   2, 5465,   15,  889,    3])\n",
            "tensor([   2, 5465,   15,  889,   17, 1328, 2470,    3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzuZ4z41T9vi"
      },
      "source": [
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch, data_cfg: DictConfig):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[data_cfg.src_lang](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[data_cfg.tgt_lang](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=data_cfg.vocab.special_symbol2index[\"<pad>\"])\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=data_cfg.vocab.special_symbol2index[\"<pad>\"])\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "def get_collate_fn(cfg: DictConfig):\n",
        "    return partial(collate_fn, data_cfg=cfg.data)\n",
        "\n",
        "def get_multi30k_dataloader(\n",
        "    split_mode: str, language_pair, batch_size: int, collate_fn\n",
        "):\n",
        "    iter = Multi30k(split=split_mode, language_pair=language_pair)\n",
        "    dataset = to_map_style_dataset(iter)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, collate_fn=collate_fn\n",
        "    )\n",
        "    return dataloader\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH0CHCr7hh-8"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKy2UW9qWa__"
      },
      "source": [
        "def _text_postprocessing(res: List[str]) -> str:\n",
        "    if \"<eos>\" in res:\n",
        "        res = res[:res.index(\"<eos>\")]\n",
        "    if \"<pad>\" in res:\n",
        "        res = res[:res.index(\"<pad>\")]\n",
        "    res = \" \".join(res).replace(\"<bos>\", \"\")\n",
        "    return res\n",
        "\n",
        "class BaseTranslateLightningModule(pl.LightningModule):\n",
        "    def __init__(self, cfg: DictConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(\n",
        "            ignore_index=cfg.data.vocab.special_symbol2index[\"<pad>\"]\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self._optimizers, self._schedulers = configure_optimizers_from_cfg(\n",
        "            self.cfg, self\n",
        "        )\n",
        "        return self._optimizers, self._schedulers\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(x):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _forward(self, src, tgt, mode: str, teacher_forcing_ratio: float = 0.5):\n",
        "\n",
        "        assert mode in [\"train\", \"val\", \"test\"]\n",
        "\n",
        "        # get predictions\n",
        "        tgt_inputs = tgt[:-1, :] # delete ends...\n",
        "        outputs = self(src, tgt_inputs, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        tgt_outputs = tgt[1:, :]  # remove start tokens..\n",
        "\n",
        "        loss = self.loss_function(\n",
        "            outputs.reshape(-1, outputs.shape[-1]),\n",
        "            tgt_outputs.reshape(-1)\n",
        "        )\n",
        "\n",
        "        logs_detail = {\n",
        "            f\"{mode}_src\": src,\n",
        "            f\"{mode}_tgt\": tgt,\n",
        "            f\"{mode}_results\": outputs,\n",
        "        }\n",
        "\n",
        "        if mode in [\"val\", \"test\"]:\n",
        "            _, tgt_results = torch.max(outputs, dim=2)\n",
        "\n",
        "            src_texts = []\n",
        "            tgt_texts = []\n",
        "            res_texts = []\n",
        "\n",
        "            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n",
        "                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n",
        "                src_texts.append(_text_postprocessing(res))\n",
        "\n",
        "            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n",
        "                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n",
        "                tgt_texts.append(_text_postprocessing(res))\n",
        "\n",
        "            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n",
        "                res = vocab_transform[cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n",
        "                res_texts.append(_text_postprocessing(res))\n",
        "\n",
        "            text_result_summary = {\n",
        "                f\"{mode}_src_text\": src_texts,\n",
        "                f\"{mode}_tgt_text\": tgt_texts,\n",
        "                f\"{mode}_results_text\": res_texts,\n",
        "            }\n",
        "            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text:{res_texts[0]}\")\n",
        "            logs_detail.update(text_result_summary)\n",
        "\n",
        "        return {f\"{mode}_loss\": loss}, logs_detail\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src, tgt = batch[0], batch[1]\n",
        "\n",
        "        logs, logs_detail = self._forward(src, tgt, \"train\", self.cfg.model.teacher_forcing_ratio)\n",
        "        self.log_dict(logs)\n",
        "        logs[\"loss\"] = logs[\"train_loss\"]\n",
        "        return logs\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src, tgt = batch[0], batch[1]\n",
        "        logs, logs_detail = self._forward(src, tgt, \"val\", 0.0)\n",
        "        self.log_dict(logs)\n",
        "        logs[\"loss\"] = logs[\"val_loss\"]\n",
        "        logs.update(logs_detail)\n",
        "\n",
        "        return logs\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        src, tgt = batch[0], batch[1]\n",
        "        logs, logs_detail = self._forward(images, labels, \"test\", 0.0)\n",
        "        self.log_dict(logs)\n",
        "        logs[\"loss\"] = logs[\"test_loss\"]\n",
        "        logs.update(logs_detail)\n",
        "        # wandb_logger, tensorboard_logger = self.logger.experiment\n",
        "        # wandb_logger.log(logs_detail)\n",
        "        # self.log_dict(logs)\n",
        "        return logs\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbnX8ZJZibpy"
      },
      "source": [
        "# weight initialization\n",
        "def init_weights(model: Union[nn.Module, pl.LightningModule]):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTD0MaS_vVN"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        embed_dim: int,\n",
        "        hidden_dim: int,\n",
        "        n_layers: int,\n",
        "        dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # embedded = [src len, batch size, emb dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        # outputs = [src len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # outputs are always from the top hidden layer\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim: int,\n",
        "        embed_dim: int,\n",
        "        hidden_dim: int,\n",
        "        n_layers: int,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        # input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        # output = [seq len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        # output = [1, batch size, hid dim]\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # cell = [n layers, batch size, hid dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class LSTMSeq2Seq(BaseTranslateLightningModule):\n",
        "    def __init__(self, cfg: DictConfig):\n",
        "        super().__init__(cfg)\n",
        "        # encoder, decoder, device\n",
        "\n",
        "        self.encoder = LSTMEncoder(**cfg.model.enc)\n",
        "        self.decoder = LSTMDecoder(**cfg.model.dec)\n",
        "        # self.device = device\n",
        "\n",
        "        assert self.encoder.hidden_dim == self.decoder.hidden_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert self.encoder.n_layers == self.decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        # src = [src len, batch size]\n",
        "        # trg = [trg len, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU + Bidirectional"
      ],
      "metadata": {
        "id": "l-NbEV1wPcJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Concat(Addictive) Attention 기반의 모델 새로 정의\n",
        "# encoder decoder RNN 이 다를 수 있다\n",
        "class BidirectionalGRUEncoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim: int,\n",
        "            embed_dim: int,\n",
        "            enc_hidden_dim: int,\n",
        "            dec_hidden_dim: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, enc_hidden_dim, n_layers, bidirectional = True, dropout = dropout)\n",
        "        self.fc = nn.Linear(enc_hidden_dim * 2 , dec_hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # initialization of weights\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # embedded = [src len, batch size, emb dim]\n",
        "        outputs, hidden = self.rnn(embedded) # lstm과 달리 hidden으로 받는다\n",
        "\n",
        "        # outputs = [src len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # hidden -> [forward_1, barchward_1, forward_2, backward_2..]\n",
        "        # 우리가 필요한것은 맨위 마지막 layer의 forward backward 두개 concat 한개 필요\n",
        "        # =>torch.cat((hidden[-2, : , :], hidden[-1, : , :]), dim =1)\n",
        "\n",
        "        # encoder RNNs fed through a linear layer to connect decoder.\n",
        "        hidden = torch.tanh(self.fc(\n",
        "            torch.cat((hidden[-2, : , :], hidden[-1, : , :]), dim =1)\n",
        "        ))\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class ConcatAttention(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim : int, dec_hidden_dim : int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim, dec_hidden_dim)\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden = [batch_size, dec_hidden_dim] => from decoder (query)\n",
        "        # encoder_outputs = [src_len, batch_len, enc_hidden_dim * 2] (key, value)\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # repeat decoder hidden state src_len times ...\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        # hidden : [batch_size, src_len, dec_hidden_dim]\n",
        "        # encoder_outputs = [batch_dize, src_len, enc_hidden_dim *2]\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        # energy = [batch_size, src_len, dec_hidden_dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        # attention = [batch_size, src_len]\n",
        "\n",
        "        return F.softmax(attention, dim =1)\n",
        "\n",
        "class AttentionRNNDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim: int,\n",
        "        embed_dim: int,\n",
        "        enc_hidden_dim: int,\n",
        "        dec_hidden_dim: int,\n",
        "        n_layers: int,\n",
        "        dropout: float,\n",
        "        attention : nn.Module\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.rnn = nn.GRU((enc_hidden_dim * 2) + embed_dim, dec_hidden_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear((enc_hidden_dim *2) + dec_hidden_dim + embed_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "\n",
        "        # input [batch size] # start token\n",
        "        # hidden [batch_size, dec_hidden_dim]\n",
        "        # encoder_outputs [src_len, batch_size, enc_hidden_dim*2]\n",
        "\n",
        "        input = input.unsqueeze(0) # input = [1, batch_size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input)) # [1, batch_size, embed_dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs) # [batch_size, src_len]\n",
        "        a = a.unsqueeze(1) # [batch_size, 1, src_len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1,0,2) #[batch_size, src_len, enc_hidden_dim *2]\n",
        "        weighted = torch.bmm(a, encoder_outputs) # [batch_size, 1, enc_hidden_dim*2]\n",
        "\n",
        "        weighted = weighted.permute(1,0,2) # [1, batch_size, enc_hidden_dim *2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2) # [1, batch_dize, (enc_hidden_dim *2 + embed_dim)]\n",
        "\n",
        "        # hidden.unsqueeze : [1, batch_size, dec_hidden_dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        # output = [seq_len, batch_size , dec_hidden_dim * n directions] => [1, batch_size, dec_hidden_dim]\n",
        "        # hidden = [n layers * n_directions, batch_size, dec_hidden_dim] => [1, batch_size, dec_hidden_dim]\n",
        "\n",
        "        if not (output == hidden).all():\n",
        "            raise AssertionError()\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0)\n",
        "\n",
        "class AttentionBasedSeq2Seq(BaseTranslateLightningModule):\n",
        "    def __init__(self, cfg : DictConfig):\n",
        "        super().__init__(cfg)\n",
        "        self.encoder = BidirectionalGRUEncoder(**cfg.model.enc)\n",
        "        self.attention = ConcatAttention(**cfg.model.attention)\n",
        "        self.decoder = AttentionRNNDecoder(\n",
        "            attention = self.attention, **cfg.model.dec\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "            # src = [src len, batch size]\n",
        "            # trg = [trg len, batch size]\n",
        "            # teacher_forcing_ratio is probability to use teacher forcing\n",
        "            # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "            batch_size = trg.shape[1]\n",
        "            trg_len = trg.shape[0]\n",
        "            trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "            #tensor to store decoder outputs\n",
        "            outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "            #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "            encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "            #first input to the decoder is the <sos> tokens\n",
        "            input = trg[0,:]\n",
        "\n",
        "            for t in range(1, trg_len):\n",
        "\n",
        "                #insert input token embedding, previous hidden and previous cell states\n",
        "                #receive output tensor (predictions) and new hidden and cell states\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "                #place predictions in a tensor holding predictions for each token\n",
        "                outputs[t] = output\n",
        "\n",
        "                #decide if we are going to use teacher forcing or not\n",
        "                teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "                #get the highest predicted token from our predictions\n",
        "                top1 = output.argmax(1)\n",
        "\n",
        "                #if teacher forcing, use actual next token as next input\n",
        "                #if not, use predicted token\n",
        "                input = trg[t] if teacher_force else top1\n",
        "\n",
        "            return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "-5iq55TsPbc5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydyN6EmnQfLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8890fa-6ac9-4ade-98ab-23c4dd4581cc"
      },
      "source": [
        "\n",
        "# data configs\n",
        "data_spacy_de_en_cfg = {\n",
        "    \"name\": \"spacy_de_en\",\n",
        "    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n",
        "    \"tokenizer\": \"spacy\",\n",
        "    \"src_lang\": \"de\",\n",
        "    \"tgt_lang\": \"en\",\n",
        "    \"src_index\": 0,\n",
        "    \"tgt_index\": 1,\n",
        "    \"vocab\": {\n",
        "        \"special_symbol2index\": {\n",
        "            # Define special symbols and indices\n",
        "            # UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "            # Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "            '<unk>': 0,\n",
        "            '<pad>': 1,\n",
        "            '<bos>': 2,\n",
        "            '<eos>': 3,\n",
        "        },\n",
        "        \"special_first\": True,\n",
        "        \"min_freq\": 2\n",
        "    }\n",
        "}\n",
        "\n",
        "data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n",
        "\n",
        "# get dataset\n",
        "# data_root = os.path.join(os.getcwd(), \"data\")\n",
        "train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n",
        "\n",
        "token_transform = get_token_transform(data_cfg)\n",
        "vocab_transform = get_vocab_transform(data_cfg)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIjxSubTLAKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede9c0b7-7c3b-4f44-c79c-2c01afe047d5"
      },
      "source": [
        "# model configs\n",
        "model_translate_lstm_seq2seq_cfg = {\n",
        "    \"name\": \"LSTMSeq2Seq\",\n",
        "    \"out_dim\": len(vocab_transform[data_cfg.src_lang]),\n",
        "    \"enc\": {\n",
        "        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n",
        "        \"embed_dim\": 256,\n",
        "        \"hidden_dim\": 256,\n",
        "        \"n_layers\": 2,\n",
        "        \"dropout\": 0.5,\n",
        "    },\n",
        "    \"dec\": {\n",
        "        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n",
        "        \"embed_dim\": 256,\n",
        "        \"hidden_dim\": 256,\n",
        "        \"n_layers\": 2,\n",
        "        \"dropout\": 0.5,\n",
        "    },\n",
        "    \"teacher_forcing_ratio\": 0.5\n",
        "}\n",
        "\n",
        "model_translate_attention_based_seq2seq_cfg = {\n",
        "    \"name\": \"AttentionBasedSeq2Seq\",\n",
        "    \"enc\": {\n",
        "        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n",
        "        \"embed_dim\": 256,\n",
        "        \"enc_hidden_dim\": 512,\n",
        "        \"dec_hidden_dim\": 512,\n",
        "        \"n_layers\": 1,\n",
        "        \"dropout\": 0.5,\n",
        "    },\n",
        "    \"dec\": {\n",
        "        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n",
        "        \"embed_dim\": 256,\n",
        "        \"enc_hidden_dim\": 512,\n",
        "        \"dec_hidden_dim\": 512,\n",
        "        \"n_layers\": 1,\n",
        "        \"dropout\": 0.5,\n",
        "    },\n",
        "    \"attention\" : {\n",
        "        \"enc_hidden_dim\": 512,\n",
        "        \"dec_hidden_dim\": 512,\n",
        "    },\n",
        "    \"teacher_forcing_ratio\": 0.5\n",
        "}\n",
        "\n",
        "\n",
        "# optimizer configs\n",
        "opt_cfg = {\n",
        "    \"optimizers\": [\n",
        "        {\n",
        "            \"name\": \"RAdam\",\n",
        "            \"kwargs\": {\n",
        "                \"lr\": 1e-3,\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        "    \"lr_schedulers\": [\n",
        "        {\n",
        "            \"name\": None,\n",
        "            \"kwargs\": {\n",
        "                \"warmup_end_steps\": 1000\n",
        "            }\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "_merged_cfg_presets = {\n",
        "    \"LSTM_seq2seq_de_en_translate\": {\n",
        "        \"opt\": opt_cfg,\n",
        "        \"data\": data_spacy_de_en_cfg,\n",
        "        \"model\": model_translate_lstm_seq2seq_cfg,\n",
        "    },\n",
        "    \"attention_based_seq2seq_de_en_translate\": {\n",
        "        \"opt\": opt_cfg,\n",
        "        \"data\": data_spacy_de_en_cfg,\n",
        "        \"model\": model_translate_attention_based_seq2seq_cfg,\n",
        "    },\n",
        "}\n",
        "\n",
        "# clear config instance first\n",
        "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
        "\n",
        "# register preset configs\n",
        "register_config(_merged_cfg_presets)\n",
        "\n",
        "# initialize & make config\n",
        "## select mode here ##\n",
        "# .................. #\n",
        "hydra.initialize(config_path=None)\n",
        "cfg = hydra.compose(\"attention_based_seq2seq_de_en_translate\")\n",
        "\n",
        "# override some cfg\n",
        "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n",
        "\n",
        "# Define other train configs & log_configs\n",
        "# Merge configs into one & register it to Hydra.\n",
        "project_root_dir = os.path.join(\n",
        "    drive_project_root, \"runs\", \"de_en_translate\"\n",
        ")\n",
        "save_dir = os.path.join(project_root_dir, run_name)\n",
        "run_root_dir = os.path.join(project_root_dir, run_name)\n",
        "# train configs\n",
        "train_cfg ={\n",
        "    'train_batch_size' : 128,\n",
        "    'val_batch_size' : 32,\n",
        "    'test_batch_size' : 32,\n",
        "    'train_val_split' : [0.9,0.1],\n",
        "    'run_root_dir' : run_root_dir,\n",
        "    'trainer_kwargs' : {\n",
        "        'accelerator': 'gpu',\n",
        "        'num_nodes' : 0,\n",
        "        'max_epochs' :50,\n",
        "        'val_check_interval': 1.0, #train 1epoch당 val 1회\n",
        "        'log_every_n_steps' : 100,\n",
        "        # 'flush_logs_every_n_steps' : 100, #100번 step마다\n",
        "    }\n",
        "\n",
        "}\n",
        "# logger configs\n",
        "log_cfg = {\n",
        "    'loggers' : {\n",
        "        'WandbLogger' : {\n",
        "            'project' : 'fastcampus_de_en_translate_tutorials',\n",
        "            'name' : run_name,\n",
        "            'tags' : ['fastcampus_de_en_translate_tutorials'],\n",
        "            'save_dir' : run_root_dir,\n",
        "\n",
        "        },\n",
        "        'TensorBoardLogger' : {\n",
        "            'save_dir' : project_root_dir,\n",
        "            'name' : run_name,\n",
        "        }\n",
        "    },\n",
        "    'callbacks' : {\n",
        "        'ModelCheckpoint' : {\n",
        "            'save_top_k' : 3,\n",
        "            'monitor' : 'val_loss',\n",
        "            'mode' : 'min',\n",
        "            'verbose' : True,\n",
        "            'dirpath' : os.path.join(run_root_dir, 'weights'),\n",
        "            'filename' : '{epoch}-{val_loss:.3f}',\n",
        "\n",
        "        },\n",
        "        'EarlyStopping' : {\n",
        "            'monitor' : 'val_loss',\n",
        "            'mode' : 'min',\n",
        "            'patience' : 3,\n",
        "            'verbose' : True\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# unlock config & set train, log config\n",
        "OmegaConf.set_struct(cfg, False)\n",
        "cfg.train =train_cfg\n",
        "cfg.log = log_cfg\n",
        "\n",
        "# lock config\n",
        "OmegaConf.set_struct(cfg, True)\n",
        "print(OmegaConf.to_yaml(cfg))\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opt:\n",
            "  optimizers:\n",
            "  - name: RAdam\n",
            "    kwargs:\n",
            "      lr: 0.001\n",
            "  lr_schedulers:\n",
            "  - name: null\n",
            "    kwargs:\n",
            "      warmup_end_steps: 1000\n",
            "data:\n",
            "  name: spacy_de_en\n",
            "  data_root: /content/data\n",
            "  tokenizer: spacy\n",
            "  src_lang: de\n",
            "  tgt_lang: en\n",
            "  src_index: 0\n",
            "  tgt_index: 1\n",
            "  vocab:\n",
            "    special_symbol2index:\n",
            "      <unk>: 0\n",
            "      <pad>: 1\n",
            "      <bos>: 2\n",
            "      <eos>: 3\n",
            "    special_first: true\n",
            "    min_freq: 2\n",
            "model:\n",
            "  name: AttentionBasedSeq2Seq\n",
            "  enc:\n",
            "    input_dim: 8014\n",
            "    embed_dim: 256\n",
            "    enc_hidden_dim: 512\n",
            "    dec_hidden_dim: 512\n",
            "    n_layers: 1\n",
            "    dropout: 0.5\n",
            "  dec:\n",
            "    output_dim: 6191\n",
            "    embed_dim: 256\n",
            "    enc_hidden_dim: 512\n",
            "    dec_hidden_dim: 512\n",
            "    n_layers: 1\n",
            "    dropout: 0.5\n",
            "  attention:\n",
            "    enc_hidden_dim: 512\n",
            "    dec_hidden_dim: 512\n",
            "  teacher_forcing_ratio: 0.5\n",
            "train:\n",
            "  train_batch_size: 128\n",
            "  val_batch_size: 32\n",
            "  test_batch_size: 32\n",
            "  train_val_split:\n",
            "  - 0.9\n",
            "  - 0.1\n",
            "  run_root_dir: /content/drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en\n",
            "  trainer_kwargs:\n",
            "    accelerator: gpu\n",
            "    num_nodes: 0\n",
            "    max_epochs: 50\n",
            "    val_check_interval: 1.0\n",
            "    log_every_n_steps: 100\n",
            "log:\n",
            "  loggers:\n",
            "    WandbLogger:\n",
            "      project: fastcampus_de_en_translate_tutorials\n",
            "      name: 2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en\n",
            "      tags:\n",
            "      - fastcampus_de_en_translate_tutorials\n",
            "      save_dir: /content/drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en\n",
            "    TensorBoardLogger:\n",
            "      save_dir: /content/drive/MyDrive/#fastcampus/runs/de_en_translate\n",
            "      name: 2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en\n",
            "  callbacks:\n",
            "    ModelCheckpoint:\n",
            "      save_top_k: 3\n",
            "      monitor: val_loss\n",
            "      mode: min\n",
            "      verbose: true\n",
            "      dirpath: /content/drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en/weights\n",
            "      filename: '{epoch}-{val_loss:.3f}'\n",
            "    EarlyStopping:\n",
            "      monitor: val_loss\n",
            "      mode: min\n",
            "      patience: 3\n",
            "      verbose: true\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YG__y9MXx2N"
      },
      "source": [
        "# dataloader def\n",
        "train_dataloader = get_multi30k_dataloader(\n",
        "    \"train\",\n",
        "    (cfg.data.src_lang, cfg.data.tgt_lang),\n",
        "    cfg.train.train_batch_size,\n",
        "    collate_fn=get_collate_fn(cfg)\n",
        ")\n",
        "val_dataloader = get_multi30k_dataloader(\n",
        "    \"valid\",\n",
        "    (cfg.data.src_lang, cfg.data.tgt_lang),\n",
        "    cfg.train.val_batch_size,\n",
        "    collate_fn=get_collate_fn(cfg)\n",
        ")\n",
        "test_dataloader = get_multi30k_dataloader(\n",
        "    \"test\",\n",
        "    (cfg.data.src_lang, cfg.data.tgt_lang),\n",
        "    cfg.train.test_batch_size,\n",
        "    collate_fn=get_collate_fn(cfg)\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_77rnkHeMiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0cf324-f8da-4082-df86-038b40cbb871"
      },
      "source": [
        "# model definition\n",
        "def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n",
        "    if cfg.model.name == \"LSTMSeq2Seq\":\n",
        "        model = LSTMSeq2Seq(cfg)\n",
        "    elif cfg.model.name ==\"AttentionBasedSeq2Seq\":\n",
        "        model =AttentionBasedSeq2Seq(cfg)\n",
        "    else:\n",
        "        raise NotImplementedError(\"not implemented model\")\n",
        "\n",
        "    if checkpoint_path is not None:\n",
        "        model = model.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
        "    return model\n",
        "\n",
        "model = None\n",
        "model = get_pl_model(cfg)\n",
        "print(model)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentionBasedSeq2Seq(\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (encoder): BidirectionalGRUEncoder(\n",
            "    (embedding): Embedding(8014, 256)\n",
            "    (rnn): GRU(256, 512, dropout=0.5, bidirectional=True)\n",
            "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (attention): ConcatAttention(\n",
            "    (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
            "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder): AttentionRNNDecoder(\n",
            "    (attention): ConcatAttention(\n",
            "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
            "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
            "    )\n",
            "    (embedding): Embedding(6191, 256)\n",
            "    (rnn): GRU(1280, 512, dropout=0.5)\n",
            "    (fc_out): Linear(in_features=1792, out_features=6191, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWOEyvQNhG4O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "68f56ec3-49f4-4b47-dbaa-76049fa7df94"
      },
      "source": [
        "# pytorch-lightning trainer def\n",
        "\n",
        "logger = get_loggers(cfg)\n",
        "callbacks = get_callbacks(cfg)\n",
        "\n",
        "trainer =pl.Trainer(\n",
        "    callbacks = callbacks,\n",
        "    logger = logger,\n",
        "    default_root_dir= cfg.train.run_root_dir,\n",
        "    num_sanity_val_steps=2,\n",
        "    **cfg.train.trainer_kwargs\n",
        ")\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">2023-07-24T08:02:44-AttentionBasedSeq2Seq-spacy_de_en</strong> at: <a href='https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials/runs/omclqff6' target=\"_blank\">https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials/runs/omclqff6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:02:44-AttentionBasedSeq2Seq-spacy_de_en/wandb/run-20230724_080342-omclqff6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhyejun12123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en/wandb/run-20230724_080810-xf6bczg1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials/runs/xf6bczg1' target=\"_blank\">2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en</a></strong> to <a href='https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials' target=\"_blank\">https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials/runs/xf6bczg1' target=\"_blank\">https://wandb.ai/hyejun12123/fastcampus_de_en_translate_tutorials/runs/xf6bczg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkhYbOhBbDC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2799f03f44dd4f38a0dcb721f0b96e4c",
            "a80daed5fa9d4ed18bb9be8bacf0e659",
            "2d85aff9d2e54df5ac1a56c82f1ca66a",
            "a4a36f0e37b5475c852b43aa72502515",
            "c1b62997f2084108b9c2cd57a4b4a05c",
            "36e89b93f90241d7a4efb8ca51300b86",
            "f60411d1ffab44cb96c5aff4d12e919d",
            "5d5bbf93333a48b38d85b61f4ad30cf6",
            "5a4b8752a3ed494ab359aa53a16acdfe",
            "401e0315c5c3436f84a6f06a7087e1a1",
            "f86306777ecb45558bd45f01e8e6ec4b",
            "5fd47fe92dcc4f5e9fc9f48f5e1e46e8",
            "9f099a4891b74a1ab8b39f90da08c18f",
            "db185360f7644f17a717d4036268b828",
            "2f7c0a4a5f7c4ebaa258b3fc7bbbd61e",
            "07e561d8afe14ab4b409c7a4fd435c5a",
            "72917c7f2cda4e098e93a8324415c75c",
            "6736af2e43804ee5a06d4e6be6493f0d",
            "96d37e1cbb114ea7962881bf6602701f",
            "ffd36a1654244adb90a44f258b9924e5",
            "6087d06126c64405bbf6ef8e518e5017",
            "2cc7849ea7ae49929b3d91ab21fffab4",
            "d905b29a1c1b4359a7024ac267887a41",
            "d1e2f9dffeb0448ba69098cbebd81c3b",
            "695a58b53b064cb09422286309e6e8dc",
            "a8e230b424c24f7786fa3e5cbaedd56b",
            "955bdbae46fa4806bd16cf7c598b5637",
            "1875d792c0574e79a17f798c986aa223",
            "61ec06e1ff954e8ea051b4706db301d9",
            "fb2a48efbcc84e588b1c486ef667a1b8",
            "d82e59a88ac940fb8afad8baa07befef",
            "37bdf34c8d2347e197bf02e7b53fb470",
            "9c7e157b9f6d4612908c2f9aa6a3afd1",
            "6415d451fd7a44efa22513a5003b06a3",
            "fc047986b7814a97ab69431bc1f844c1",
            "2cee393505154b7f9f7fe2c6fa03501b",
            "3db2911db72f4ddaae6aa80200184527",
            "affba57a3f1940cd935834aec3f0100d",
            "3116240132a64a4e899560930618bb75",
            "cf8c53d96bd94ae29c8156e06f39a05e",
            "e65ecb4645734a5b8b9bfe9cd87c03df",
            "23e1f7819e254bdfa5ad03dc22fd4ea3",
            "9038b0185256420a8ba939c84b5bf1fb",
            "4442255c2e3f4af0b8b6afcb0f52f43c"
          ]
        },
        "outputId": "5f86a546-37ad-46e6-95f2-08735a05a9fc"
      },
      "source": [
        "trainer.fit(model, train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name          | Type                    | Params\n",
            "----------------------------------------------------------\n",
            "0 | loss_function | CrossEntropyLoss        | 0     \n",
            "1 | encoder       | BidirectionalGRUEncoder | 4.9 M \n",
            "2 | attention     | ConcatAttention         | 787 K \n",
            "3 | decoder       | AttentionRNNDecoder     | 16.2 M\n",
            "----------------------------------------------------------\n",
            "21.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "21.2 M    Total params\n",
            "84.681    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2799f03f44dd4f38a0dcb721f0b96e4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 step: \n",
            " src_text:  Eine Gruppe von Männern lädt <unk> auf einen Lastwagen, \n",
            " tgt_text:  A group of men are loading cotton onto a truck, \n",
            " result_text:<unk> television downwards toil Capris casting wrapping Cricket fake buoy loader hurt waking Factory skier process remember Lone Welcome briefs Rocks tents Vendors butcher milk mounds\n",
            "0 step: \n",
            " src_text:  Kind spielt auf einem Spielplatz und hängt dabei an Stangen ., \n",
            " tgt_text:  Child playing on a playground , hanging from bars ., \n",
            " result_text:<unk> television downwards toil Capris casting wrapping Cricket fake buoy loader hurt waking Factory skier process remember Lone Welcome briefs Rocks tents Vendors butcher milk mounds screwdriver almost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 35. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fd47fe92dcc4f5e9fc9f48f5e1e46e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d905b29a1c1b4359a7024ac267887a41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227 step: \n",
            " src_text:  Eine Gruppe von Männern lädt <unk> auf einen Lastwagen, \n",
            " tgt_text:  A group of men are loading cotton onto a truck, \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Kind spielt auf einem Spielplatz und hängt dabei an Stangen ., \n",
            " tgt_text:  Child playing on a playground , hanging from bars ., \n",
            " result_text:<unk> man in a a a a a .\n",
            "227 step: \n",
            " src_text:  Eine bunt gekleidete Frau geht an einem weißen Lastwagen vorbei , der mit Flaschen gefüllt ist ., \n",
            " tgt_text:  A woman in a colorful outfit is walking by a white truck filled with bottles ., \n",
            " result_text:<unk> man in a a a a a a a a .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 34. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 23. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227 step: \n",
            " src_text:  Zwei Kinder graben Löcher in die Erde ., \n",
            " tgt_text:  <unk> children dig holes in the dirt ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Mann in einem kleinen weißen Boot auf einem See ., \n",
            " tgt_text:  Man in a small white boat on a lake ., \n",
            " result_text:<unk> man in a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein junger Mann mit blondem Haar spricht in ein Mikrofon ., \n",
            " tgt_text:  A young blond - haired man speaks into a microphone ., \n",
            " result_text:<unk> man in a a a a a a .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 22. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 27. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227 step: \n",
            " src_text:  Ein Hund rennt durch das Gras auf die Kamera zu ., \n",
            " tgt_text:  A dog runs through the grass towards the camera ., \n",
            " result_text:<unk> man in a a a a . .\n",
            "227 step: \n",
            " src_text:  Eine Frau und ein kleiner Junge teilen sich einen Stuhl ., \n",
            " tgt_text:  A woman and little boy share a chair ., \n",
            " result_text:<unk> man in a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein schwarzer Hund steht im Gras und hält einen weißen <unk> in seinem Maul ., \n",
            " tgt_text:  A black dog standing in some grass holding a white plastic item in its mouth ., \n",
            " result_text:<unk> man in a a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Zwei schwarze Hunde rennen auf beiden Seiten eines befestigten Weges, \n",
            " tgt_text:  Two black dogs running down either side of a paved pathway, \n",
            " result_text:<unk> man in a a a a a .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 25. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227 step: \n",
            " src_text:  Vier Frauen haben sich mit lustigen Kostümen verkleidet ., \n",
            " tgt_text:  Four women dressed up in funny costumes ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Eine Person beim <unk> auf dem Meer ., \n",
            " tgt_text:  A person is hang gliding in the ocean ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Zwei Jungen innerhalb eines <unk> springen in die Luft und halten dabei einen Basketball ., \n",
            " tgt_text:  Two boys inside a fence jump in the air while holding a basketball ., \n",
            " result_text:<unk> man in a a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Eine junge Frau spielt ein Saiteninstrument im Haus ., \n",
            " tgt_text:  A young woman practices a string instrument indoors ., \n",
            " result_text:<unk> man in a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein Typ rennt vor einem schwarzen Bullen davon ., \n",
            " tgt_text:  A guy is running away from a black bull ., \n",
            " result_text:<unk> man in a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein kleines Mädchen lächelt und streckt ihren Daumen nach oben , während sie vor einer Schildkröte posiert ., \n",
            " tgt_text:  A young girl smiles and sticks her thumb up while posing in front of a turtle ., \n",
            " result_text:<unk> man in a a a a a a a a a . .\n",
            "227 step: \n",
            " src_text:  Der schwarze Hund springt über dem Wasser in Richtung einer Frisbeescheibe , die in der Nähe eines Boots treibt ., \n",
            " tgt_text:  The black dog jumps above the water towards a Frisbee floating near a boat ., \n",
            " result_text:<unk> man in a a a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein Mann blickt auf einen seiner vier <unk> ., \n",
            " tgt_text:  A man is looking at one of his four flat screen computers ., \n",
            " result_text:<unk> man in a a a a a .\n",
            "227 step: \n",
            " src_text:  Läufer <unk> einen <unk> in der Stadt ., \n",
            " tgt_text:  Runners pass a check point in the city ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Szene eines Mannes beim <unk> , <unk> , die in einer kalten , winterlichen Umgebung herumlaufen, \n",
            " tgt_text:  <unk> of a person shoveling snow citizens walking around in a cold winter environment, \n",
            " result_text:<unk> man in a a a a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein alter Mann sitzt mit einem Tablett auf dem Schoß da ., \n",
            " tgt_text:  An old man sits with a tray in his lap ., \n",
            " result_text:<unk> man in a a a a a a .\n",
            "227 step: \n",
            " src_text:  Ein <unk> , bei dem drei Jugendliche um den Football kämpfen ., \n",
            " tgt_text:  A youth football game <unk> three youths are fighting for a football ., \n",
            " result_text:<unk> man in a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Menschen nehmen an einem <unk> <unk> <unk> <unk> teil ., \n",
            " tgt_text:  People attending a <unk> <unk> <unk> Challenge ., \n",
            " result_text:<unk> man in a a a a a a .\n",
            "227 step: \n",
            " src_text:  Menschen auf der Straße bei einem Straßenfest ., \n",
            " tgt_text:  People on the street at a block party ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Ein Mann mit einem Namensschild sitzt in einem Stuhl ., \n",
            " tgt_text:  A man with a name tag on is sitting in a chair ., \n",
            " result_text:<unk> man in a a a a a a .\n",
            "227 step: \n",
            " src_text:  Kinder werden in einem <unk> <unk> ., \n",
            " tgt_text:  Kids being <unk> around in a glass spinner ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Ein <unk> Auto , das von vielen <unk> <unk> wird ., \n",
            " tgt_text:  A <unk> car with many firefighters cutting into the car ., \n",
            " result_text:<unk> man in a a a a a a a .\n",
            "227 step: \n",
            " src_text:  Drei Mädchen reiten auf Pferden , wobei sich das <unk> Mädchen im <unk> befindet ., \n",
            " tgt_text:  Three girls are horseback riding with the focus on the <unk> girl ., \n",
            " result_text:<unk> man in a a a a a a a .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 26. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 28. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric val_loss improved. New best score: 5.371\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 227: 'val_loss' reached 5.37063 (best 5.37063), saving model to '/content/drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en/weights/epoch=0-val_loss=5.371.ckpt' as top 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227 step: \n",
            " src_text:  Ein kleiner Junge zeigt seine <unk> <unk> ., \n",
            " tgt_text:  A young boy shows his brown and green bead necklace ., \n",
            " result_text:<unk> man in a a a a .\n",
            "227 step: \n",
            " src_text:  Der Junge springt mit einem <unk> aus dem Bett ., \n",
            " tgt_text:  The boy leaps of his bed with a karate kick ., \n",
            " result_text:<unk> man in a a a a a . .\n",
            "227 step: \n",
            " src_text:  Zwei Männer aus gegnerischen Teams rennen in Richtung eines Fußballs ., \n",
            " tgt_text:  Two men on opposing teams race toward a soccer ball ., \n",
            " result_text:<unk> man in a a a a a a .\n",
            "227 step: \n",
            " src_text:  Zwei schwarz gekleidete Männer mit einer grünen und einer roten Fliege treten vor einer Menschenmenge auf ., \n",
            " tgt_text:  Two men in black clothes with blue and red bowties are performing in front of a crowd ., \n",
            " result_text:<unk> man in a a a a a a a a a a .\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6415d451fd7a44efa22513a5003b06a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "454 step: \n",
            " src_text:  Eine Gruppe von Männern lädt <unk> auf einen Lastwagen, \n",
            " tgt_text:  A group of men are loading cotton onto a truck, \n",
            " result_text:<unk> group of are a a a . .\n",
            "454 step: \n",
            " src_text:  Kind spielt auf einem Spielplatz und hängt dabei an Stangen ., \n",
            " tgt_text:  Child playing on a playground , hanging from bars ., \n",
            " result_text:<unk> man is a a a a a . .\n",
            "454 step: \n",
            " src_text:  Eine bunt gekleidete Frau geht an einem weißen Lastwagen vorbei , der mit Flaschen gefüllt ist ., \n",
            " tgt_text:  A woman in a colorful outfit is walking by a white truck filled with bottles ., \n",
            " result_text:<unk> man in a a shirt is a a a a a . .\n",
            "454 step: \n",
            " src_text:  Zwei Kinder graben Löcher in die Erde ., \n",
            " tgt_text:  <unk> children dig holes in the dirt ., \n",
            " result_text:<unk> group of are a a a . .\n",
            "454 step: \n",
            " src_text:  Mann in einem kleinen weißen Boot auf einem See ., \n",
            " tgt_text:  Man in a small white boat on a lake ., \n",
            " result_text:<unk> man in a a shirt is a a . .\n",
            "454 step: \n",
            " src_text:  Ein junger Mann mit blondem Haar spricht in ein Mikrofon ., \n",
            " tgt_text:  A young blond - haired man speaks into a microphone ., \n",
            " result_text:<unk> man in a a shirt is a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein Hund rennt durch das Gras auf die Kamera zu ., \n",
            " tgt_text:  A dog runs through the grass towards the camera ., \n",
            " result_text:<unk> group of are a a a .\n",
            "454 step: \n",
            " src_text:  Eine Frau und ein kleiner Junge teilen sich einen Stuhl ., \n",
            " tgt_text:  A woman and little boy share a chair ., \n",
            " result_text:<unk> man in a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein schwarzer Hund steht im Gras und hält einen weißen <unk> in seinem Maul ., \n",
            " tgt_text:  A black dog standing in some grass holding a white plastic item in its mouth ., \n",
            " result_text:<unk> man in a a a a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Zwei schwarze Hunde rennen auf beiden Seiten eines befestigten Weges, \n",
            " tgt_text:  Two black dogs running down either side of a paved pathway, \n",
            " result_text:<unk> group of are a a a a a .\n",
            "454 step: \n",
            " src_text:  Vier Frauen haben sich mit lustigen Kostümen verkleidet ., \n",
            " tgt_text:  Four women dressed up in funny costumes ., \n",
            " result_text:<unk> group of are a a a . .\n",
            "454 step: \n",
            " src_text:  Eine Person beim <unk> auf dem Meer ., \n",
            " tgt_text:  A person is hang gliding in the ocean ., \n",
            " result_text:<unk> group of are a a a . .\n",
            "454 step: \n",
            " src_text:  Zwei Jungen innerhalb eines <unk> springen in die Luft und halten dabei einen Basketball ., \n",
            " tgt_text:  Two boys inside a fence jump in the air while holding a basketball ., \n",
            " result_text:<unk> men of a a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Eine junge Frau spielt ein Saiteninstrument im Haus ., \n",
            " tgt_text:  A young woman practices a string instrument indoors ., \n",
            " result_text:<unk> man in a a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein Typ rennt vor einem schwarzen Bullen davon ., \n",
            " tgt_text:  A guy is running away from a black bull ., \n",
            " result_text:<unk> man in a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein kleines Mädchen lächelt und streckt ihren Daumen nach oben , während sie vor einer Schildkröte posiert ., \n",
            " tgt_text:  A young girl smiles and sticks her thumb up while posing in front of a turtle ., \n",
            " result_text:<unk> man in a a a a a a a a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Der schwarze Hund springt über dem Wasser in Richtung einer Frisbeescheibe , die in der Nähe eines Boots treibt ., \n",
            " tgt_text:  The black dog jumps above the water towards a Frisbee floating near a boat ., \n",
            " result_text:<unk> man in a a a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein Mann blickt auf einen seiner vier <unk> ., \n",
            " tgt_text:  A man is looking at one of his four flat screen computers ., \n",
            " result_text:<unk> man in a a a a a . .\n",
            "454 step: \n",
            " src_text:  Läufer <unk> einen <unk> in der Stadt ., \n",
            " tgt_text:  Runners pass a check point in the city ., \n",
            " result_text:<unk> group of are a a a . .\n",
            "454 step: \n",
            " src_text:  Szene eines Mannes beim <unk> , <unk> , die in einer kalten , winterlichen Umgebung herumlaufen, \n",
            " tgt_text:  <unk> of a person shoveling snow citizens walking around in a cold winter environment, \n",
            " result_text:<unk> young in a a a and , a the while a the .\n",
            "454 step: \n",
            " src_text:  Ein alter Mann sitzt mit einem Tablett auf dem Schoß da ., \n",
            " tgt_text:  An old man sits with a tray in his lap ., \n",
            " result_text:<unk> man in a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein <unk> , bei dem drei Jugendliche um den Football kämpfen ., \n",
            " tgt_text:  A youth football game <unk> three youths are fighting for a football ., \n",
            " result_text:<unk> men of are a a a a the .\n",
            "454 step: \n",
            " src_text:  Menschen nehmen an einem <unk> <unk> <unk> <unk> teil ., \n",
            " tgt_text:  People attending a <unk> <unk> <unk> Challenge ., \n",
            " result_text:<unk> group of are a a a a . .\n",
            "454 step: \n",
            " src_text:  Menschen auf der Straße bei einem Straßenfest ., \n",
            " tgt_text:  People on the street at a block party ., \n",
            " result_text:<unk> group of are a a . .\n",
            "454 step: \n",
            " src_text:  Ein Mann mit einem Namensschild sitzt in einem Stuhl ., \n",
            " tgt_text:  A man with a name tag on is sitting in a chair ., \n",
            " result_text:<unk> man in a a shirt is a a . .\n",
            "454 step: \n",
            " src_text:  Kinder werden in einem <unk> <unk> ., \n",
            " tgt_text:  Kids being <unk> around in a glass spinner ., \n",
            " result_text:<unk> group of are a a a . .\n",
            "454 step: \n",
            " src_text:  Ein <unk> Auto , das von vielen <unk> <unk> wird ., \n",
            " tgt_text:  A <unk> car with many firefighters cutting into the car ., \n",
            " result_text:<unk> men of are a a a a a . .\n",
            "454 step: \n",
            " src_text:  Drei Mädchen reiten auf Pferden , wobei sich das <unk> Mädchen im <unk> befindet ., \n",
            " tgt_text:  Three girls are horseback riding with the focus on the <unk> girl ., \n",
            " result_text:<unk> group of are a a a a a a a . .\n",
            "454 step: \n",
            " src_text:  Ein kleiner Junge zeigt seine <unk> <unk> ., \n",
            " tgt_text:  A young boy shows his brown and green bead necklace ., \n",
            " result_text:<unk> young is a a a a a . .\n",
            "454 step: \n",
            " src_text:  Der Junge springt mit einem <unk> aus dem Bett ., \n",
            " tgt_text:  The boy leaps of his bed with a karate kick ., \n",
            " result_text:<unk> man is a a a a a a . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric val_loss improved by 0.141 >= min_delta = 0.0. New best score: 5.229\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 454: 'val_loss' reached 5.22931 (best 5.22931), saving model to '/content/drive/MyDrive/#fastcampus/runs/de_en_translate/2023-07-24T08:08:02-AttentionBasedSeq2Seq-spacy_de_en/weights/epoch=1-val_loss=5.229.ckpt' as top 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "454 step: \n",
            " src_text:  Zwei Männer aus gegnerischen Teams rennen in Richtung eines Fußballs ., \n",
            " tgt_text:  Two men on opposing teams race toward a soccer ball ., \n",
            " result_text:<unk> group of are a a a a a . .\n",
            "454 step: \n",
            " src_text:  Zwei schwarz gekleidete Männer mit einer grünen und einer roten Fliege treten vor einer Menschenmenge auf ., \n",
            " tgt_text:  Two men in black clothes with blue and red bowties are performing in front of a crowd ., \n",
            " result_text:<unk> man in a a shirt and a a a a a a a a a . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Few1__7N7NXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}