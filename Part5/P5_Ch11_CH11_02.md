# <Generative model series #3> Latent Variable Models - 2. Variational Inference (변분 추론) 
## Variational Inference (VI)
<img width="165" alt="스크린샷 2023-08-10 오후 6 00 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/00afa8d8-82b2-4e51-9d8e-ac9dcc61832c">

- 목표: 위의 모델이 주어졌을때, 잠재변수의 posterior distribution의 획득(p(z|x))

  <img width="230" alt="스크린샷 2023-08-10 오후 6 01 34" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7e752997-4822-4616-a493-e8611b419990">
- 하지만 우리는 p(x|z)를 쉽게 구할 수 없다. p(z) -> x로의 생성모델이기 때문이다.

변분추론이란? 
- 사후확률 분포를 구하는게 어렵기 때문에,
- p모델(생성 모델) 외에 이를 근사하는 q모델(q(z))을 만들어 variational distribution로 posterior(p(z|x))를 근사하자는 것.
이는 곧, 2개의 분포, 즉 q(z)와 p(z|x) 사이의 거리를 줄여 최적화하는 문제
  - KL Divergence 사용
 
## Recap: KL Divergence
KL Divergence
- 두 확률분포의 다른 정도를 나타내는 척도
- Cross entrophy에서 entrophy 값을 빼서 Relative entrophy라고도 함.

  <img width="515" alt="스크린샷 2023-08-10 오후 6 08 01" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f893b2ef-eaed-4a1a-808d-7358713c0cf9">

KL Divergence의 성질
1. 교환법칙이 성립하지 않는다.
2. 0이 되는 시점 : q 분포와 p 분포가 동일할 때
3. KL divergence는 0과 같거나 크다. 

3번 유도방법 :  <img width="347" alt="스크린샷 2023-08-10 오후 6 08 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/8cc22611-5ad2-4784-b4b2-81d6530b5535"> (jensen's inequality)

## Variational Inference (VI)
<img width="161" alt="스크린샷 2023-08-10 오후 6 12 58" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/f9795eb7-4643-40c3-81fc-5e0fbffe552c">

즉, $q_x(z)$로 true posterior p(z|x)를 근사하는 문제는, 2개의 분포($q_x(z)$ || p(z|x))의 거리를 줄여 최적화하는 문제와 같다.

<img width="347" alt="스크린샷 2023-08-10 오후 6 10 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/0a60d56c-1087-4c95-ab07-a452f60107f1">

- 이때 stochastic sample에 의해 expection이 근사되고, 모든 expection은 O(1)에 계산될 수 있다.
- 여기서 logp(x)에 대해 정리해보자.

## Variational Inference (VI) – Variational lower bound (VLB) & ELBO
<img width="482" alt="스크린샷 2023-08-10 오후 6 13 25" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d060f549-b082-4c88-b876-349c46404750">

- log p(x)에 대해 정리하여 loss식을 정의한다. 

  <img width="482" alt="스크린샷 2023-08-10 오후 6 14 17" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/db4afc32-54e7-40e8-a282-128d153701fa">

  - <img width="150" alt="스크린샷 2023-08-10 오후 6 17 23" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/655100fc-3848-43b2-8a4b-cd1821287cac">을 variational lower bound(VLB)라고 한다.
  - 여기서 <img width="100" alt="스크린샷 2023-08-10 오후 6 18 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/c6fcca94-e614-4683-872d-cfcd978855b6">는 KL divergence의 성질에 의해 항상 0보다 같거나 크고,
  - VLB의 최적의 $q_x(z)$ 상태는 VLB =logp(x)일떄이며, KL식이 0이 되는때, 즉 p(z|x)일때이다.


- 만약 데이터가 주어졌을때 ( $x \sim$ $p_{data}$ (x) $ ), generative model을 VLB를 maximizing하게 학습하면된다.

  <img width="400" alt="스크린샷 2023-08-10 오후 6 21 32" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/7faf43d3-3328-4d53-898f-dde75864d7da">


