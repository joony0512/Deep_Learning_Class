## CH03_05-Regularization-Early-stopping

### Early Stopping (수치 최적화(numerical optimization)에 따른 정형화)

- 큰모델을 학습할 때,  Train error는 학습이 진행되면서 점점 내려가지만, Validation error는 감소하다가 어느 순간부터 다시 증가한다.
    - 이때 멈춘다면, 최적의 결과를 얻을 수 있지 않을까?
    - → Early Stopping
    ![Untitled (12)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/b38d6a77-ce23-426c-9e3e-3488567bb8bb)

    
- Early Stopping은 사실 L2정형화와 같다.
    - w*로 가기전, $\tilde{w}$에서 멈춰라!
    
    ![Untitled (13)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ddff68d0-8af8-4dd7-8bee-3a83cbc3e0db)

- Early Stopping은 모델을 한번만 학습하면 되는 반면, L2을 위한 weight decay는 적절한 hyperparameter값을 찾기 위해 여러번 학습을 돌려야한다.
- 사람마다 선호하는 방법이 다르기 때문에 사용하면서 선택한다.

## CH03_06-Regularization-파라미터공유

- 학습과정에서 weight 를 두번이상 사용해 표현을 변환하는데, 그 변환과정이 서로 거의 같아야 한다는 것을 알고있을때 사용하는 방법
    - 각 파라미터가 가까워지게 n-norm의 penalty를 사용하면 $**\Omega(w_a, w_b) =||w_a, w_b||^n_n$** 라고 표현할 수 있다.
    - 이때,  weight들이 서로 같아진다면, 즉 $w_a =w_b$라면, weight를 공유한 상태라고 말할 수 있고, 이를 parameter sharing 이라고 부른다.
        - 파라미터를 공유하면, 모델의 수용량 및 복잡도를 줄일 수 있다.
- CNN의 파라미터 공유
    - 일반적인 이미지 처리에서, 이미지의 특정한부분을 변환할때 유용한 방법이 다른부분을 변환할때도 유용한경우가 통계적으로 높게 나타난다.
        - 모든 다른 이미지 필드의 위치에서, 같은종류의 변환을 해도 된다면 파라미터( weight)를 공유해 볼 수 있다.
        - 이는 또한 학습하여야 하는 파라미터를 매우 줄인다.
        - 메모리와 연산속도에서 이득이다.
        ![Untitled (14)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/89c34b99-2af0-4f27-8771-6ef894ca89b5)

        
        
- RNN의 파라미터 공유( parameter sharing)
    - RNN역시 각각 다른 time step마다 같은 변형을 취하면 된다는 가정에 세운 모델이므로 파라미터를 공유할 수 있다.
    - 파라미터가 공유되어있기 때문에, RNN을 다른 시퀀스 사이즈로 바꾸면서 학습이 가능하다.
    ![Untitled (15)](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2ab6fda1-3474-4b7f-b727-107700c6cd73)

    
