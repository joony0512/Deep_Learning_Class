# Pretrained Large-Scale Transformer -1. Pre-trained Big Transformer가 나오기까지
## Recap: 트랜스포머 (Transformer)
  RNN은 sequence-to-sequence에서 필수적이지 않다?  
  오히려 RNN 구조로 인해 long-term dependency가 야기된다.  
  우리가 필요한 것은 “attention” 뿐이다.   -> ‘attention is all you need’  
  2017년 Vaswani, Ashish 등 Google 연구팀이 발표.
  
  <img width="202" alt="스크린샷 2023-08-08 오후 12 35 52" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9ed3c1b6-6537-46ea-83a4-09391b29a92c">

## Recap: 트랜스포머 (Transformer)의 종류
주로 아래 방향으로 발전!
1. Complexity의 개선 (하단의 efficient transformer 구분표)
2. 성능개선
3. 도메인 확장

   <img width="252" alt="스크린샷 2023-08-08 오후 12 38 19" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/6db26164-1a98-482e-9f53-ae2e068d19c6">


개선 방법에 따라 아래와 같이 범주화 할 수 있다!
- Module level
- Architecture level
- Pre-Train -> 이번 시간에 다룰 내용!  
- Application

  <img width="307" alt="스크린샷 2023-08-08 오후 12 38 45" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/2719e74c-7bf7-4187-b893-3d3fea422f83">

## Recap: Encoder-decoder Model vs Encoder Model vs Decoder Model
<img width="443" alt="스크린샷 2023-08-08 오후 12 40 06" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/edad5f54-2790-42e1-a8b3-1a6ae4990959">

- 초록색 박스: encoder : input의 representation을 학습.
- 주황색 박스: decoder : generative model로 encoder의 결과 혹은 이전에 생성한 결과로 output을 생성
- 파란색 박스: encoder-decoder attention : encoder와 decoder을 연결

## Encoder-decoder Transformer Model - Encoder Model, Decoder Model
<img width="200" alt="스크린샷 2023-08-08 오후 12 41 55" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1446fbd9-724f-4f30-843d-62e1c8757e62">

- 초록색 박스: encoder
	- input의 representation을 학습.
- 주황색 박스: decoder
	- generative model로 encoder의 결과 혹은 이전에 생성한 결과로 output을 생성
- 파란색 박스: encoder-decoder attention
	- encoder와 decoder을 연결

<img width="300" alt="스크린샷 2023-08-08 오후 12 43 44" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/4f80810d-2d73-43ef-8de9-54af7bf47767">
  
Transformer도 마찬가지로 encoder block과 decoder로 나눌 수 있다.  
Encoder-Decoder 구조만 따와서 응용해볼 수 있을까?

## Recap: Pre-trained models - Transfer learning (semi-supervised learning)이 약 2016년부터 주목..!
<img width="300" alt="스크린샷 2023-08-08 오후 12 45 12" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/928f0fb5-e7f4-453d-acf0-d24b018a5481">
<img width="300" alt="스크린샷 2023-08-08 오후 12 45 40" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/ff4f3cb6-3803-4f00-b512-0cf447499c9f">

## Recap: Pre-trained models - ELMo (Embedding for Language Model)
- 기존의 word2vec, GloVe 등은 동음이의어 등을 반영하지 못한다는 문제가 있었다.
  예) bear: 견디다 / 지탱하다 / 곰
  People seem to bear this unnecessary noise vs My favorite animal is a bear.  

- Language model(언어 모델)을 이용해 문장에서 각 단어의 문맥을 학습하게 하자!

- ELMo는 RNN에 기반한 embedding
  가장 큰 특징으로, 사전 훈련된 언어 모델(pre-trained language model)을 사용한다 !

![image](https://github.com/joony0512/Deep_Learning_Class/assets/109457820/730ae569-bb82-4f6c-a0c4-fe8b9cf586ca)

- biLM (bi-directional language model)을 이용하는데, 
  char CNN 방법 등을 이용해 character 단위로 계산하여 최초의 embedding을 먼저 한다.
	- fast text과 같이 subword 정보를 받으므로, OOV에도 견고하다!

- 그 이후 LSTM 기반의 LM을 각각 forward, backward로 pretraining하고, finetune한다.
- 최종 값을 구하기 위해 아래 과정을 거친다.
1. 각 층의 출력 값을 concatenate한다.
2. 각 층의 출력 값 별로 가중치를 주고 더한다 (weighted sum).
3. 벡터의 크기를 결정하는 스칼라 매개변수를 곱하여 최종적인 representation을 구한다.

## Pre-training + Transformer (enc, dec, enc + dec) - 새로운 혁신!
<img width="1035" alt="스크린샷 2023-08-08 오후 1 03 49" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/d717f2de-fb38-4266-bd3e-90466a0734e9">
<img width="646" alt="스크린샷 2023-08-08 오후 1 04 16" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/9673ce55-5de0-4fa4-a873-f4101b17f4d1">

##+ Going to large-scale transformer 
<img width="387" alt="스크린샷 2023-08-08 오후 1 08 20" src="https://github.com/joony0512/Deep_Learning_Class/assets/109457820/1e0ecd72-b813-4958-a1b1-4d5480719279">
